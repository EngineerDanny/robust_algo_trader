{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from PIL import Image\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.autograd import Variable\n",
    "from keras.datasets import mnist\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "\n",
    "def resize(pics):\n",
    "    pictures = []\n",
    "    for image in pics:\n",
    "        image = Image.fromarray(image).resize((dim, dim))\n",
    "        image = np.array(image)\n",
    "        pictures.append(image)\n",
    "    return np.array(pictures)\n",
    "\n",
    "\n",
    "dim = 60\n",
    "\n",
    "x_train, x_test = resize(x_train), resize(x_test) # because my real problem is in 60x60\n",
    "\n",
    "x_train = x_train.reshape(-1, 1, dim, dim).astype('float32') / 255\n",
    "x_test = x_test.reshape(-1, 1, dim, dim).astype('float32') / 255\n",
    "#### float32 -> int64\n",
    "y_train, y_test = y_train.astype('int64'), y_test.astype('int64')\n",
    "\n",
    "#### no reason to test for cuda before converting to numpy\n",
    "\n",
    "#### I assume you were taking a subset for debugging? No reason to not use all the data\n",
    "x_train = torch.from_numpy(x_train)\n",
    "x_test = torch.from_numpy(x_test)\n",
    "y_train = torch.from_numpy(y_train)\n",
    "y_test = torch.from_numpy(y_test)\n",
    "\n",
    "\n",
    "class ConvNet(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, 3)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3)\n",
    "        self.conv3 = nn.Conv2d(64, 128, 3)\n",
    "\n",
    "        self.fc1 = nn.Linear(5*5*128, 1024)\n",
    "        self.fc2 = nn.Linear(1024, 2048)\n",
    "        #### 1 -> 10\n",
    "        self.fc3 = nn.Linear(2048, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2))\n",
    "        x = F.max_pool2d(F.relu(self.conv2(x)), (2, 2))\n",
    "        x = F.max_pool2d(F.relu(self.conv3(x)), (2, 2))\n",
    "\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.dropout(x, 0.5)\n",
    "        #### removed sigmoid\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "net = ConvNet()\n",
    "\n",
    "#### 0.03 -> 1e-3\n",
    "optimizer = optim.Adam(net.parameters(), lr=1e-3)\n",
    "\n",
    "#### BCELoss -> CrossEntropyLoss\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "class FaceTrain:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.len = x_train.shape[0]\n",
    "        self.x_train = x_train\n",
    "        self.y_train = y_train\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        #### .unsqueeze(0) removed\n",
    "        return x_train[index], y_train[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "\n",
    "\n",
    "class FaceTest:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.len = x_test.shape[0]\n",
    "        self.x_test = x_test\n",
    "        self.y_test = y_test\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        #### .unsqueeze(0) removed\n",
    "        return x_test[index], y_test[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "\n",
    "\n",
    "train = FaceTrain()\n",
    "test = FaceTest()\n",
    "\n",
    "train_loader = DataLoader(dataset=train, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(dataset=test, batch_size=64, shuffle=True)\n",
    "\n",
    "epochs = 10\n",
    "steps = 0\n",
    "train_losses, test_losses = [], []\n",
    "for e in range(epochs):\n",
    "    running_loss = 0\n",
    "    #### put net in train mode\n",
    "    net.train()\n",
    "    for idx, (images, labels) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        log_ps = net(images)\n",
    "        loss = loss_function(log_ps, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    else:\n",
    "        test_loss = 0\n",
    "        accuracy = 0\n",
    "\n",
    "        #### put net in eval mode\n",
    "        net.eval()\n",
    "        with torch.no_grad():\n",
    "            for images, labels in test_loader:\n",
    "                log_ps = net(images)\n",
    "                test_loss += loss_function(log_ps, labels)\n",
    "                #### removed torch.exp() since exponential is monotone, taking it doesn't change the order of outputs. Similarly with torch.softmax()\n",
    "                top_p, top_class = log_ps.topk(1, dim=1)\n",
    "                #### convert to float/long using proper methods. what you have won't work for cuda tensors.\n",
    "                equals = top_class.long() == labels.long().view(*top_class.shape)\n",
    "                accuracy += torch.mean(equals.float())\n",
    "        train_losses.append(running_loss/len(train_loader))\n",
    "        test_losses.append(test_loss/len(test_loader))\n",
    "        print(\"[Epoch: {}/{}] \".format(e+1, epochs),\n",
    "              \"[Training Loss: {:.3f}] \".format(running_loss/len(train_loader)),\n",
    "              \"[Test Loss: {:.3f}] \".format(test_loss/len(test_loader)),\n",
    "              \"[Test Accuracy: {:.3f}]\".format(accuracy/len(test_loader)))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
