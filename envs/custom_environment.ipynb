{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gymnasium.envs.registration import register\n",
    "import gymnasium as gym\n",
    "from stable_baselines3 import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tensorboard --logdir ./ppo_custom_env_tensorboard\n",
    "register(\n",
    "     id=\"my_envs/CustomGridWorld-v0\",\n",
    "     entry_point=\"my_envs:CustomGridWorldEnv\",\n",
    "     max_episode_steps=100,\n",
    ")\n",
    "\n",
    "env = gym.make(\"my_envs/CustomGridWorld-v0\", render_mode=\"human\")\n",
    "model = A2C('MultiInputPolicy', env, verbose=1, tensorboard_log=\"./ppo_custom_env_tensorboard/\")\n",
    "model.learn(total_timesteps=100_000, log_interval=10, tb_log_name=\"first_run\")\n",
    "model.save(\"ppo_custom_env\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # save the model\n",
    "# model.save(\"ppo_custom_env\")\n",
    "\n",
    "# # load the model\n",
    "# model = PPO.load(\"ppo_custom_env\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PPO.load(\"ppo_custom_env\")\n",
    "\n",
    "observation, info = env.reset(seed=42)\n",
    "for _ in range(3000*5):\n",
    "    action_arr, _states = model.predict(observation)\n",
    "    action = action_arr.item()\n",
    "    observation, reward, terminated, truncated, info = env.step(action)\n",
    "    env.render()\n",
    "    \n",
    "    if terminated or truncated:\n",
    "        observation, info = env.reset()\n",
    "        \n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from gymnasium.envs.registration import register\n",
    "from stable_baselines3 import *\n",
    "from copy import deepcopy\n",
    "from gymnasium.envs.registration import register\n",
    "import pandas as pd\n",
    "import talib\n",
    "\n",
    "##TODO: add path to csv file\n",
    "index_name = \"Time\"\n",
    "df = pd.read_csv(path, parse_dates=True, index_col=index_name)\n",
    "prices = df[\"Close\"].values\n",
    "sma = talib.SMA(prices, timeperiod=200)\n",
    "# add sma to df\n",
    "df[\"SMA\"] = sma\n",
    "# remove NaNs\n",
    "df = df.dropna()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "register(\n",
    "    id='trade_env/CustomForex-v0',\n",
    "    entry_point='trade_env:CustomForexEnv',\n",
    "    #  max_episode_steps=100,\n",
    "    kwargs={\n",
    "        'df': deepcopy(df),\n",
    "        'window_size': 24,\n",
    "        'frame_bound': (24, len(df))\n",
    "    }\n",
    ")\n",
    "\n",
    "env = gym.make(\n",
    "              'trade_env/CustomForex-v0',\n",
    "               df = deepcopy(df),\n",
    "               window_size = 10,\n",
    "               frame_bound = (10, 3000),\n",
    "            #    num_envs=3,\n",
    "            #    asynchronous=False,\n",
    "            #    wrappers=None,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# from stable_baselines3 import A2C\n",
    "from sb3_contrib import RecurrentPPO\n",
    "policy_kwargs = dict(net_arch=[64, 'lstm', dict(vf=[128, 128, 128], pi=[64, 64])])\n",
    "model = RecurrentPPO('MlpLstmPolicy', env, verbose=1, tensorboard_log=\"./trade_env_tensorboard/\")\n",
    "model.learn(total_timesteps=30_000,  log_interval=10, tb_log_name=\"trade_name\")\n",
    "\n",
    "# model = PPO('MlpPolicy', env, verbose=1)\n",
    "# model.learn(total_timesteps=1_000_000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "observation, info = env.reset(seed = 2)\n",
    "\n",
    "while True:\n",
    "    action, _state = model.predict(observation)\n",
    "    # print(\"action:\", action)\n",
    "    # action = env.action_space.sample()\n",
    "    observation, reward, terminated, truncated, info = env.step(action)\n",
    "    # observation, reward, done, info = env.step(action)\n",
    "    # env.render()\n",
    "    if terminated:\n",
    "        print(\"info:\", info)\n",
    "        break\n",
    "\n",
    "plt.cla()\n",
    "env.render_all()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "from copy import deepcopy\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "from gymnasium.utils import seeding\n",
    "from enum import Enum\n",
    "from sklearn.preprocessing import StandardScaler, PowerTransformer\n",
    "import talib\n",
    "\n",
    "\n",
    "    \n",
    "class MyForexEnv(gym.Env):\n",
    "    metadata = {\"render_modes\": [\"human\"]}\n",
    "    \n",
    "    def __init__(self,  df, window_size, frame_bound):\n",
    "        assert len(frame_bound) == 2\n",
    "        super().__init__()\n",
    "        \n",
    "        self.frame_bound = frame_bound\n",
    "        # self.seed()\n",
    "        self.df = df\n",
    "        self.window_size = window_size\n",
    "        self.prices, self.signal_features = self._process_data()\n",
    "        self.shape = (window_size, self.signal_features.shape[1])\n",
    "        # Inf should be a large enough upper bound\n",
    "        INF = 1e9\n",
    "\n",
    "        # spaces\n",
    "        self.observation_space = spaces.Box(low=-INF, \n",
    "                                            high=INF,\n",
    "                                            shape=self.shape, \n",
    "                                            dtype=np.float64)\n",
    "        self.action_space = spaces.MultiDiscrete([2, 2, 4, 2])\n",
    "\n",
    "        # episode\n",
    "        self._start_tick = self.window_size\n",
    "        self._end_tick = len(self.prices) - 1\n",
    "        self._current_tick = None\n",
    "        self._last_trade_tick = None\n",
    "        self._done = None\n",
    "        \n",
    "        self._position = None\n",
    "        self._position_history = None\n",
    "        self._total_reward = None\n",
    "        self._total_profit = None\n",
    "        \n",
    "        self._first_rendering = None\n",
    "        self._first_time = None\n",
    "        \n",
    "        self.history = None\n",
    "        self.trade_fee = 0.0003 \n",
    "        self._seed()\n",
    "        \n",
    "        # self.reset()\n",
    "        # self.step([0, 0, 13, 13])\n",
    "        # self.step([1,  0, 10, 14])\n",
    "    #    print(\"self._position_history:\", self._position_history)\n",
    "        # self.step(0)\n",
    "        # self.step(0)\n",
    "        # self.step(1)\n",
    "        # self.step(1)\n",
    "        # self.step(1)\n",
    "        # self.step(0)\n",
    "   \n",
    "    def _process_data(self):\n",
    "        prices = self.df.loc[:, 'Close'].to_numpy()\n",
    "        sma = self.df.loc[:, 'SMA'].to_numpy()\n",
    "        ema = self.df.loc[:, 'EMA'].to_numpy()\n",
    "        \n",
    "        prices = prices[self.frame_bound[0] - self.window_size:self.frame_bound[1]]\n",
    "        diff = np.insert(np.diff(prices), 0, 0)\n",
    "        sma = sma[self.frame_bound[0] - self.window_size:self.frame_bound[1]]\n",
    "        ema = ema[self.frame_bound[0] - self.window_size:self.frame_bound[1]]\n",
    "        \n",
    "        signal_features = np.column_stack((sma, ema))\n",
    "        # scale signal features\n",
    "        # scaler = StandardScaler()\n",
    "        # signal_features = scaler.fit_transform(signal_features)\n",
    "        # transformer = PowerTransformer()\n",
    "        # signal_features = transformer.fit_transform(signal_features)\n",
    "        return sma, signal_features\n",
    "    \n",
    "    def _seed(self, seed=None):\n",
    "        self.np_random, seed = seeding.np_random(seed)\n",
    "        return [seed]     \n",
    "    \n",
    "    def reset(self, seed=None, options=None):\n",
    "        print(\"reset\\n\")\n",
    "        super().reset(seed=seed)\n",
    "        self._done = False\n",
    "        self._current_tick = self._start_tick\n",
    "        self._last_trade_tick = self._current_tick - 1\n",
    "\n",
    "        self._total_reward = 0.\n",
    "        self._total_profit = 1.  # unit\n",
    "        \n",
    "        self._first_rendering = True\n",
    "        self._first_time = True\n",
    "        \n",
    "        self.history = {}\n",
    "        observation = self._get_observation()\n",
    "        info = dict(\n",
    "            total_reward = self._total_reward,\n",
    "            total_profit = self._total_profit,\n",
    "        )\n",
    "        self.existing_trade = None\n",
    "        return observation, info\n",
    "\n",
    "\n",
    "    def step(self, action):\n",
    "        # print(\"action:\", action)\n",
    "        self._done = False\n",
    "        self._current_tick += 1\n",
    "\n",
    "        if self._current_tick == self._end_tick:\n",
    "            self._done = True\n",
    "\n",
    "        step_reward = self._calculate_reward(action)\n",
    "        self._total_reward += step_reward\n",
    "        self._update_profit(action)\n",
    "\n",
    "        observation = self._get_observation()\n",
    "        info = dict(\n",
    "            total_reward = self._total_reward,\n",
    "            total_profit = self._total_profit,\n",
    "        )\n",
    "        \n",
    "        # self._position_history.append(self._position)\n",
    "        # if not self.history:\n",
    "        #     self.history = {key: [] for key in info.keys()}\n",
    "        # for key, value in info.items():\n",
    "        #     self.history[key].append(value)\n",
    "            \n",
    "        return observation, step_reward, self._done, False, info\n",
    "\n",
    "\n",
    "    def _calculate_reward(self, action):\n",
    "        actual_state, trade_position, take_profit, stop_loss = action\n",
    "        current_price = self.prices[self._current_tick]\n",
    "        # initialize step reward to 0\n",
    "        step_reward = 0  \n",
    "\n",
    "        ## EXISTING TRADE STATE\n",
    "        if self.existing_trade:\n",
    "            existing_trade_position = self.existing_trade['trade_position']\n",
    "            existing_tp = self.existing_trade['take_profit']\n",
    "            existing_sl = self.existing_trade['stop_loss']\n",
    "            entry_price = self.existing_trade['entry_price']\n",
    "            \n",
    "            \n",
    "            price_diff = current_price - entry_price\n",
    "            # if existing_trade_position is 1, and current_price triggers take_profit or stop_loss\n",
    "            if existing_trade_position == 1: \n",
    "                # check if current_price is close to take_profit than stop_loss\n",
    "                # if abs(current_price - existing_tp) < abs(current_price - existing_sl):\n",
    "                if current_price >= existing_tp:\n",
    "                    # step_reward += price_diff * 10000\n",
    "                    step_reward += 1\n",
    "                  \n",
    "                elif current_price <= existing_sl:\n",
    "                    # step_reward += price_diff * 5000\n",
    "                    step_reward += 0.5\n",
    "                elif price_diff >= 0:\n",
    "                    step_reward += 1   \n",
    "                else:\n",
    "                    step_reward += 0.5       \n",
    "            else:\n",
    "                if current_price <= existing_tp:\n",
    "                    step_reward += 1\n",
    "                    # step_reward += price_diff * 10000\n",
    "                elif current_price >= existing_sl:\n",
    "                    step_reward += 0.5\n",
    "                    # step_reward += price_diff * 5000\n",
    "                elif price_diff <= 0:\n",
    "                    step_reward += 1\n",
    "                else:\n",
    "                    step_reward += 0.5\n",
    "            # Deductions\n",
    "            # if actual_state is TRADE then deduct 0.5\n",
    "            if actual_state == 1:\n",
    "                step_reward -= 0.2\n",
    "            # if SLdiff is greater than 2*TPdiff then deduct 0.3\n",
    "            if abs(existing_sl - existing_tp) > 2 * abs(existing_tp - entry_price):\n",
    "                step_reward -= 0.3                      \n",
    "                  \n",
    "        ## EXISTING IDLE STATE\n",
    "        else:\n",
    "            # if current_state is IDLE\n",
    "            if actual_state == 0:\n",
    "                if self._first_time:\n",
    "                    step_reward += 0.2\n",
    "                else:\n",
    "                    step_reward += 0.5    \n",
    "            # if current_state is TRADE\n",
    "            else:\n",
    "                step_reward += 0.6    \n",
    "        \n",
    "        print(\"\\nstep_reward:\", step_reward)     \n",
    "        print(\"action:\", action)\n",
    "        return step_reward\n",
    "    \n",
    "    \n",
    "    def _update_profit(self, action):\n",
    "        actual_state, trade_position, take_profit, stop_loss = action\n",
    "        if self.existing_trade:\n",
    "            existing_trade_position = self.existing_trade['trade_position']\n",
    "            existing_tp = self.existing_trade['take_profit']\n",
    "            existing_sl = self.existing_trade['stop_loss']\n",
    "            entry_price = self.existing_trade['entry_price']\n",
    "            current_price = self.prices[self._current_tick]\n",
    "            \n",
    "            # if existing_trade_position is 1, and current_price triggers take_profit or stop_loss\n",
    "            percent_price_diff = (current_price - entry_price)/ entry_price\n",
    "            if existing_trade_position == 1: \n",
    "                if current_price >= existing_tp:\n",
    "                    self._total_profit += percent_price_diff\n",
    "                    # close the trade\n",
    "                    self.existing_trade = None\n",
    "                elif current_price <= existing_sl:\n",
    "                    self._total_profit += percent_price_diff\n",
    "                    # close the trade\n",
    "                    self.existing_trade = None\n",
    "            else:\n",
    "                if current_price <= existing_tp:\n",
    "                    self._total_profit += percent_price_diff\n",
    "                    # close the trade\n",
    "                    self.existing_trade = None\n",
    "                elif current_price >= existing_sl:\n",
    "                    self._total_profit += percent_price_diff\n",
    "                    # close the trade\n",
    "                    self.existing_trade = None\n",
    "                    \n",
    "        if actual_state == 1 and self.existing_trade is None:\n",
    "            self._first_time = False\n",
    "            current_price = self.prices[self._current_tick]\n",
    "            delta = 0.01\n",
    "            if trade_position == 1:\n",
    "                # if there is a buy trade\n",
    "                take_profit_price = current_price + ((take_profit + 1) * delta)\n",
    "                stop_loss_price = current_price - ((stop_loss + 1) * delta)\n",
    "            else:\n",
    "                # if there is a sell trade\n",
    "                take_profit_price = current_price - ((take_profit + 1) * delta)\n",
    "                stop_loss_price = current_price + ((stop_loss + 1) * delta)    \n",
    "            \n",
    "            self.existing_trade = {\n",
    "                'trade_position': trade_position,\n",
    "                'take_profit': take_profit_price,\n",
    "                'stop_loss': stop_loss_price,\n",
    "                'entry_price': current_price,\n",
    "                'trade_tick': self._current_tick,\n",
    "            }\n",
    "            print(\"action:\", action)\n",
    "            print(\"total_profit:\", self._total_profit)\n",
    "            print(\"existing_trade:\", self.existing_trade)\n",
    "        \n",
    "        \n",
    "        \n",
    "    def _get_observation(self):\n",
    "        return self.signal_features[(self._current_tick-self.window_size+1):self._current_tick+1]\n",
    "\n",
    "\n",
    "    \n",
    "env = MyForexEnv(\n",
    "       df = deepcopy(df),\n",
    "       window_size = 500,\n",
    "       frame_bound = (500, 1_000),\n",
    "    #    frame_bound = (1_000, 2_000),\n",
    "    #    frame_bound = (7_000, 15_000),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "from gymnasium.utils import seeding\n",
    "from enum import Enum\n",
    "from sklearn.preprocessing import StandardScaler, PowerTransformer\n",
    "import talib\n",
    "\n",
    "\n",
    "class Actions(Enum):\n",
    "    Sell = 0\n",
    "    Buy = 1\n",
    "\n",
    "\n",
    "class Positions(Enum):\n",
    "    Short = 0\n",
    "    Long = 1\n",
    "\n",
    "    def switch(self):\n",
    "        return Positions.Short if self == Positions.Long else Positions.Long\n",
    "  \n",
    "    \n",
    "class MyForexEnv(gym.Env):\n",
    "    metadata = {\"render_modes\": [\"human\"]}\n",
    "    \n",
    "    def __init__(self,  df, window_size, frame_bound):\n",
    "        assert len(frame_bound) == 2\n",
    "        super().__init__()\n",
    "        \n",
    "        self.frame_bound = frame_bound\n",
    "        # self.seed()\n",
    "        self.df = df\n",
    "        self.window_size = window_size\n",
    "        self.prices, self.signal_features = self._process_data()\n",
    "        self.shape = (window_size, self.signal_features.shape[1])\n",
    "        # Inf should be a large enough upper bound\n",
    "        INF = 1e9\n",
    "\n",
    "        # spaces\n",
    "        self.observation_space = spaces.Box(low=-INF, \n",
    "                                            high=INF,\n",
    "                                            shape=self.shape, \n",
    "                                            dtype=np.float64)\n",
    "        self.action_space = spaces.Discrete(2)\n",
    "\n",
    "        # episode\n",
    "        self._start_tick = self.window_size\n",
    "        self._end_tick = len(self.prices) - 1\n",
    "        self._current_tick = None\n",
    "        self._last_trade_tick = None\n",
    "        self._done = None\n",
    "        \n",
    "        self._position = None\n",
    "        self._position_history = None\n",
    "        self._total_reward = None\n",
    "        self._total_profit = None\n",
    "        self._first_rendering = None\n",
    "        self.history = None\n",
    "        self.trade_fee = 0.0003 \n",
    "        self._seed()\n",
    "        \n",
    "        # self.reset()\n",
    "        # self.step(0)\n",
    "        # self.step(0)\n",
    "        # self.step(1)\n",
    "        # self.step(1)\n",
    "        # self.step(1)\n",
    "        # self.step(0)\n",
    "   \n",
    "    def _seed(self, seed=None):\n",
    "        self.np_random, seed = seeding.np_random(seed)\n",
    "        return [seed]     \n",
    "    \n",
    "    def reset(self, seed=None, options=None):\n",
    "        super().reset(seed=seed)\n",
    "        self._done = False\n",
    "        self._current_tick = self._start_tick\n",
    "        self._last_trade_tick = self._current_tick - 1\n",
    "        self._position = Positions.Long\n",
    "        self._position_history = (self.window_size * [None]) + [self._position]\n",
    "        self._total_reward = 0.\n",
    "        self._total_profit = 1.  # unit\n",
    "        self._first_rendering = True\n",
    "        self.history = {}\n",
    "        observation = self._get_observation()\n",
    "        info = dict(\n",
    "            total_reward = self._total_reward,\n",
    "            total_profit = self._total_profit,\n",
    "            position = self._position.value\n",
    "        )\n",
    "        return observation, info\n",
    "\n",
    "        \n",
    "    def _process_data(self):\n",
    "        prices = self.df.loc[:, 'Close'].to_numpy()\n",
    "        sma = self.df.loc[:, 'SMA'].to_numpy()\n",
    "        prices = prices[self.frame_bound[0] - self.window_size:self.frame_bound[1]]\n",
    "        sma = sma[self.frame_bound[0] - self.window_size:self.frame_bound[1]]\n",
    "        diff = np.insert(np.diff(prices), 0, 0)\n",
    "        \n",
    "        signal_features = np.column_stack(( sma, sma))\n",
    "        # scale signal features\n",
    "        # scaler = StandardScaler()\n",
    "        # signal_features = scaler.fit_transform(signal_features)\n",
    "        # transformer = PowerTransformer()\n",
    "        # signal_features = transformer.fit_transform(signal_features)\n",
    "        \n",
    "        return prices, signal_features\n",
    "\n",
    "    def step(self, action):\n",
    "        # print(\"action:\", action)\n",
    "        self._done = False\n",
    "        self._current_tick += 1\n",
    "\n",
    "        if self._current_tick == self._end_tick:\n",
    "            self._done = True\n",
    "\n",
    "        step_reward = self._calculate_reward(action)\n",
    "        self._total_reward += step_reward\n",
    "        self._update_profit(action)\n",
    "\n",
    "        if self._has_traded(action):\n",
    "            self._position = self._position.switch()\n",
    "            self._last_trade_tick = self._current_tick\n",
    "\n",
    "        self._position_history.append(self._position)\n",
    "        observation = self._get_observation()\n",
    "        info = dict(\n",
    "            total_reward = self._total_reward,\n",
    "            total_profit = self._total_profit,\n",
    "            position = self._position.value\n",
    "        )\n",
    "        \n",
    "        if not self.history:\n",
    "            self.history = {key: [] for key in info.keys()}\n",
    "        for key, value in info.items():\n",
    "            self.history[key].append(value)\n",
    "            \n",
    "        return observation, step_reward, self._done, False, info\n",
    "\n",
    "\n",
    "    def _calculate_reward(self, action):\n",
    "        # initialize step reward to 0\n",
    "        step_reward = 0  \n",
    "        trade = self._has_traded(action)\n",
    "\n",
    "        if trade:\n",
    "            prev_trade_price = self.prices[self._last_trade_tick]\n",
    "            current_price = self.prices[self._current_tick]\n",
    "            price_diff = current_price - prev_trade_price\n",
    "\n",
    "            if self._position == Positions.Short:\n",
    "                if price_diff < 0:\n",
    "                    # this is correct\n",
    "                    step_reward += -price_diff * 10000\n",
    "                else:\n",
    "                    # give more penalty for wrong action\n",
    "                    step_reward += -price_diff * 10000\n",
    "                    \n",
    "            elif self._position == Positions.Long:\n",
    "                if price_diff > 0:\n",
    "                    # this is correct\n",
    "                    step_reward += price_diff * 10000\n",
    "                else:\n",
    "                    # give more penalty for wrong action\n",
    "                    step_reward += price_diff * 10000\n",
    "        \n",
    "        # print(\"step_reward:\", step_reward)\n",
    "        # print(\"profit:\", self._total_profit)\n",
    "        return step_reward\n",
    "    \n",
    "    def _has_traded(self, action):\n",
    "        return ((action == Actions.Buy.value and \n",
    "                 self._position == Positions.Short) or\n",
    "            (action == Actions.Sell.value and \n",
    "             self._position == Positions.Long))\n",
    "        \n",
    "    def _get_observation(self):\n",
    "        return self.signal_features[(self._current_tick-self.window_size+1):self._current_tick+1]\n",
    "\n",
    "    def _update_profit(self, action):\n",
    "        if self._has_traded(action) or self._done:\n",
    "            current_price = self.prices[self._current_tick]\n",
    "            prev_trade_price = self.prices[self._last_trade_tick]\n",
    "            price_diff = current_price - prev_trade_price\n",
    "            if self._position == Positions.Short:\n",
    "                self._total_profit = self._total_profit * (1 - price_diff)\n",
    "            elif self._position == Positions.Long:\n",
    "                self._total_profit = self._total_profit * (1 + price_diff)\n",
    "            \n",
    "            # print(\"current_price:\", self._total_profit ) \n",
    "            # quantity = self._total_profit / last_trade_price\n",
    "            # self._total_profit = quantity * (current_price - self.trade_fee)\n",
    "\n",
    "    \n",
    "env = MyForexEnv(\n",
    "       df = deepcopy(df),\n",
    "       window_size = 100,\n",
    "       frame_bound = (100, 3000),\n",
    "    #    frame_bound = (2000, 5000),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from gym.utils import seeding\n",
    "from gym import spaces\n",
    "from enum import Enum\n",
    "from typing import List, Dict\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('seaborn')\n",
    "\n",
    "class TradingEnvAction(Enum):\n",
    "    STAY = 0\n",
    "    BUY = 1\n",
    "    SELL = 2\n",
    "    CLOSE = 3\n",
    "\n",
    "class TradingEnvTicket(object):\n",
    "    def __init__(self, order_type, open_price, take_profit, stop_loss, lots):\n",
    "        self.order_type = order_type\n",
    "        self.open_price = open_price\n",
    "        self.take_profit = take_profit\n",
    "        self.stop_loss = stop_loss\n",
    "        self.lots = lots\n",
    "        self.trade_fee = 0.0003  # unit\n",
    "\n",
    "class TradingEnvAccountInformation(object):\n",
    "    def __init__(self, initial_balance):\n",
    "        self.balance = initial_balance\n",
    "        self.fixed_balance = initial_balance\n",
    "        self.total_pips_buy = 10\n",
    "        self.total_pips_sell = 10\n",
    "\n",
    "    def items(self):\n",
    "        return [('balance', self.balance), ('fixed_balance', self.fixed_balance), ('total_pips_buy', self.total_pips_buy), ('total_pips_sell', self.total_pips_sell)]\n",
    "\n",
    "\n",
    "class TradingEnv(gym.Env):\n",
    "\n",
    "    metadata = {'render.modes': ['human']}\n",
    "\n",
    "    def __init__(self, df, window_size, frame_bound):\n",
    "        assert len(frame_bound) == 2\n",
    "\n",
    "        self.frame_bound = frame_bound\n",
    "\n",
    "        self.trade_fee_bid_percent = 0.01  # unit\n",
    "        self.trade_fee_ask_percent = 0.005  # unit\n",
    "\n",
    "        assert df.ndim == 2\n",
    "\n",
    "        self.seed()\n",
    "        self.df = df\n",
    "        self.window_size = window_size\n",
    "        self.prices, self.signal_features = self._process_data()\n",
    "        self.shape = (window_size, self.signal_features.shape[1])\n",
    "\n",
    "        # spaces\n",
    "        self.action_space = spaces.Discrete(len(TradingEnvAction))\n",
    "        self.observation_space = spaces.Box(low=-np.inf, high=np.inf, shape=self.shape, dtype=np.float32)\n",
    "\n",
    "        # episode\n",
    "        self._start_tick = self.window_size\n",
    "        self._end_tick = len(self.prices) - 1\n",
    "        self._done = None\n",
    "        self._current_tick = None\n",
    "        self._last_trade_tick = None\n",
    "        self._position = None\n",
    "        self._position_history = None\n",
    "        self._total_reward = None\n",
    "        self._total_profit = None\n",
    "        self._first_rendering = None\n",
    "        self.history = None\n",
    "\n",
    "\n",
    "    def seed(self, seed=None):\n",
    "        self.np_random, seed = seeding.np_random(seed)\n",
    "        return [seed]\n",
    "\n",
    "\n",
    "    def reset(self):\n",
    "        self._done = False\n",
    "        self._current_tick = self._start_tick\n",
    "        self._last_trade_tick = self._current_tick - 1\n",
    "        self._position = TradingEnvAction.STAY.value\n",
    "        self._position_history = (self.window_size * [None]) + [self._position]\n",
    "        self._total_reward = 0.\n",
    "        self._total_profit = 1.  # unit\n",
    "        self._first_rendering = True\n",
    "        self.history = {}\n",
    "        return self._get_observation()\n",
    "\n",
    "\n",
    "    def step(self, action):\n",
    "        self._done = False\n",
    "        self._current_tick += 1\n",
    "\n",
    "        if self._current_tick == self._end_tick:\n",
    "            self._done = True\n",
    "\n",
    "        step_reward = self._calculate_reward(action)\n",
    "        self._total_reward += step_reward\n",
    "\n",
    "        self._update_profit(action)\n",
    "\n",
    "        trade = False\n",
    "        if (action != None):\n",
    "            trade = True\n",
    "\n",
    "        if trade:\n",
    "            self._position = action\n",
    "            self._last_trade_tick = self._current_tick\n",
    "\n",
    "        self._position_history.append(self._position)\n",
    "        observation = self._get_observation()\n",
    "        info = dict(\n",
    "            total_reward = self._total_reward,\n",
    "            total_profit = self._total_profit,           \n",
    "            position = Counter(self._position_history),\n",
    "            last_15_position_predictions = self._position_history[-15:],\n",
    "            position_predictions = self._position_history\n",
    "        )\n",
    "        self._update_history(info)\n",
    "\n",
    "        return observation, step_reward, self._done, info\n",
    "\n",
    "\n",
    "    def _get_observation(self):\n",
    "        return self.signal_features[(self._current_tick-self.window_size):self._current_tick]\n",
    "\n",
    "\n",
    "    def _update_history(self, info):\n",
    "        if not self.history:\n",
    "            self.history = {key: [] for key in info.keys()}\n",
    "\n",
    "        for key, value in info.items():\n",
    "            self.history[key].append(value)\n",
    "\n",
    "\n",
    "    def render(self, mode='human'):\n",
    "\n",
    "        def _plot_position(position, tick):\n",
    "            color = None\n",
    "            marker = 'o'\n",
    "\n",
    "            if position == TradingEnvAction.SELL.value:\n",
    "                color = 'red'\n",
    "                marker = 'v'\n",
    "            elif position == TradingEnvAction.BUY.value:\n",
    "                color = 'green'\n",
    "                marker = '^'\n",
    "            if position == TradingEnvAction.STAY.value:\n",
    "                color = 'yellow'\n",
    "                marker = 'o'\n",
    "            elif position == TradingEnvAction.CLOSE.value:\n",
    "                color = 'blue'\n",
    "                marker = 'o'\n",
    "            elif position == None:\n",
    "                color = 'purple'                \n",
    "            if color:\n",
    "                plt.scatter(tick, self.prices[tick], marker=marker,color=color)\n",
    "\n",
    "        if self._first_rendering:\n",
    "            self._first_rendering = False\n",
    "            plt.cla()\n",
    "            plt.plot(self.prices)\n",
    "            start_position = self._position_history[self._start_tick]\n",
    "            _plot_position(start_position, self._start_tick)\n",
    "\n",
    "        _plot_position(self._position, self._current_tick)\n",
    "\n",
    "        plt.suptitle(\n",
    "            \"Total Reward: %.6f\" % self._total_reward + ' ~ ' +\n",
    "            \"Total Profit: %.6f\" % self._total_profit\n",
    "        )\n",
    "\n",
    "        plt.pause(0.01)\n",
    "\n",
    "\n",
    "    def render_all(self, mode='human'):\n",
    "        window_ticks = np.arange(len(self._position_history))\n",
    "        plt.plot(self.prices)\n",
    "\n",
    "        short_ticks = []\n",
    "        long_ticks = []\n",
    "        close_ticks = []\n",
    "        stay_ticks = []        \n",
    "        for i, tick in enumerate(window_ticks):\n",
    "            if self._position_history[i] == TradingEnvAction.SELL.value:\n",
    "                short_ticks.append(tick)\n",
    "            elif self._position_history[i] == TradingEnvAction.BUY.value:\n",
    "                long_ticks.append(tick)\n",
    "            elif self._position_history[i] == TradingEnvAction.CLOSE.value:\n",
    "                close_ticks.append(tick)\n",
    "            elif self._position_history[i] == TradingEnvAction.STAY.value:\n",
    "                stay_ticks.append(tick)\n",
    "\n",
    "        \n",
    "        plt.plot(short_ticks, self.prices[short_ticks], 'rv')\n",
    "        plt.plot(long_ticks, self.prices[long_ticks], 'g^')\n",
    "        plt.plot(close_ticks, self.prices[close_ticks], 'bo')\n",
    "        plt.plot(stay_ticks, self.prices[stay_ticks], 'yo')        \n",
    "\n",
    "        plt.suptitle(\n",
    "            \"Total Reward: %.6f\" % self._total_reward + ' ~ ' +\n",
    "            \"Total Profit: %.6f\" % self._total_profit\n",
    "        )\n",
    "        \n",
    "        \n",
    "    def close(self):\n",
    "        plt.close()\n",
    "\n",
    "\n",
    "    def save_rendering(self, filepath):\n",
    "        plt.savefig(filepath)\n",
    "\n",
    "\n",
    "    def pause_rendering(self):\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "    def _process_data(self):\n",
    "        raise NotImplementedError\n",
    "\n",
    "\n",
    "    def _calculate_reward(self, action):\n",
    "        step_reward = 0\n",
    "        # print(\"p:\",TradingEnvAction(self._position),\"a:\",TradingEnvAction(action),\"-1:\",TradingEnvAction(self._position_history[-1]),\"c:\",list(filter(None,self._position_history)))\n",
    "        trade = False\n",
    "        if (self._position != None and action != None):\n",
    "            trade = True\n",
    "\n",
    "        if trade:\n",
    "\n",
    "            current_price = self.prices[self._current_tick]\n",
    "            last_trade_price = self.prices[self._last_trade_tick]\n",
    "            price_diff = current_price - last_trade_price\n",
    "            # print(action,self._position_history[-1:],price_diff)\n",
    "            if list(filter(None,self._position_history)) == []:\n",
    "                step_reward = -abs(price_diff)\n",
    "\n",
    "            if (action == TradingEnvAction.BUY.value) or (action == TradingEnvAction.SELL.value):\n",
    "                step_reward = abs(price_diff)\n",
    "\n",
    "            if (action == TradingEnvAction.STAY.value) and (self._position_history[-1] == TradingEnvAction.BUY.value) and (current_price > last_trade_price ):\n",
    "                step_reward += abs(price_diff)/15\n",
    "\n",
    "            if (action == TradingEnvAction.STAY.value) and (self._position_history[-1] == TradingEnvAction.SELL.value) and (current_price < last_trade_price ):\n",
    "                step_reward += abs(price_diff)/15\n",
    "\n",
    "            if (action == TradingEnvAction.CLOSE.value) and (self._position_history[-1] == TradingEnvAction.SELL.value) and (current_price > last_trade_price ):\n",
    "                step_reward += abs(price_diff)/15\n",
    "\n",
    "            if (action == TradingEnvAction.CLOSE.value) and (self._position_history[-1] == TradingEnvAction.BUY.value) and (current_price < last_trade_price ):\n",
    "                step_reward += abs(price_diff)/15                                            \n",
    "\n",
    "            if (action == TradingEnvAction.STAY.value) and (self._position_history[-1] == TradingEnvAction.CLOSE.value):\n",
    "                step_reward += -abs(price_diff) \n",
    "            \n",
    "            if (action == TradingEnvAction.STAY.value) and (self._position_history[-1] == TradingEnvAction.STAY.value) and ((current_price < self.prices[-2]) or (current_price > self.prices[-2])):\n",
    "                step_reward += abs(price_diff) \n",
    "\n",
    "            if (action == TradingEnvAction.STAY.value) and (self._position_history[-1] == TradingEnvAction.STAY.value):\n",
    "                step_reward += abs(price_diff) \n",
    "\n",
    "    \n",
    "        return step_reward\n",
    "\n",
    "\n",
    "    def _update_profit(self, action):\n",
    "        trade = False\n",
    "        if (self._position != None and action != None):\n",
    "            trade = True\n",
    "\n",
    "        if trade or self._done:\n",
    "            current_price = self.prices[self._current_tick]\n",
    "            last_trade_price = self.prices[self._last_trade_tick]\n",
    "\n",
    "            if self._position == TradingEnvAction.BUY.value:\n",
    "                shares = (self._total_profit * (1 - self.trade_fee_ask_percent)) / last_trade_price\n",
    "                self._total_profit = (shares * (1 - self.trade_fee_bid_percent)) * current_price\n",
    "\n",
    "\n",
    "\n",
    "    def max_possible_profit(self):\n",
    "        self.trade_fee = 0.0003  # unit\n",
    "        current_tick = self._start_tick\n",
    "        last_trade_tick = current_tick - 1\n",
    "        profit = 1.\n",
    "\n",
    "        while current_tick <= self._end_tick:\n",
    "            position = None\n",
    "            if self.prices[current_tick] < self.prices[current_tick - 1]:\n",
    "                while (current_tick <= self._end_tick and\n",
    "                       self.prices[current_tick] < self.prices[current_tick - 1]):\n",
    "                    current_tick += 1\n",
    "                position = TradingEnvAction.SELL.value\n",
    "            else:\n",
    "                while (current_tick <= self._end_tick and\n",
    "                       self.prices[current_tick] >= self.prices[current_tick - 1]):\n",
    "                    current_tick += 1\n",
    "                position = TradingEnvAction.BUY.value\n",
    "\n",
    "            current_price = self.prices[current_tick - 1]\n",
    "            last_trade_price = self.prices[last_trade_tick]\n",
    "\n",
    "            if self._position_history[-1] == TradingEnvAction.CLOSE.value:\n",
    "                if position == TradingEnvAction.SELL.value:\n",
    "                    quantity = profit * (last_trade_price - self.trade_fee)\n",
    "                    profit = quantity / current_price\n",
    "\n",
    "            elif self._position_history[-1] == TradingEnvAction.STAY.value:\n",
    "                if position == TradingEnvAction.BUY.value:\n",
    "                    quantity = profit / last_trade_price\n",
    "                    profit = quantity * (current_price - self.trade_fee)\n",
    "\n",
    "            last_trade_tick = current_tick - 1\n",
    "\n",
    "        return profit"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# My Work Environment MultiDiscrete Action Space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "from copy import deepcopy\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "from gymnasium.utils import seeding\n",
    "from enum import Enum\n",
    "from sklearn.preprocessing import StandardScaler, PowerTransformer\n",
    "import talib\n",
    "\n",
    "\n",
    "    \n",
    "class MyForexEnv(gym.Env):\n",
    "    metadata = {\"render_modes\": [\"human\"]}\n",
    "    \n",
    "    def __init__(self,  df, window_size, frame_bound):\n",
    "        assert len(frame_bound) == 2\n",
    "        super().__init__()\n",
    "        \n",
    "        self.frame_bound = frame_bound\n",
    "        # self.seed()\n",
    "        self.df = df\n",
    "        self.window_size = window_size\n",
    "        self.prices, self.signal_features = self._process_data()\n",
    "        self.shape = (window_size, self.signal_features.shape[1])\n",
    "\n",
    "        # spaces\n",
    "        # Inf should be a large enough upper bound\n",
    "        INF = 1e9\n",
    "        self.observation_space = spaces.Box(low=-INF, \n",
    "                                            high=INF,\n",
    "                                            shape=self.shape, \n",
    "                                            dtype=np.float64)\n",
    "        self.action_space = spaces.MultiDiscrete([2, 2, 4, 2])\n",
    "\n",
    "        # episode\n",
    "        self._start_tick = self.window_size\n",
    "        # self._start_tick = 0\n",
    "        self._end_tick = len(self.prices) - 1\n",
    "        \n",
    "        self._current_tick = self._start_tick\n",
    "        self._last_trade_tick = None\n",
    "        self._done = None\n",
    "        \n",
    "        self._position = None\n",
    "        self._position_history = None\n",
    "        self._total_reward = None\n",
    "        self._total_profit = None\n",
    "        \n",
    "        self._first_rendering = None\n",
    "        \n",
    "        self.trade_history = []\n",
    "        self.trade_fee = 0.0003 \n",
    "        self._seed()\n",
    "        \n",
    "        # self.reset()\n",
    "        # self.step([0, 0, 13, 13])\n",
    "        # self.step([1,  0, 10, 14])\n",
    "    #    print(\"self._position_history:\", self._position_history)\n",
    "        # self.step(0)\n",
    "        # self.step(0)\n",
    "        # self.step(1)\n",
    "        # self.step(1)\n",
    "        # self.step(1)\n",
    "        # self.step(0)\n",
    "   \n",
    "    def _process_data(self):\n",
    "        # prices = self.df.loc[:, 'Close'].to_numpy()\n",
    "        sma = self.df.loc[:, 'SMA'].to_numpy()\n",
    "        start, end = self.frame_bound\n",
    "        sma = sma[start-self.window_size:end]\n",
    "        diff = np.diff(sma, prepend=0)\n",
    "        signal_features = np.column_stack((sma, diff))\n",
    "        return sma, signal_features\n",
    "    \n",
    "    def _seed(self, seed=None):\n",
    "        self.np_random, seed = seeding.np_random(seed)\n",
    "        return [seed]     \n",
    "    \n",
    "    def reset(self, seed=None, options=None):\n",
    "        super().reset(seed=seed)\n",
    "        print(\"\\nreset\")\n",
    "        self._done = False\n",
    "        self._total_reward = 0\n",
    "        self._total_profit = 1\n",
    "        \n",
    "        self._first_rendering = True\n",
    "        self._first_time = True\n",
    "        \n",
    "        self.history = {}\n",
    "        observation = self._get_observation()\n",
    "        info = dict(\n",
    "            total_reward = self._total_reward,\n",
    "            total_profit = self._total_profit,\n",
    "        )\n",
    "        self.existing_trade = None\n",
    "        return observation, info\n",
    "\n",
    "\n",
    "    def step(self, action):\n",
    "        # print(\"action:\", action)\n",
    "        self._done = False\n",
    "        has_traded = self._update_profit(action)\n",
    "        if has_traded:\n",
    "            self._current_tick = self._get_updated_current_tick()\n",
    "        else:\n",
    "            self._current_tick += 1\n",
    "        \n",
    "        step_reward, episode_ended = self._calculate_reward(action)\n",
    "        self._total_reward += step_reward\n",
    "        info = dict(\n",
    "            total_reward = self._total_reward,\n",
    "            total_profit = self._total_profit,\n",
    "        )\n",
    "\n",
    "        observation = self._get_observation()\n",
    "        # The experiment is done when _current_tick is at a point in the data stream \n",
    "        # where it cannot look forward window_size ticks\n",
    "        # If that is the case, the reset the _current_tick to the start_tick\n",
    "        if self._current_tick == self._end_tick:\n",
    "            self._done = True\n",
    "            self._current_tick = self._start_tick\n",
    "            # clear self.trade_history list\n",
    "            self.trade_history = []\n",
    "            print(\"done\")\n",
    "        \n",
    "            \n",
    "        return observation, step_reward, episode_ended, self._done, info\n",
    "\n",
    "    def _get_updated_current_tick(self):\n",
    "        my_ticker = self._current_tick\n",
    "        if self.existing_trade:\n",
    "            tp_sl_hit = False\n",
    "            while (not tp_sl_hit) and (my_ticker < self._end_tick):\n",
    "                existing_trade_position = self.existing_trade['trade_position']\n",
    "                existing_tp = self.existing_trade['take_profit']\n",
    "                existing_sl = self.existing_trade['stop_loss']\n",
    "                entry_price = self.existing_trade['entry_price']\n",
    "                current_price = self.prices[my_ticker]\n",
    "                \n",
    "                price_diff = current_price - entry_price\n",
    "                # if existing_trade_position is 1, and current_price triggers take_profit or stop_loss\n",
    "                if (existing_trade_position == 1) and (current_price >= existing_tp or current_price <= existing_sl): \n",
    "                    # check if current_price is close to take_profit than stop_loss\n",
    "                    tp_sl_hit = True\n",
    "                elif (existing_trade_position == 0) and (current_price <= existing_tp or current_price >= existing_sl):\n",
    "                    tp_sl_hit = True\n",
    "                else:\n",
    "                    my_ticker += 1\n",
    "                \n",
    "        return my_ticker\n",
    "\n",
    "\n",
    "    def _calculate_reward(self, action):\n",
    "        actual_state, trade_position, take_profit, stop_loss = action\n",
    "        current_price = self.prices[self._current_tick]\n",
    "        # initialize step reward to 0\n",
    "        step_reward = 0\n",
    "        episode_ended = False  \n",
    "\n",
    "        ## EXISTING TRADE STATE\n",
    "        if self.existing_trade:\n",
    "            existing_trade_position = self.existing_trade['trade_position']\n",
    "            existing_tp = self.existing_trade['take_profit']\n",
    "            existing_sl = self.existing_trade['stop_loss']\n",
    "            entry_price = self.existing_trade['entry_price']\n",
    "            \n",
    "            \n",
    "            price_diff = current_price - entry_price\n",
    "            # if existing_trade_position is 1, and current_price triggers take_profit or stop_loss\n",
    "            if existing_trade_position == 1: \n",
    "                # check if current_price is close to take_profit than stop_loss\n",
    "                if current_price >= existing_tp:\n",
    "                    step_reward += price_diff * 10000\n",
    "                    episode_ended = True\n",
    "                    self.existing_trade = None\n",
    "                    \n",
    "                elif current_price <= existing_sl:\n",
    "                    # step_reward += (10000 + price_diff) * 0.01\n",
    "                    episode_ended = True\n",
    "                    self.existing_trade = None\n",
    "                    \n",
    "            else:\n",
    "                if current_price <= existing_tp:\n",
    "                    step_reward += price_diff * 10000\n",
    "                    episode_ended = True\n",
    "                    self.existing_trade = None\n",
    "                    \n",
    "                elif current_price >= existing_sl:\n",
    "                    # step_reward += price_diff * 15000\n",
    "                    # step_reward += (10000 + price_diff) * 0.01\n",
    "                    episode_ended = True\n",
    "                    self.existing_trade = None\n",
    "        \n",
    "        # print(\"\\nstep_reward:\", step_reward)     \n",
    "        # print(\"action:\", action)\n",
    "        return step_reward, episode_ended\n",
    "    \n",
    "    \n",
    "    def _update_profit(self, action):\n",
    "        actual_state, trade_position, take_profit, stop_loss = action\n",
    "        has_traded = False\n",
    "                    \n",
    "        if actual_state == 1 and self.existing_trade is None:\n",
    "            current_price = self.prices[self._current_tick]\n",
    "            delta = 0.05\n",
    "            if trade_position == 1:\n",
    "                # if there is a buy trade\n",
    "                take_profit_price = current_price + ((take_profit + 1) * delta)\n",
    "                stop_loss_price = current_price - ((stop_loss + 1) * delta)\n",
    "            else:\n",
    "                # if there is a sell trade\n",
    "                take_profit_price = current_price - ((take_profit + 1) * delta)\n",
    "                stop_loss_price = current_price + ((stop_loss + 1) * delta)    \n",
    "            \n",
    "            self.existing_trade = {\n",
    "                'trade_position': trade_position,\n",
    "                'take_profit': take_profit_price,\n",
    "                'stop_loss': stop_loss_price,\n",
    "                'entry_price': current_price,\n",
    "                'trade_tick': self._current_tick,\n",
    "            }\n",
    "            print(\"action:\", action)\n",
    "            # print(\"total_profit:\", self._total_profit)\n",
    "            print(\"existing_trade:\", self.existing_trade)\n",
    "            has_traded = True\n",
    "            # add existing trade to history\n",
    "            self.trade_history.append(self.existing_trade)\n",
    "            \n",
    "        \n",
    "        # if self.existing_trade:\n",
    "        #     existing_trade_position = self.existing_trade['trade_position']\n",
    "        #     existing_tp = self.existing_trade['take_profit']\n",
    "        #     existing_sl = self.existing_trade['stop_loss']\n",
    "        #     entry_price = self.existing_trade['entry_price']\n",
    "        #     current_price = self.prices[self._current_tick]\n",
    "            \n",
    "        #     # if existing_trade_position is 1, and current_price triggers take_profit or stop_loss\n",
    "        #     percent_price_diff = (current_price - entry_price)/ entry_price\n",
    "        #     if existing_trade_position == 1: \n",
    "        #         if current_price >= existing_tp:\n",
    "        #             self._total_profit += percent_price_diff\n",
    "        #             self.existing_trade = None\n",
    "        #         elif current_price <= existing_sl:\n",
    "        #             self._total_profit += percent_price_diff\n",
    "        #             self.existing_trade = None\n",
    "        #     else:\n",
    "        #         if current_price <= existing_tp:\n",
    "        #             self._total_profit += percent_price_diff\n",
    "        #             self.existing_trade = None\n",
    "        #         elif current_price >= existing_sl:\n",
    "        #             self._total_profit += percent_price_diff\n",
    "        #             self.existing_trade = None\n",
    "        return has_traded\n",
    "       \n",
    "        \n",
    "        \n",
    "    def _get_observation(self):\n",
    "        observation = self.signal_features[(self._current_tick-self.window_size+1):self._current_tick+1]\n",
    "        return observation\n",
    "\n",
    "\n",
    "    \n",
    "env = MyForexEnv(\n",
    "       df = deepcopy(df),\n",
    "       window_size = 500,\n",
    "       frame_bound = (500, 700),\n",
    "    #    frame_bound = (700,  1_000),\n",
    "    #    frame_bound = (7_000, 15_000),\n",
    ")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# My Work Environment Discrete Action Space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "from copy import deepcopy\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "from gymnasium.utils import seeding\n",
    "from enum import Enum\n",
    "from sklearn.preprocessing import StandardScaler, PowerTransformer\n",
    "import talib\n",
    "\n",
    "class TradingEnvAction(Enum):\n",
    "    NEUTRAL = 0\n",
    "    BUY = 1\n",
    "    SELL = 2\n",
    "\n",
    "class TradingPositions(Enum):\n",
    "    NEUTRAL = 0\n",
    "    LONG = 1\n",
    "    SHORT = 2\n",
    "    \n",
    "\n",
    "class MyForexEnv(gym.Env):\n",
    "    metadata = {\"render_modes\": [\"human\"]}\n",
    "    \n",
    "    def __init__(self,  df, window_size, frame_bound):\n",
    "        assert len(frame_bound) == 2\n",
    "        super().__init__()\n",
    "        \n",
    "        self.frame_bound = frame_bound\n",
    "        # self.seed()\n",
    "        self.df = df\n",
    "        self.window_size = window_size\n",
    "        self.prices, self.signal_features = self._process_data()\n",
    "        self.shape = (window_size, self.signal_features.shape[1])\n",
    "\n",
    "        # spaces\n",
    "        # Inf should be a large enough upper bound\n",
    "        INF = 1e9\n",
    "        self.action_space = spaces.Discrete(len(TradingEnvAction))\n",
    "        self.observation_space = spaces.Box(low=-INF, \n",
    "                                            high=INF,\n",
    "                                            shape=self.shape, \n",
    "                                            dtype=np.float64)\n",
    "        \n",
    "        # episode\n",
    "        self._start_tick = self.window_size\n",
    "        self._end_tick = len(self.prices) - 1\n",
    "        self._neutral_counter = 0\n",
    "        \n",
    "        self._current_tick = self._start_tick\n",
    "        self._last_trade_tick = None\n",
    "        self._done = None\n",
    "        \n",
    "        self._position = None\n",
    "        self._position_history = None\n",
    "        self._total_reward = None\n",
    "        self._total_profit = None\n",
    "        \n",
    "        self._first_rendering = None\n",
    "        \n",
    "        self.trade_history = []\n",
    "        self.trade_fee = 0.0003 \n",
    "        self._seed()\n",
    "        \n",
    "        # self.step([0, 0, 13, 13])\n",
    "        # self.step([1,  0, 10, 14])\n",
    "    #    print(\"self._position_history:\", self._position_history)\n",
    "        # self.reset()\n",
    "        # self.step(0)\n",
    "        # self.step(0)\n",
    "        # self.step(1)\n",
    "        # self.step(1)\n",
    "        # self.step(1)\n",
    "        # self.step(0)\n",
    "   \n",
    "    def _process_data(self):\n",
    "        # prices = self.df.loc[:, 'Close'].to_numpy()\n",
    "        sma = self.df.loc[:, 'SMA'].to_numpy()\n",
    "        start, end = self.frame_bound\n",
    "        sma = sma[start-self.window_size:end]\n",
    "        diff = np.diff(sma, prepend=0)\n",
    "        signal_features = np.column_stack((sma, diff))\n",
    "        return sma, signal_features\n",
    "    \n",
    "    def _get_observation(self):\n",
    "        return self.signal_features[(self._current_tick-self.window_size):self._current_tick]\n",
    "\n",
    "    def _seed(self, seed=None):\n",
    "        self.np_random, seed = seeding.np_random(seed)\n",
    "        return [seed]     \n",
    "    \n",
    "    def reset(self, seed=None, options=None):\n",
    "        super().reset(seed=seed)\n",
    "        # print(\"\\nreset\")\n",
    "        self._done = False\n",
    "        self._total_reward = 0.0\n",
    "        self._total_profit = 1.0\n",
    "        self._first_rendering = True\n",
    "        self._position = TradingEnvAction.NEUTRAL.value\n",
    "        self._position_history = []\n",
    "        self.history = {}\n",
    "        observation = self._get_observation()\n",
    "        info = dict(\n",
    "            total_reward = self._total_reward,\n",
    "            total_profit = self._total_profit,\n",
    "        )\n",
    "        return observation, info\n",
    "\n",
    "\n",
    "    def step(self, action):\n",
    "        print(\"action:\", action)\n",
    "        self._done = False\n",
    "        \n",
    "        step_reward, episode_ended = self._calculate_reward(action)\n",
    "        self._total_reward += step_reward\n",
    "        \n",
    "        # has_traded = self._update_profit(action)\n",
    "      \n",
    "        \n",
    "        info = dict(\n",
    "            total_reward = self._total_reward,\n",
    "            total_profit = self._total_profit,\n",
    "        )\n",
    "        observation = self._get_observation()\n",
    "        self._current_tick += 1\n",
    "        if self._current_tick == self._end_tick:\n",
    "            self._done = True\n",
    "            self._current_tick = self._start_tick\n",
    "            self._neutral_counter = 0\n",
    "            \n",
    "            \n",
    "        return observation, step_reward, episode_ended, self._done, info\n",
    "\n",
    "\n",
    "    def _calculate_reward1(self, action):\n",
    "        # Append the current action to the position history\n",
    "        self._position_history.append(action)\n",
    "\n",
    "        # Define some constants\n",
    "        NEUTRAL = 0 # Action and state of having no position\n",
    "        BUY = 1 # Action to open a long position\n",
    "        SELL = 2 # Action to open a short position\n",
    "        ALPHA = 0.01 # A small positive constant to avoid taking log of zero\n",
    "\n",
    "        # Get the current and previous prices\n",
    "        current_price = self.prices[self._current_tick]\n",
    "        \n",
    "        # Check if self._current_tick is 0\n",
    "        if self._current_tick == 0:\n",
    "            # Assign a default value to previous_price, such as 1\n",
    "            previous_price = 1\n",
    "            # Alternatively, assign a zero reward for the first time step\n",
    "            # reward = 0.\n",
    "            # return float(reward), episode_ended\n",
    "        else:\n",
    "            # Get the previous price from the prices array\n",
    "            previous_price = self.prices[self._current_tick - 1]\n",
    "\n",
    "        # Calculate the price change ratio\n",
    "        ratio = current_price / previous_price\n",
    "\n",
    "        # Initialize the reward to zero\n",
    "        reward = 0.\n",
    "        episode_ended = False\n",
    "\n",
    "        # Get the previous action and state\n",
    "        previous_action = self._position_history[-2] if len(self._position_history) > 1 else TradingEnvAction.NEUTRAL.value\n",
    "        previous_state = self._position\n",
    "\n",
    "        # Reward for staying neutral\n",
    "        if action == NEUTRAL and previous_action == NEUTRAL:\n",
    "            reward += 0.01\n",
    "\n",
    "        # Reward for opening a long or short position\n",
    "        if action == BUY and previous_action == NEUTRAL:\n",
    "            reward += np.log(ratio + ALPHA)\n",
    "            self._position = BUY # Update the state to long\n",
    "            self._entry_price = current_price # Record the entry price\n",
    "        if action == SELL and previous_action == NEUTRAL:\n",
    "            reward += np.log(1 / ratio + ALPHA)\n",
    "            self._position = SELL # Update the state to short\n",
    "            self._entry_price = current_price # Record the entry price\n",
    "\n",
    "        # Reward for maintaining a long or short position\n",
    "        if action == BUY and previous_action == BUY:\n",
    "            reward += np.log(ratio + ALPHA)\n",
    "            self._position = BUY # Update the state to long (no change)\n",
    "        if action == SELL and previous_action == SELL:\n",
    "            reward += np.log(1 / ratio + ALPHA)\n",
    "            self._position = SELL # Update the state to short (no change)\n",
    "\n",
    "        # Reward for closing a long or short position\n",
    "        if action == NEUTRAL and previous_action == BUY:\n",
    "            reward += np.log(ratio + ALPHA)\n",
    "            self._position = NEUTRAL # Update the state to neutral\n",
    "            if current_price > self._entry_price: # Check if the trade was profitable\n",
    "                reward += np.log(ratio / (self._entry_price / previous_price) + ALPHA) # Give additional reward for profit\n",
    "            else:\n",
    "                reward -= np.log(ratio / (self._entry_price / previous_price) + ALPHA) # Give negative reward for loss\n",
    "            episode_ended = True # End the episode after closing the trade\n",
    "        if action == NEUTRAL and previous_action == SELL:\n",
    "            reward += np.log(1 / ratio + ALPHA)\n",
    "            self._position = NEUTRAL # Update the state to neutral\n",
    "            if current_price < self._entry_price: # Check if the trade was profitable\n",
    "                reward += np.log((self._entry_price / previous_price) / ratio + ALPHA) # Give additional reward for profit\n",
    "            else:\n",
    "                reward -= np.log((self._entry_price / previous_price) / ratio + ALPHA) # Give negative reward for loss\n",
    "            episode_ended = True # End the episode after closing the trade\n",
    "\n",
    "        return float(reward), episode_ended \n",
    "\n",
    "    def _calculate_reward(self, action):\n",
    "        # Append the current action to the position history\n",
    "        self._position_history.append(action)\n",
    "\n",
    "        # Define some constants\n",
    "        NEUTRAL = 0 # Action and state of having no position\n",
    "        BUY = 1 # Action to open a long position\n",
    "        SELL = 2 # Action to open a short position\n",
    "        ALPHA = 0.01 # A small positive constant to avoid taking log of zero\n",
    "        MAX_TRADE_STEPS = 10 # A maximum number of time steps that the agent can stay in trade\n",
    "\n",
    "        # Get the current and previous prices\n",
    "        current_price = self.prices[self._current_tick]\n",
    "        \n",
    "        # Check if self._current_tick is 0\n",
    "        if self._current_tick == 0:\n",
    "            # Assign a default value to previous_price, such as 1\n",
    "            previous_price = 1\n",
    "            # Alternatively, assign a zero reward for the first time step\n",
    "            # reward = 0.\n",
    "            # return float(reward), episode_ended\n",
    "        else:\n",
    "            # Get the previous price from the prices array\n",
    "            previous_price = self.prices[self._current_tick - 1]\n",
    "\n",
    "        # Calculate the price change ratio\n",
    "        ratio = current_price / previous_price\n",
    "\n",
    "        # Initialize the reward to zero\n",
    "        reward = 0.\n",
    "        episode_ended = False\n",
    "\n",
    "        # Get the previous action and state\n",
    "        previous_action = self._position_history[-2] if len(self._position_history) > 1 else TradingEnvAction.NEUTRAL.value\n",
    "        previous_state = self._position\n",
    "\n",
    "        # Reward for staying neutral\n",
    "        if action == NEUTRAL and previous_action == NEUTRAL:\n",
    "            reward += 0.01\n",
    "\n",
    "            # Penalize for giving too many 000s\n",
    "            self._neutral_counter += 1 # Increment the counter for consecutive 000s\n",
    "            reward -= 0.001 * self._neutral_counter # Subtract a small amount from the reward for each 000\n",
    "        else:\n",
    "            self._neutral_counter = 0 # Reset the counter when the agent takes a different action\n",
    "\n",
    "        # Reward for opening a long or short position\n",
    "        if action == BUY and previous_action == NEUTRAL:\n",
    "            reward += np.log(ratio + ALPHA)\n",
    "            self._position = BUY # Update the state to long\n",
    "            self._entry_price = current_price # Record the entry price\n",
    "            self._trade_counter = 1 # Initialize the counter for consecutive trade steps\n",
    "        if action == SELL and previous_action == NEUTRAL:\n",
    "            reward += np.log(1 / ratio + ALPHA)\n",
    "            self._position = SELL # Update the state to short\n",
    "            self._entry_price = current_price # Record the entry price\n",
    "            self._trade_counter = 1 # Initialize the counter for consecutive trade steps\n",
    "\n",
    "        # Reward for maintaining a long or short position\n",
    "        if action == BUY and previous_action == BUY:\n",
    "            reward += np.log(ratio + ALPHA)\n",
    "            self._position = BUY # Update the state to long (no change)\n",
    "            \n",
    "            # Penalize for staying in trade for too long\n",
    "            self._trade_counter += 1 # Increment the counter for consecutive trade steps\n",
    "            reward -= 0.001 * self._trade_counter # Subtract a small amount from the reward for each trade step\n",
    "            \n",
    "            # Check if the agent has exceeded the maximum number of trade steps\n",
    "            if self._trade_counter > MAX_TRADE_STEPS:\n",
    "                reward -= 0.5 # Give a large negative reward for exceeding the limit\n",
    "                episode_ended = True # End the episode after exceeding the limit\n",
    "\n",
    "        if action == SELL and previous_action == SELL:\n",
    "            reward += np.log(1 / ratio + ALPHA)\n",
    "            self._position = SELL # Update the state to short (no change)\n",
    "\n",
    "            # Penalize for staying in trade for too long\n",
    "            self._trade_counter += 1 # Increment the counter for consecutive trade steps\n",
    "            reward -= 0.001 * self._trade_counter # Subtract a small amount from the reward for each trade step\n",
    "            \n",
    "            # Check if the agent has exceeded the maximum number of trade steps\n",
    "            if self._trade_counter > MAX_TRADE_STEPS:\n",
    "                reward -= 0.5 # Give a large negative reward for exceeding the limit\n",
    "                episode_ended = True # End the episode after exceeding the limit\n",
    "\n",
    "        # Reward for closing a long or short position\n",
    "        if action == NEUTRAL and previous_action == BUY:\n",
    "            reward += np.log(ratio + ALPHA)\n",
    "            self._position = NEUTRAL # Update the state to neutral\n",
    "            if current_price > self._entry_price: # Check if the trade was profitable\n",
    "                reward += np.log(ratio / (self._entry_price / previous_price) + ALPHA) # Give additional reward for profit\n",
    "            else:\n",
    "                reward -= np.log(ratio / (self._entry_price / previous_price) + ALPHA) # Give negative reward for loss\n",
    "            episode_ended = True # End the episode after closing the trade\n",
    "            self._trade_counter = 0 # Reset the counter when the agent closes the trade\n",
    "        if action == NEUTRAL and previous_action == SELL:\n",
    "            reward += np.log(1 / ratio + ALPHA)\n",
    "            self._position = NEUTRAL # Update the state to neutral\n",
    "            if current_price < self._entry_price: # Check if the trade was profitable\n",
    "                reward += np.log((self._entry_price / previous_price) / ratio + ALPHA) # Give additional reward for profit\n",
    "            else:\n",
    "                reward -= np.log((self._entry_price / previous_price) / ratio + ALPHA) # Give negative reward for loss\n",
    "            episode_ended = True # End the episode after closing the trade\n",
    "            self._trade_counter = 0 # Reset the counter when the agent closes the trade\n",
    "\n",
    "        # Penalize for switching from a long to a short position, or vice versa, without closing the trade first\n",
    "        if action == SELL and previous_action == BUY:\n",
    "            reward -= 0.5 # Give a negative reward for switching positions\n",
    "            self._position = SELL # Update the state to short\n",
    "            self._entry_price = current_price # Record the new entry price\n",
    "            self._trade_counter = 1 # Initialize the counter for consecutive trade steps\n",
    "        if action == BUY and previous_action == SELL:\n",
    "            reward -= 0.5 # Give a negative reward for switching positions\n",
    "            self._position = BUY # Update the state to long\n",
    "            self._entry_price = current_price # Record the new entry price\n",
    "            self._trade_counter = 1 # Initialize the counter for consecutive trade steps\n",
    "\n",
    "        return float(reward), episode_ended \n",
    "\n",
    "\n",
    "    \n",
    "env = MyForexEnv(\n",
    "       df = deepcopy(df),\n",
    "       window_size = 500,\n",
    "       frame_bound = (500, 700),\n",
    "    #    frame_bound = (700,  1_000),\n",
    "    #    frame_bound = (7_000, 15_000),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sb3_contrib import RecurrentPPO\n",
    "from stable_baselines3.common.callbacks import CheckpointCallback\n",
    "from stable_baselines3.common.callbacks import EvalCallback, StopTrainingOnNoModelImprovement\n",
    "import torch as th\n",
    "\n",
    "\n",
    "# Save a checkpoint every 1000 steps\n",
    "checkpoint_callback = CheckpointCallback(\n",
    "  save_freq=10_000,\n",
    "  save_path=\"./logs/\",\n",
    "  name_prefix=\"trade_rl_model\",\n",
    "  save_replay_buffer=True,\n",
    "  save_vecnormalize=True,\n",
    ")\n",
    "\n",
    "policy_kwargs = dict(activation_fn=th.nn.ReLU,\n",
    "                     net_arch=dict(pi=[128, 128], vf=[128, 128]))\n",
    "\n",
    "# policy_kwargs = dict(net_arch=[ dict(vf=[128, 128], pi=[64, 64])])\n",
    "\n",
    "# policy_kwargs = dict( net_arch=dict(pi=[32, 32], vf=[32, 32]))\n",
    "model = RecurrentPPO('MlpLstmPolicy', env, \n",
    "                    #  verbose=1,  \n",
    "                    #  n_steps=256, \n",
    "                    #  gamma=0.95, \n",
    "                    #  n_epochs=500, \n",
    "                    #  target_kl=0.001, \n",
    "                    #  learning_rate=0.01,\n",
    "                     tensorboard_log=\"./trade_env_tensorboard/\")\n",
    "model.learn(total_timesteps=1_000_000,  \n",
    "            log_interval=10, \n",
    "            tb_log_name=\"trade_name\",\n",
    "            callback=checkpoint_callback,\n",
    "            )\n",
    "\n",
    "# model = PPO('MlpPolicy', env, verbose=1)\n",
    "# model.learn(total_timesteps=1_000_000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3 import PPO\n",
    "\n",
    "model = PPO(\"MlpPolicy\", env, verbose=1)\n",
    "model.learn(total_timesteps=1_000_000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sb3_contrib import RecurrentPPO\n",
    "\n",
    "# Load the model from a zip file\n",
    "model = RecurrentPPO.load(\"/Users/newuser/Projects/robust-algo-trader/envs/logs/trade_rl_model_1000000_steps.zip\", env=env)\n",
    "\n",
    "model.learn(total_timesteps=10_000_000,  \n",
    "            log_interval=10, \n",
    "            tb_log_name=\"trade_name\",\n",
    "            callback=checkpoint_callback,\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "observation, info = env.reset(seed=0)\n",
    "episode_count = 0\n",
    "\n",
    "while True:\n",
    "    action, _state = model.predict(observation)\n",
    "    # action = env.action_space.sample()\n",
    "    # print(action)\n",
    "    observation, reward, terminated, truncated, info = env.step(action)\n",
    "    if terminated:\n",
    "        episode_count += 1\n",
    "    # observation, reward, done, info = env.step(action)\n",
    "    # env.render()\n",
    "    if truncated:\n",
    "        print(\"info:\", info)\n",
    "        break\n",
    "print(\"episode_count:\", episode_count)\n",
    "# plt.cla()\n",
    "# env.render_all()\n",
    "# plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EUR/USD Dataset Viz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "df['SMA'].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import talib\n",
    "\n",
    "real_df = pd.read_table('/Users/newuser/Projects/robust-algo-trader/data/EURUSD/EURUSD_H1_200702210000_202304242100.tsv')\n",
    "\n",
    "df = real_df.copy()\n",
    "# take only last 7000 rows\n",
    "df = df.iloc[:1_000]\n",
    "# df = df.iloc[-7_00:]\n",
    "\n",
    "# remove the following columns <TICKVOL>, <VOL> and <SPREAD>\n",
    "df = df.drop(['<TICKVOL>', '<VOL>', '<SPREAD>'], axis=1)\n",
    "# rename the columns\n",
    "df = df.rename(columns={'<DATE>': 'Date', \n",
    "                                '<TIME>': 'Time', \n",
    "                                '<OPEN>': 'Open', \n",
    "                                '<HIGH>': 'High', \n",
    "                                '<LOW>': 'Low', \n",
    "                                '<CLOSE>': 'Close'\n",
    "                                })\n",
    "# combine the date and time columns\n",
    "df['Date_Time'] = df['Date'] + ' ' + df['Time']\n",
    "# convert the date_time column to datetime\n",
    "df['Date_Time'] = pd.to_datetime(df['Date_Time'], format='%Y%m%d %H:%M:%S.%f')\n",
    "# remove the date and time columns\n",
    "df = df.drop(['Date', 'Time'], axis=1)\n",
    "# Rename Date_Time to Time\n",
    "df = df.rename(columns={'Date_Time': 'Time'})\n",
    "df.index = df['Time']\n",
    "# remove the Time column\n",
    "df = df.drop(['Time'], axis=1)\n",
    "\n",
    "\n",
    "prices = df[\"Close\"].values\n",
    "df[\"SMA\"] = talib.SMA(prices, timeperiod=200)\n",
    "df[\"EMA\"] = talib.EMA(prices, timeperiod=200)\n",
    "# df['TEMA'] = talib.TRIMA(prices, timeperiod=200*2)\n",
    "\n",
    "df['ATR'] = talib.NATR(df['High'], df['Low'], df['Close'], timeperiod=200)\n",
    "# df[\"EMA\"] = ema\n",
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.iloc[-7_0:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Close'].plot()\n",
    "df['SMA'].plot()\n",
    "\n",
    "# df['EMA'].plot()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['SMA'].plot()\n",
    "\n",
    "# add a new column namely smoothed SMA data\n",
    "# df['smoothed_sma'] = df['SMA'].rolling(window=300).mean()\n",
    "# df['smoothed_sma'].plot()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Close'].plot()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define the x-coordinates\n",
    "x = np.linspace(0, 2*np.pi, 100)\n",
    "\n",
    "# Evaluate the function y = sin(x)\n",
    "y = np.sin(x)\n",
    "\n",
    "# Calculate the gradient using numpy.gradient\n",
    "grad = np.gradient(y, x)\n",
    "\n",
    "# Find the indices where the gradient is close to zero (within a tolerance)\n",
    "tol = 0.01 # You can adjust this value as needed\n",
    "zero_grad_indices = np.where(np.abs(grad) < tol)\n",
    "\n",
    "# Plot the curve and its gradient\n",
    "plt.plot(x, y, label='y = sin(x)')\n",
    "plt.plot(x, grad, label=\"y' = cos(x)\")\n",
    "\n",
    "# Plot the dots on where the gradient is close to zero\n",
    "plt.plot(x[zero_grad_indices], y[zero_grad_indices], 'ro', label='zero gradient points')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler, PowerTransformer\n",
    "\n",
    "scaler = StandardScaler()\n",
    "copy_df = df.copy()\n",
    "\n",
    "# scale all columns in copy_df \n",
    "copy_df['SMA'] = scaler.fit_transform(copy_df['SMA'].values.reshape(-1, 1))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "copy_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "copy_df['SMA'].plot()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Define the parameters of the sine wave\n",
    "frequency = 0.2 # cycles per day\n",
    "amplitude = 0.3 # peak value\n",
    "phase = 0 # initial angle\n",
    "offset = 1 # vertical shift\n",
    "\n",
    "# Define the time range and interval\n",
    "start_time = \"2022-01-01 00:00:00\" # start date and time\n",
    "end_time = \"2023-01-01 00:00:00\" # start date and time\n",
    "# end_time = \"2023-01-31 23:00:00\" # end date and time\n",
    "time_interval = \"1H\" # hourly interval\n",
    "\n",
    "# Create a time index using pandas date_range function\n",
    "time_index = pd.date_range(start_time, end_time, freq=time_interval)\n",
    "\n",
    "# Convert the time index to radians using numpy pi and timedelta function\n",
    "time_radians = 2 * np.pi * frequency * (time_index - time_index[0]) / pd.Timedelta(days=1) + phase\n",
    "\n",
    "# Calculate the sine values using numpy sin function and the parameters\n",
    "sine_values = amplitude * np.sin(time_radians) + offset\n",
    "\n",
    "# Create a dataframe using pandas DataFrame function and the time index and sine values\n",
    "df = pd.DataFrame({\"Time\": time_index, \"SMA\": sine_values})\n",
    "\n",
    "# Set the Time column as the index column using pandas set_index function\n",
    "df = df.set_index(\"Time\")\n",
    "\n",
    "df['SMA'].plot()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['SMA'].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sma = df.loc[:, 'SMA'].to_numpy()\n",
    "sma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sma = df.loc[:, 'SMA'].to_numpy()\n",
    "frame_bound = (10, 500)\n",
    "window_size = 10\n",
    "sma = sma[frame_bound[0] - window_size:frame_bound[1]]\n",
    "diff = np.insert(np.diff(sma), 0, 0)\n",
    "signal_features = np.column_stack((sma, diff))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(signal_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use iloc instead of loc to access the SMA column by index\n",
    "sma = df.loc[:, 'SMA'].to_numpy()\n",
    "# Unpack the frame_bound tuple into two variables\n",
    "start, end = (10, 500)\n",
    "window_size = 10\n",
    "# Use negative indexing to avoid computing the start index of the slice\n",
    "sma = sma[-window_size + start:end]\n",
    "# Use np.diff with prepend option instead of np.insert\n",
    "diff = np.diff(sma, prepend=0)\n",
    "# Use np.c_ instead of np.column_stack for readability\n",
    "signal_features = np.column_stack((sma, diff))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "signal_features_df = pd.DataFrame(signal_features, columns=[\"SMA\", \"diff\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 0\n",
    "window_size = 10\n",
    "signal_features_0_to_100 = signal_features[(index):(index + window_size)]\n",
    "signal_features_0_to_100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "signal_features_df\n",
    "\n",
    "window = 10\n",
    "index = window_size + 0\n",
    "\n",
    "filtered_df = signal_features_df.iloc[index-window:window-1:-1]\n",
    "\n",
    "# filtered_df = signal_features_df.iloc[index:window_size:-1]\n",
    "\n",
    "filtered_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_current_tick = 0\n",
    "observation = signal_features[(_current_tick-window_size+1):_current_tick+1]\n",
    "observation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "observation = signal_features[-window_size:1]\n",
    "observation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import treetensor.torch as ttorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cmath import inf\n",
    "from typing import Any, List\n",
    "from easydict import EasyDict\n",
    "from abc import abstractmethod\n",
    "from collections import namedtuple\n",
    "from enum import Enum\n",
    "from pprint import pprint\n",
    "from copy import deepcopy\n",
    "import os\n",
    "import copy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "from gymnasium.utils import seeding\n",
    "import torch\n",
    "import numbers\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def to_ndarray(item: Any, dtype: np.dtype = None):\n",
    "    def transform(d):\n",
    "        if dtype is None:\n",
    "            return np.array(d)\n",
    "        else:\n",
    "            return np.array(d, dtype=dtype)\n",
    "\n",
    "    if isinstance(item, dict):\n",
    "        new_data = {}\n",
    "        for k, v in item.items():\n",
    "            new_data[k] = to_ndarray(v, dtype)\n",
    "        return new_data\n",
    "    elif isinstance(item, list) or isinstance(item, tuple):\n",
    "        if len(item) == 0:\n",
    "            return None\n",
    "        elif isinstance(item[0], numbers.Integral) or isinstance(item[0], numbers.Real):\n",
    "            return transform(item)\n",
    "        elif hasattr(item, '_fields'):  # namedtuple\n",
    "            return type(item)(*[to_ndarray(t, dtype) for t in item])\n",
    "        else:\n",
    "            new_data = []\n",
    "            for t in item:\n",
    "                new_data.append(to_ndarray(t, dtype))\n",
    "            return new_data\n",
    "    elif isinstance(item, torch.Tensor):\n",
    "        if dtype is None:\n",
    "            return item.numpy()\n",
    "        else:\n",
    "            return item.numpy().astype(dtype)\n",
    "    elif isinstance(item, np.ndarray):\n",
    "        if dtype is None:\n",
    "            return item\n",
    "        else:\n",
    "            return item.astype(dtype)\n",
    "    elif isinstance(item, bool) or isinstance(item, str):\n",
    "        return item\n",
    "    elif np.isscalar(item):\n",
    "        return np.array(item)\n",
    "    elif item is None:\n",
    "        return None\n",
    "    else:\n",
    "        raise TypeError(\"not support item type: {}\".format(type(item)))\n",
    "\n",
    "\n",
    "def load_dataset(name, index_name):\n",
    "    df = pd.read_csv(name, parse_dates=True, index_col=index_name)\n",
    "    return df\n",
    "\n",
    "class Actions(int, Enum):\n",
    "    DOUBLE_SELL = 0\n",
    "    SELL = 1\n",
    "    HOLD = 2\n",
    "    BUY = 3\n",
    "    DOUBLE_BUY = 4\n",
    "\n",
    "class Positions(int, Enum):\n",
    "    SHORT = -1.\n",
    "    FLAT = 0.\n",
    "    LONG = 1.\n",
    "\n",
    "def transform(position, action):\n",
    "    if action == Actions.SELL:\n",
    "        if position == Positions.LONG:\n",
    "            return Positions.FLAT, False\n",
    "\n",
    "        if position == Positions.FLAT:\n",
    "            return Positions.SHORT, True\n",
    "\n",
    "    if action == Actions.BUY:\n",
    "        if position == Positions.SHORT:\n",
    "            return Positions.FLAT, False\n",
    "\n",
    "        if position == Positions.FLAT:\n",
    "            return Positions.LONG, True\n",
    "\n",
    "    if action == Actions.DOUBLE_SELL and (position == Positions.LONG or position == Positions.FLAT):\n",
    "        return Positions.SHORT, True\n",
    "\n",
    "    if action == Actions.DOUBLE_BUY and (position == Positions.SHORT or position == Positions.FLAT):\n",
    "        return Positions.LONG, True\n",
    "\n",
    "    return position, False\n",
    "\n",
    "\n",
    "class TradingEnv(gym.Env):\n",
    "\n",
    "    def __init__(self, cfg):\n",
    "\n",
    "        self._cfg = cfg\n",
    "        self._env_id = cfg.env_id\n",
    "        #======== param to plot =========\n",
    "        self.cnt = 0\n",
    "\n",
    "        if 'plot_freq' not in self._cfg:\n",
    "            self.plot_freq = 10\n",
    "        else:\n",
    "            self.plot_freq = self._cfg.plot_freq\n",
    "        if 'save_path' not in self._cfg:\n",
    "            self.save_path = './'\n",
    "        else:\n",
    "            self.save_path = self._cfg.save_path\n",
    "\n",
    "        #================================\n",
    "        self.train_range = cfg.train_range\n",
    "        self.test_range = cfg.test_range\n",
    "        self.window_size = cfg.window_size\n",
    "        self.prices = None\n",
    "        self.signal_features = None\n",
    "        self.feature_dim_len = None\n",
    "        self.shape = (cfg.window_size, 3)\n",
    "\n",
    "        #======== param about episode =========\n",
    "        # self._end_tick = 0\n",
    "        self._current_tick = None\n",
    "        self._done = None\n",
    "        self._last_trade_tick = None\n",
    "        self._position = None\n",
    "        self._position_history = None\n",
    "        self._total_reward = None\n",
    "        #======================================\n",
    "        self._init_flag = True\n",
    "        \n",
    "        # ====== load Google stocks data =======\n",
    "        raw_data = load_dataset(self._cfg.stocks_data_filename, 'Date')\n",
    "        self.raw_prices = raw_data.loc[:, 'Close'].to_numpy()\n",
    "        self.df = deepcopy(raw_data)\n",
    "        \n",
    "        EPS = 1e-10\n",
    "        if self.train_range == None or self.test_range == None:\n",
    "            self.df = self.df.apply(lambda x: (x - x.mean()) / (x.std() + EPS), axis=0)\n",
    "        else:\n",
    "            boundary = int(len(self.df) * self.train_range)\n",
    "            train_data = raw_data[:boundary].copy()\n",
    "            boundary = int(len(raw_data) * (1 + self.test_range))\n",
    "            test_data = raw_data[boundary:].copy()\n",
    "\n",
    "            train_data = train_data.apply(lambda x: (x - x.mean()) / (x.std() + EPS), axis=0)\n",
    "            test_data = test_data.apply(lambda x: (x - x.mean()) / (x.std() + EPS), axis=0)\n",
    "            self.df.loc[train_data.index, train_data.columns] = train_data\n",
    "            self.df.loc[test_data.index, test_data.columns] = test_data\n",
    "        # ======================================\n",
    "        # set cost\n",
    "        self.trade_fee_bid_percent = 0.01  # unit\n",
    "        self.trade_fee_ask_percent = 0.005  # unit\n",
    "        \n",
    "        # self.prices, self.signal_features, self.feature_dim_len = self._process_data()\n",
    "        start_idx = 0\n",
    "        self.prices, self.signal_features, self.feature_dim_len = self._process_data()\n",
    "        self._start_tick = cfg.window_size\n",
    "        self._end_tick = len(self.prices) - 1\n",
    "        self.shape = (self.window_size, self.feature_dim_len)\n",
    "        self._action_space = spaces.Discrete(len(Actions))\n",
    "        self._observation_space = spaces.Box(low=-np.inf, high=np.inf, shape=self.shape, dtype=np.float64)\n",
    "        self._reward_space = spaces.Box(-inf, inf, shape=(1, ), dtype=np.float32)\n",
    "        self._done = False\n",
    "        self._seed()\n",
    "        \n",
    "        # self.reset()\n",
    "        # self.step(0)\n",
    "        # self.step(0)\n",
    "        # self.step(0)\n",
    "        # self.step(1)\n",
    "        # self.step(1)\n",
    "        # self.step(1)\n",
    "        # self.step(0)\n",
    "    \n",
    "    def _seed(self, seed=None):\n",
    "        self.np_random, seed = seeding.np_random(seed)\n",
    "        return [seed]          \n",
    "\n",
    "    # def seed(self, seed, dynamic_seed):\n",
    "    #     self._seed = seed\n",
    "    #     self._dynamic_seed = dynamic_seed\n",
    "    #     np.random.seed(self._seed)\n",
    "    #     self.np_random, seed = seeding.np_random(seed)\n",
    "\n",
    "    def reset(self, seed=None, options=None):\n",
    "        super().reset(seed=seed)\n",
    "        print(\"\\nreset\")\n",
    "        self.cnt += 1\n",
    "        self._current_tick = self._start_tick\n",
    "        self._last_trade_tick = self._current_tick - 1\n",
    "        self._position = Positions.FLAT\n",
    "        self._position_history = [self._position]\n",
    "        self._profit_history = [1.]\n",
    "        self._total_reward = 0.\n",
    "        \n",
    "        info = dict(\n",
    "            total_reward = self._total_reward,\n",
    "            total_profit = self._profit_history,\n",
    "        )\n",
    "        obs = self._get_observation()\n",
    "\n",
    "        return obs, info\n",
    "\n",
    "    def random_action(self):\n",
    "        return np.array([self.action_space.sample()])\n",
    "\n",
    "    def step(self, action):\n",
    "        print('step')\n",
    "        print('action', action)\n",
    "        # assert isinstance(action, np.ndarray), type(action)\n",
    "        # if action.shape == (1, ):\n",
    "        #     action = action.item()  # 0-dim array\n",
    "\n",
    "        self._done = False\n",
    "        self._current_tick += 1\n",
    "\n",
    "        if self._current_tick >= self._end_tick:\n",
    "            self._done = True\n",
    "\n",
    "        step_reward = self._calculate_reward(action)\n",
    "        self._total_reward += step_reward\n",
    "        self._position, trade = transform(self._position, action)\n",
    "\n",
    "        if trade:\n",
    "            self._last_trade_tick = self._current_tick\n",
    "            print('trade')\n",
    "\n",
    "        self._position_history.append(self._position)\n",
    "        self._profit_history.append(float(np.exp(self._total_reward)))\n",
    "        observation = self._get_observation()\n",
    "        info = dict(\n",
    "            total_reward=self._total_reward,\n",
    "            position=self._position.value,\n",
    "        )\n",
    "\n",
    "        if self._done:\n",
    "            if self._env_id[-1] == 'e' and self.cnt % self.plot_freq == 0:\n",
    "                self.render()\n",
    "            info['max_possible_profit'] = np.log(self.max_possible_profit())\n",
    "            info['eval_episode_return'] = self._total_reward\n",
    "\n",
    "        # step_reward = to_ndarray([step_reward]).astype(np.float32)\n",
    "        print('done', self._done)\n",
    "        return observation, step_reward,  False, self._done, info\n",
    "        # return BaseEnvTimestep(observation, step_reward, self._done, info)\n",
    "\n",
    "    def _get_observation(self):\n",
    "        # sig_features = to_ndarray(\n",
    "        #                  ).reshape(-1).astype(np.float32)\n",
    "        sig_features = self.signal_features[(self._current_tick - self.window_size+1):self._current_tick + 1]\n",
    "\n",
    "        # tick = (self._current_tick - self._last_trade_tick) / self._cfg.eps_length\n",
    "        # obs = np.hstack([sig_features, to_ndarray([self._position.value]), to_ndarray([tick])])\n",
    "        return sig_features\n",
    "\n",
    "    # override\n",
    "    def _process_data(self, start_idx = None):\n",
    "        all_feature_name = ['Close', 'Open', 'High', 'Low', 'Adj Close', 'Volume']\n",
    "        all_feature = {k: self.df.loc[:, k].to_numpy() for k in all_feature_name}\n",
    "        # add feature \"Diff\"\n",
    "        prices = self.df.loc[:, 'Close'].to_numpy()\n",
    "        diff = np.insert(np.diff(prices), 0, 0)\n",
    "        all_feature_name.append('Diff')\n",
    "        all_feature['Diff'] = diff\n",
    "        # =================================\n",
    "\n",
    "        # you can select features you want\n",
    "        selected_feature_name = ['Close', 'Diff', 'Volume']\n",
    "        selected_feature = np.column_stack([all_feature[k] for k in selected_feature_name])\n",
    "        feature_dim_len = len(selected_feature_name)\n",
    "\n",
    "        # validate index\n",
    "        if start_idx is None:\n",
    "            if self.train_range == None or self.test_range == None:\n",
    "                self.start_idx = np.random.randint(self.window_size, len(self.df) - self._cfg.eps_length)\n",
    "            elif self._env_id[-1] == 'e':\n",
    "                boundary = int(len(self.df) * (1 + self.test_range))\n",
    "                assert len(self.df) - self._cfg.eps_length > boundary + self.window_size,\\\n",
    "                 \"parameter test_range is too large!\"\n",
    "                self.start_idx = np.random.randint(boundary + self.window_size, len(self.df) - self._cfg.eps_length)\n",
    "            else:\n",
    "                boundary = int(len(self.df) * self.train_range)\n",
    "                assert boundary - self._cfg.eps_length > self.window_size,\\\n",
    "                 \"parameter test_range is too small!\"\n",
    "                self.start_idx = np.random.randint(self.window_size, boundary - self._cfg.eps_length)\n",
    "        else:\n",
    "            self.start_idx = start_idx\n",
    "\n",
    "        self._start_tick = self.start_idx\n",
    "        self._end_tick = self._start_tick + self._cfg.eps_length - 1\n",
    "\n",
    "        return prices, selected_feature, feature_dim_len\n",
    "    \n",
    "   \n",
    "    # override\n",
    "    def _calculate_reward(self, action):\n",
    "        step_reward = 0.\n",
    "        current_price = (self.raw_prices[self._current_tick])\n",
    "        last_trade_price = (self.raw_prices[self._last_trade_tick])\n",
    "        ratio = current_price / last_trade_price\n",
    "        cost = np.log((1 - self.trade_fee_ask_percent) * (1 - self.trade_fee_bid_percent))\n",
    "\n",
    "        if action == Actions.BUY and self._position == Positions.SHORT:\n",
    "            step_reward = np.log(2 - ratio) + cost\n",
    "\n",
    "        if action == Actions.SELL and self._position == Positions.LONG:\n",
    "            step_reward = np.log(ratio) + cost\n",
    "\n",
    "        if action == Actions.DOUBLE_SELL and self._position == Positions.LONG:\n",
    "            step_reward = np.log(ratio) + cost\n",
    "\n",
    "        if action == Actions.DOUBLE_BUY and self._position == Positions.SHORT:\n",
    "            step_reward = np.log(2 - ratio) + cost\n",
    "\n",
    "        step_reward = float(step_reward)\n",
    "        return step_reward \n",
    "\n",
    "    # def render(self):\n",
    "    #     import matplotlib.pyplot as plt\n",
    "    #     plt.clf()\n",
    "    #     plt.xlabel('trading days')\n",
    "    #     plt.ylabel('profit')\n",
    "    #     plt.plot(self._profit_history)\n",
    "    #     plt.savefig(self.save_path + str(self._env_id) + \"-profit.png\")\n",
    "\n",
    "    #     plt.clf()\n",
    "    #     plt.xlabel('trading days')\n",
    "    #     plt.ylabel('close price')\n",
    "    #     window_ticks = np.arange(len(self._position_history))\n",
    "    #     eps_price = self.raw_prices[self._start_tick:self._end_tick + 1]\n",
    "    #     plt.plot(eps_price)\n",
    "\n",
    "    #     short_ticks = []\n",
    "    #     long_ticks = []\n",
    "    #     flat_ticks = []\n",
    "    #     for i, tick in enumerate(window_ticks):\n",
    "    #         if self._position_history[i] == Positions.SHORT:\n",
    "    #             short_ticks.append(tick)\n",
    "    #         elif self._position_history[i] == Positions.LONG:\n",
    "    #             long_ticks.append(tick)\n",
    "    #         else:\n",
    "    #             flat_ticks.append(tick)\n",
    "\n",
    "    #     plt.plot(long_ticks, eps_price[long_ticks], 'g^', markersize=3, label=\"Long\")\n",
    "    #     plt.plot(flat_ticks, eps_price[flat_ticks], 'bo', markersize=3, label=\"Flat\")\n",
    "    #     plt.plot(short_ticks, eps_price[short_ticks], 'rv', markersize=3, label=\"Short\")\n",
    "    #     plt.legend(loc='upper left', bbox_to_anchor=(0.05, 0.95))\n",
    "    #     plt.savefig(self.save_path + str(self._env_id) + '-price.png')\n",
    "\n",
    "    # def close(self):\n",
    "    #     import matplotlib.pyplot as plt\n",
    "    #     plt.close()\n",
    "\n",
    "    # # override\n",
    "    # def create_collector_env_cfg(cfgs):\n",
    "    #     \"\"\"\n",
    "    #     Overview:\n",
    "    #         Return a list of all of the environment from input config, used in env manager \\\n",
    "    #         (a series of vectorized env), and this method is mainly responsible for envs collecting data.\n",
    "    #         In TradingEnv, this method will rename every env_id and generate different config.\n",
    "    #     Arguments:\n",
    "    #         - cfg (:obj:`dict`): Original input env config, which needs to be transformed into the type of creating \\\n",
    "    #             env instance actually and generated the corresponding number of configurations.\n",
    "    #     Returns:\n",
    "    #         - env_cfg_list (:obj:`List[dict]`): List of ``cfg`` including all the config collector envs.\n",
    "    #     .. note::\n",
    "    #         Elements(env config) in collector_env_cfg/evaluator_env_cfg can be different, such as server ip and port.\n",
    "    #     \"\"\"\n",
    "    #     collector_env_num = cfg.pop('collector_env_num')\n",
    "    #     collector_env_cfg = [copy.deepcopy(cfg) for _ in range(collector_env_num)]\n",
    "    #     for i in range(collector_env_num):\n",
    "    #         collector_env_cfg[i]['env_id'] += ('-' + str(i) + 'e')\n",
    "    #     return collector_env_cfg\n",
    "\n",
    "    # # override\n",
    "    # def create_evaluator_env_cfg(cfg):\n",
    "    #     \"\"\"\n",
    "    #     Overview:\n",
    "    #         Return a list of all of the environment from input config, used in env manager \\\n",
    "    #         (a series of vectorized env), and this method is mainly responsible for envs evaluating performance.\n",
    "    #         In TradingEnv, this method will rename every env_id and generate different config.\n",
    "    #     Arguments:\n",
    "    #         - cfg (:obj:`dict`): Original input env config, which needs to be transformed into the type of creating \\\n",
    "    #             env instance actually and generated the corresponding number of configurations.\n",
    "    #     Returns:\n",
    "    #         - env_cfg_list (:obj:`List[dict]`): List of ``cfg`` including all the config evaluator envs.\n",
    "    #     \"\"\"\n",
    "    #     evaluator_env_num = cfg.pop('evaluator_env_num')\n",
    "    #     evaluator_env_cfg = [copy.deepcopy(cfg) for _ in range(evaluator_env_num)]\n",
    "    #     for i in range(evaluator_env_num):\n",
    "    #         evaluator_env_cfg[i]['env_id'] += ('-' + str(i) + 'e')\n",
    "    #     return evaluator_env_cfg\n",
    "\n",
    "    # # override\n",
    "    # def max_possible_profit(self):\n",
    "    #     current_tick = self._start_tick\n",
    "    #     last_trade_tick = current_tick - 1\n",
    "    #     profit = 1.\n",
    "\n",
    "    #     while current_tick <= self._end_tick:\n",
    "    #         if self.raw_prices[current_tick] < self.raw_prices[current_tick - 1]:\n",
    "    #             while (current_tick <= self._end_tick\n",
    "    #                    and self.raw_prices[current_tick] < self.raw_prices[current_tick - 1]):\n",
    "    #                 current_tick += 1\n",
    "\n",
    "    #             current_price = self.raw_prices[current_tick - 1]\n",
    "    #             last_trade_price = self.raw_prices[last_trade_tick]\n",
    "    #             tmp_profit = profit * (2 - (current_price / last_trade_price)) * (1 - self.trade_fee_ask_percent\n",
    "    #                                                                               ) * (1 - self.trade_fee_bid_percent)\n",
    "    #             profit = max(profit, tmp_profit)\n",
    "    #         else:\n",
    "    #             while (current_tick <= self._end_tick\n",
    "    #                    and self.raw_prices[current_tick] >= self.raw_prices[current_tick - 1]):\n",
    "    #                 current_tick += 1\n",
    "\n",
    "    #             current_price = self.raw_prices[current_tick - 1]\n",
    "    #             last_trade_price = self.raw_prices[last_trade_tick]\n",
    "    #             tmp_profit = profit * (current_price / last_trade_price) * (1 - self.trade_fee_ask_percent\n",
    "    #                                                                         ) * (1 - self.trade_fee_bid_percent)\n",
    "    #             profit = max(profit, tmp_profit)\n",
    "    #         last_trade_tick = current_tick - 1\n",
    "    #     return profit\n",
    "\n",
    "    @property\n",
    "    def observation_space(self):\n",
    "        return self._observation_space\n",
    "\n",
    "    @property\n",
    "    def action_space(self):\n",
    "        return self._action_space\n",
    "\n",
    "    # @property\n",
    "    # def reward_space(self):\n",
    "    #     return self._reward_space\n",
    "\n",
    "    # def __repr__(self):\n",
    "    #     return \"Trading Env\"\n",
    "    \n",
    "\n",
    "\n",
    "from easydict import EasyDict\n",
    "\n",
    "stocks_dqn_config = dict(\n",
    "    exp_name='stocks_dqn_seed0',\n",
    "    env=dict(\n",
    "        # Whether to use shared memory. Only effective if \"env_manager_type\" is 'subprocess'\n",
    "        # Env number respectively for collector and evaluator.\n",
    "        collector_env_num=8,\n",
    "        evaluator_env_num=8,\n",
    "        env_id='stocks-v0',\n",
    "        n_evaluator_episode=8,\n",
    "        stop_value=2,\n",
    "        # one trading year.\n",
    "        eps_length=253,\n",
    "        # associated with the feature length.\n",
    "        window_size=20,\n",
    "        # the path to save result image.\n",
    "        save_path='./fig/',\n",
    "        # the raw data file name\n",
    "        # stocks_data_filename='STOCKS_GOOGL',\n",
    "        stocks_data_filename='/Users/newuser/Projects/robust-algo-trader/data/STOCKS_GOOGL.csv',\n",
    "        # the stocks range percentage used by train/test.\n",
    "        # if one of them is None, train & test set will use all data by default.\n",
    "        train_range=None,\n",
    "        test_range=None,\n",
    "    ),\n",
    "    policy=dict(\n",
    "        # Whether to use cuda for network.\n",
    "        cuda=True,\n",
    "        model=dict(\n",
    "            obs_shape=62,\n",
    "            action_shape=5,\n",
    "            encoder_hidden_size_list=[128],\n",
    "            head_layer_num=1,\n",
    "            # Whether to use dueling head.\n",
    "            dueling=True,\n",
    "        ),\n",
    "        # Reward's future discount factor, aka. gamma.\n",
    "        discount_factor=0.99,\n",
    "        # How many steps in td error.\n",
    "        nstep=5,\n",
    "        # learn_mode config\n",
    "        learn=dict(\n",
    "            update_per_collect=10,\n",
    "            batch_size=64,\n",
    "            learning_rate=0.001,\n",
    "            # Frequency of target network update.\n",
    "            target_update_freq=100,\n",
    "            ignore_done=True,\n",
    "        ),\n",
    "        # collect_mode config\n",
    "        collect=dict(\n",
    "            # You can use either \"n_sample\" or \"n_episode\" in collector.collect.\n",
    "            # Get \"n_sample\" samples per collect.\n",
    "            n_sample=64,\n",
    "            # Cut trajectories into pieces with length \"unroll_len\".\n",
    "            unroll_len=1,\n",
    "        ),\n",
    "        # command_mode config\n",
    "        other=dict(\n",
    "            # Epsilon greedy with decay.\n",
    "            eps=dict(\n",
    "                # Decay type. Support ['exp', 'linear'].\n",
    "                type='exp',\n",
    "                start=0.95,\n",
    "                end=0.1,\n",
    "                decay=50000,\n",
    "            ),\n",
    "            replay_buffer=dict(replay_buffer_size=100000, )\n",
    "        ),\n",
    "    ),\n",
    ")\n",
    "main_config = EasyDict(stocks_dqn_config)\n",
    "env = TradingEnv(main_config.env)\n",
    "\n",
    "# from stable_baselines3 import PPO\n",
    "\n",
    "# model = PPO(\"MlpPolicy\", env, verbose=1)\n",
    "# model.learn(total_timesteps=1_000_000)\n",
    "# from sb3_contrib import RecurrentPPO\n",
    "\n",
    "# model = RecurrentPPO('MlpLstmPolicy', env, \n",
    "#                      tensorboard_log=\"./di_engine_trade_env_tensorboard/\")\n",
    "\n",
    "# model.learn(total_timesteps=1_000_000,  \n",
    "#             log_interval=10, \n",
    "#             tb_log_name=\"di_engine_trade\",\n",
    "#             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sb3_contrib import RecurrentPPO\n",
    "from stable_baselines3 import PPO\n",
    "\n",
    "\n",
    "model = RecurrentPPO('MlpLstmPolicy', env, \n",
    "                     tensorboard_log=\"./di_engine_trade_env_tensorboard/\")\n",
    "\n",
    "# model = PPO(\"MlpPolicy\", env, verbose=1)\n",
    "model.learn(total_timesteps=1_000_000,  \n",
    "            log_interval=10, \n",
    "            tb_log_name=\"di_engine_trade\",\n",
    "            )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs499f22",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
