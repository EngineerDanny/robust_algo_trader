{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IMPORT PACKAGES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gymnasium.envs.registration import register\n",
    "import gymnasium as gym\n",
    "from stable_baselines3 import *\n",
    "\n",
    "# from stable_baselines3 import A2C\n",
    "from sb3_contrib import RecurrentPPO\n",
    "from stable_baselines3.common.callbacks import CheckpointCallback\n",
    "from stable_baselines3.common.callbacks import EvalCallback, StopTrainingOnNoModelImprovement\n",
    "import torch as th\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "# from gymnasium.utils import seeding\n",
    "from enum import Enum\n",
    "import pandas as pd\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "from copy import deepcopy\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "from gymnasium.utils import seeding\n",
    "from enum import Enum\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"C:/Users/da2343/Projects/robust_algo_trader/data/EURUSD/trades_EURUSD_H1_2011_2022.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TradeUpdateEnv(gym.Env):\n",
    "    def __init__(self,  df, window_size, frame_bound):\n",
    "        assert len(frame_bound) == 2\n",
    "        assert frame_bound[0] >= window_size\n",
    "        assert frame_bound[1] > frame_bound[0]\n",
    "        \n",
    "        self.frame_bound = frame_bound\n",
    "        \n",
    "        # self.seed()\n",
    "        self.df = df\n",
    "        self.window_size = window_size\n",
    "        self.signal_features = self._process_data()\n",
    "        self.shape = (window_size, self.signal_features.shape[1])\n",
    "\n",
    "        # spaces\n",
    "        self.observation_space = spaces.Box(low=-100, \n",
    "                                            high=100,\n",
    "                                            shape=self.shape, \n",
    "                                            dtype=np.float64)\n",
    "        self.action_space = spaces.Discrete(2)\n",
    "\n",
    "        # episode\n",
    "        self._start_tick = self.window_size\n",
    "        self._current_tick = self._start_tick\n",
    "        self._last_final_tick = len(self.signal_features) - 1\n",
    "        self._end_tick = None\n",
    "        self._terminated = None\n",
    "        self._truncated = False\n",
    "        self._total_reward = 0\n",
    "        self._total_profit = None\n",
    "        self._prev_loss = False\n",
    "\n",
    "\n",
    "    def _process_data(self):\n",
    "        features_df = self.df[[\"position\", \"RSI\", \"ATR\", \"ADX\", \"AROON_Oscillator\", \"MFI\",\"label\"]]\n",
    "        # features_df = self.df[[\"position\", \"RSI\", \"label\"]]\n",
    "        features_df['label'] = features_df['label'].shift(1)\n",
    "        features_df = features_df.iloc[1:]\n",
    "        features_np = features_df.to_numpy()\n",
    "        _start_index = self.frame_bound[0] - self.window_size\n",
    "        _end_index = self.frame_bound[1]\n",
    "        signal_features = features_np[_start_index:_end_index]\n",
    "        return signal_features\n",
    "\n",
    "    def _get_observation(self):\n",
    "        _start_index = self._current_tick - self.window_size + 1\n",
    "        _end_index = self._current_tick + 1\n",
    "        _obs = self.signal_features[_start_index:_end_index]\n",
    "        self._current_tick += 1\n",
    "        return _obs\n",
    "\n",
    "\n",
    "    def reset(self, seed=None, options=None):\n",
    "        super().reset(seed=1)\n",
    "        self._terminated = False\n",
    "        self._truncated = False\n",
    "        self._prev_loss = False\n",
    "        # self._current_tick = np.random.randint(\n",
    "        #     self._start_tick, len(self.signal_features) - 50\n",
    "        # )\n",
    "        self._end_tick = self._current_tick + 30\n",
    "        observation = self._get_observation()\n",
    "        info = dict(total_reward = self._total_reward)\n",
    "        return observation, info\n",
    "\n",
    "\n",
    "    def step(self, action):\n",
    "        step_reward = self._calculate_reward(action)\n",
    "        self._total_reward += step_reward\n",
    "        info = dict(total_reward = self._total_reward)\n",
    "        observation = self._get_observation()\n",
    "        # set done flag to True if episode ended because of tick limit\n",
    "        if self._current_tick == self._end_tick:\n",
    "            self._terminated = True\n",
    "        # set truncated flag to True if episode ended because of tick limit\n",
    "        if self._current_tick == self._last_final_tick:\n",
    "            self._truncated = True\n",
    "            # reset the current tick to the start tick\n",
    "            self._current_tick = self._start_tick\n",
    "        return observation, step_reward, self._terminated, self._truncated, info\n",
    "\n",
    "\n",
    "    def _calculate_reward(self, action):\n",
    "        step_reward = 0\n",
    "        label_action = self.signal_features[self._current_tick+1][6]\n",
    "        # label_action = self.signal_features[self._current_tick+1][2]\n",
    "        # if action == label_action:\n",
    "        #     step_reward += 1\n",
    "    \n",
    "        if action == 1 and label_action == 0:\n",
    "            step_reward += -1\n",
    "            if self._prev_loss == True:\n",
    "                step_reward += -2\n",
    "            self._prev_loss = True\n",
    "            print(\"lost\")\n",
    "        else:\n",
    "            self._prev_loss = False\n",
    "        # else:\n",
    "            # step_reward += 0.1\n",
    "            # print(\"nothing\")\n",
    "            \n",
    "        if action == 1 and label_action == 1:\n",
    "            step_reward += 1.5\n",
    "            print(\"won\")\n",
    "            \n",
    "        return step_reward\n",
    "\n",
    "env = TradeUpdateEnv(\n",
    "       df = deepcopy(df),\n",
    "       window_size = 3,\n",
    "       frame_bound = (4, 3000)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save a checkpoint every 1000 steps\n",
    "checkpoint_callback = CheckpointCallback(\n",
    "  save_freq=10_000,\n",
    "  save_path=\"./log_10_17/\",\n",
    "  name_prefix=\"trade_drl_model\",\n",
    "  save_replay_buffer=True,\n",
    "  save_vecnormalize=True,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "# policy_kwargs = dict(activation_fn=th.nn.ReLU,\n",
    "#                      net_arch=dict(pi=[128, 128], vf=[128, 128]))\n",
    "\n",
    "# policy_kwargs = dict(\n",
    "#     net_arch=[256, 64, 64], # The number and size of the hidden layers\n",
    "#     activation_fn=th.nn.Tanh, # The activation function\n",
    "#     lstm=50 # The size of the LSTM cell\n",
    "# )\n",
    "\n",
    "policy_kwargs=dict(\n",
    "    net_arch=[256, 128, 128, 64],\n",
    "    activation_fn=th.nn.Tanh,\n",
    "          # lstm_hidden_size=64,\n",
    "      )\n",
    "\n",
    "model = RecurrentPPO('MlpLstmPolicy', \n",
    "                     env, \n",
    "                     verbose=1, \n",
    "                    gamma=0.01,\n",
    "                    gae_lambda = 0.9,\n",
    "                    #gamma=0,\n",
    "                    #n_epochs=1000,\n",
    "                    seed=1,                 \n",
    "                    #policy_kwargs = policy_kwargs, \n",
    "                    device=\"cuda\",\n",
    "                    tensorboard_log=\"./trade_env_tensorboard/\")\n",
    "# model.policy\n",
    "model.learn(total_timesteps=1_000_000_000,  \n",
    "            log_interval=10, \n",
    "            tb_log_name=\"trade_drl_experiment\",\n",
    "            callback=checkpoint_callback,\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sb3_contrib import RecurrentPPO\n",
    "\n",
    "# Load the model from a zip file\n",
    "saved_model = RecurrentPPO.load(\"C:/Users/da2343/Projects/robust_algo_trader/drl/logs/trade_drl_model_3580000_steps.zip\")\n",
    "saved_model\n",
    "\n",
    "# set the env of the saved_model\n",
    "saved_model.set_env(env, force_reset=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save a checkpoint every 1000 steps\n",
    "checkpoint_callback = CheckpointCallback(\n",
    "  save_freq=10_000,\n",
    "  save_path=\"./logs/\",\n",
    "  name_prefix=\"trade_drl_model_B\",\n",
    "  save_replay_buffer=True,\n",
    "  save_vecnormalize=True,\n",
    ")\n",
    "\n",
    "saved_model.learn(total_timesteps=1_000_000_000,  \n",
    "            log_interval=10, \n",
    "            tb_log_name=\"trade_drl_experiment\",\n",
    "            callback=checkpoint_callback,\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "# df = pd.read_csv(\"/Users/newuser/Projects/robust-algo-trader/envs/trades_2021.csv\")\n",
    "env = TradeUpdateEnv(\n",
    "       df = deepcopy(df),\n",
    "       window_size = 3,\n",
    "       frame_bound = (3000, 5000)\n",
    ")\n",
    "observation, info = env.reset()\n",
    "episode_count = 0\n",
    "step_count = 1\n",
    "step_reward_list = []\n",
    "print(\"got here\")\n",
    "lstm_states = None\n",
    "num_envs = 1\n",
    "# Episode start signals are used to reset the lstm states\n",
    "episode_starts = np.ones((num_envs,), dtype=bool)\n",
    "\n",
    "while True:\n",
    "    action, lstm_states = model.predict(observation,state=lstm_states, \n",
    "                                        episode_start=episode_starts, \n",
    "                                        deterministic=True)\n",
    "    # action = env.action_space.sample()\n",
    "    observation, reward, terminated, truncated, info = env.step(action)\n",
    "    # print(action)\n",
    "    print(reward)\n",
    "    step_reward_list.append({\n",
    "        \"step\": step_count,\n",
    "        \"test_reward\": reward,\n",
    "    })\n",
    "    step_count += 1\n",
    "    \n",
    "    # print(\"info:\", info)\n",
    "    if terminated:\n",
    "        episode_count += 1\n",
    "    # observation, reward, done, info = env.step(action)\n",
    "    # env.render()\n",
    "    if truncated:\n",
    "        print(\"info:\", info)\n",
    "        break\n",
    "print(\"episode_count:\", episode_count)\n",
    "\n",
    "test_reward_df = pd.DataFrame(step_reward_list) \n",
    "# add cumulative reward column\n",
    "test_reward_df[\"test_cum_reward\"] = test_reward_df[\"test_reward\"].cumsum()\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "# plot the cumulative reward against the step\n",
    "plt.plot(test_reward_df[\"step\"], test_reward_df[\"test_cum_reward\"])\n",
    "# show labels\n",
    "plt.xlabel(\"step\")\n",
    "plt.ylabel(\"test_cum_reward\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "# df = pd.read_csv(\"/Users/newuser/Projects/robust-algo-trader/envs/trades_2021.csv\")\n",
    "\n",
    "from sb3_contrib import RecurrentPPO\n",
    "\n",
    "# Load the model from a zip file\n",
    "# model = RecurrentPPO.load(\"/Users/newuser/Projects/robust-algo-trader/envs/logs/trade_drl_model_4100000_steps.zip\", env=env)\n",
    "\n",
    "env = TradeUpdateEnv(\n",
    "       df = deepcopy(df),\n",
    "       window_size = 3,\n",
    "    #    frame_bound = (4, 3000)\n",
    "       frame_bound = (4, 2004)\n",
    ")\n",
    "observation, info = env.reset()\n",
    "episode_count = 0\n",
    "step_count = 1\n",
    "step_reward_list = []\n",
    "print(\"got here\")\n",
    "lstm_states = None\n",
    "num_envs = 1\n",
    "# Episode start signals are used to reset the lstm states\n",
    "episode_starts = np.ones((num_envs,), dtype=bool)\n",
    "\n",
    "while True:\n",
    "    action, lstm_states = model.predict(observation,state=lstm_states, \n",
    "                                        episode_start=episode_starts, \n",
    "                                        deterministic=True)\n",
    "    # action = env.action_space.sample()\n",
    "    observation, reward, terminated, truncated, info = env.step(action)\n",
    "    print(reward)\n",
    "    # if reward != 0:\n",
    "    step_reward_list.append({\n",
    "        \"step\": step_count,\n",
    "        \"train_reward\": reward,\n",
    "    })\n",
    "    step_count += 1\n",
    "    \n",
    "    # print(\"info:\", info)\n",
    "    if terminated:\n",
    "        episode_count += 1\n",
    "    # observation, reward, done, info = env.step(action)\n",
    "    # env.render()\n",
    "    if truncated:\n",
    "        print(\"info:\", info)\n",
    "        break\n",
    "print(\"episode_count:\", episode_count)\n",
    "\n",
    "train_reward_df = pd.DataFrame(step_reward_list) \n",
    "# add cumulative reward column\n",
    "train_reward_df[\"train_cum_reward\"] = train_reward_df[\"train_reward\"].cumsum()\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "# plot the cumulative reward against the step\n",
    "plt.plot(train_reward_df[\"step\"], train_reward_df[\"train_cum_reward\"])\n",
    "# show labels\n",
    "plt.xlabel(\"step\")\n",
    "plt.ylabel(\"train_cum_reward\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_test_reward_df = pd.merge(train_reward_df, test_reward_df, on=\"step\", how=\"outer\")\n",
    "train_test_reward_df\n",
    "# choose first 1000 rows\n",
    "# train_test_reward_df = train_test_reward_df.iloc[40:50]\n",
    "\n",
    "# plot a graph of train_cum_reward and test_cum_reward against the step \n",
    "# give the train_cum_reward and test_cum_reward different colors\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming your dataframe is called df\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_test_reward_df.plot(x=\"step\", y=[\"train_cum_reward\", \"test_cum_reward\"], color=[\"red\", \"blue\"])\n",
    "plt.show()\n",
    "# save the plot as a png file\n",
    "plt.savefig(\"train_test_reward.png\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs499f22",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
