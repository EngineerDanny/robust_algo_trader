{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IMPORT PACKAGES\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gymnasium.envs.registration import register\n",
    "import gymnasium as gym\n",
    "from stable_baselines3 import *\n",
    "\n",
    "# from stable_baselines3 import A2C\n",
    "from sb3_contrib import RecurrentPPO\n",
    "from stable_baselines3.common.callbacks import CheckpointCallback\n",
    "from stable_baselines3.common.callbacks import EvalCallback, StopTrainingOnNoModelImprovement\n",
    "import torch as th\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "# from gymnasium.utils import seeding\n",
    "from enum import Enum\n",
    "import pandas as pd\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "from copy import deepcopy\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "from gymnasium.utils import seeding\n",
    "from enum import Enum\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"/Users/newuser/Projects/robust-algo-trader/data/trades_EURUSD_H1_2011_2022.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# Assuming your pandas dataframe is called df and your label column is called label\n",
    "# Import pandas and sklearn.metrics\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score, precision_score\n",
    "\n",
    "# Shift the label column upwards by one row and assign it to a new column called predicted\n",
    "df['predicted'] = df['label'].shift(1)\n",
    "\n",
    "# Drop the last row of the dataframe as it will have a missing value for predicted\n",
    "df = df.dropna()\n",
    "\n",
    "# Calculate the accuracy and precision scores between the label and predicted columns\n",
    "accuracy = accuracy_score(df['label'], df['predicted'])\n",
    "precision = precision_score(df['label'], df['predicted'])\n",
    "\n",
    "# Print the results\n",
    "print(f\"Accuracy score: {accuracy}\")\n",
    "print(f\"Precision score: {precision}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TradeUpdateEnv(gym.Env):\n",
    "    def __init__(self,  df, window_size, frame_bound):\n",
    "        assert len(frame_bound) == 2\n",
    "        assert frame_bound[0] >= window_size\n",
    "        assert frame_bound[1] > frame_bound[0]\n",
    "        \n",
    "        self.frame_bound = frame_bound\n",
    "        \n",
    "        # self.seed()\n",
    "        self.df = df\n",
    "        self.window_size = window_size\n",
    "        self.signal_features = self._process_data()\n",
    "        print(\"self.signal_features\")\n",
    "        print(self.signal_features)\n",
    "        \n",
    "        self.shape = (window_size, self.signal_features.shape[1])\n",
    "\n",
    "        # spaces\n",
    "        self.observation_space = spaces.Box(low=-100, \n",
    "                                            high=100,\n",
    "                                            shape=self.shape, \n",
    "                                            dtype=np.float64)\n",
    "        self.action_space = spaces.Discrete(2)\n",
    "\n",
    "        # episode\n",
    "        self._start_tick = self.window_size\n",
    "        self._current_tick = self._start_tick\n",
    "        self._last_final_tick = len(self.signal_features) - 1\n",
    "        self._end_tick = None\n",
    "        self._terminated = None\n",
    "        self._truncated = False\n",
    "        self._total_reward = 0\n",
    "        self._total_profit = None\n",
    "        self._prev_loss = False\n",
    "\n",
    "\n",
    "    def _process_data(self):\n",
    "        features_df = self.df[[\"position\", \"RSI\", \"ATR\", \"ADX\", \"AROON_Oscillator\", \"MFI\",\"label\"]]\n",
    "        # features_df = self.df[[\"position\", \"RSI\", \"label\"]]\n",
    "        # print(features_df)\n",
    "        features_df['label'] = features_df['label'].shift(1)\n",
    "        # print(features_df)\n",
    "        features_df = features_df.iloc[1:]\n",
    "        # print(features_df)\n",
    "        # print the last row\n",
    "        # print(features_df.iloc[-1])\n",
    "        features_np = features_df.to_numpy()\n",
    "        _start_index = self.frame_bound[0] - self.window_size\n",
    "        _end_index = self.frame_bound[1]\n",
    "        signal_features = features_np[_start_index:_end_index]\n",
    "        return signal_features\n",
    "\n",
    "    def _get_observation(self):\n",
    "        _start_index = self._current_tick - self.window_size + 1\n",
    "        _end_index = self._current_tick + 1\n",
    "        _obs = self.signal_features[_start_index:_end_index]\n",
    "        self._current_tick += 1\n",
    "        # print(\"self.signal_features\")\n",
    "        # print(self.signal_features)\n",
    "        return _obs\n",
    "\n",
    "\n",
    "    def reset(self, seed=None, options=None):\n",
    "        super().reset(seed=1)\n",
    "        self._terminated = False\n",
    "        self._truncated = False\n",
    "        self._prev_loss = False\n",
    "        self._end_tick = self._current_tick + 30\n",
    "        observation = self._get_observation()\n",
    "        info = {'total_reward': self._total_reward}\n",
    "        return observation, info\n",
    "\n",
    "\n",
    "    def step(self, action):\n",
    "        step_reward = self._calculate_reward(action)\n",
    "        self._total_reward += step_reward\n",
    "        info = dict(total_reward = self._total_reward)\n",
    "        # set done flag to True if episode ended because of tick limit\n",
    "        if self._current_tick == self._end_tick:\n",
    "            self._terminated = True\n",
    "        # set truncated flag to True if episode ended because of tick limit\n",
    "        if self._current_tick == self._last_final_tick:\n",
    "            self._truncated = True\n",
    "            # reset the current tick to the start tick\n",
    "            self._current_tick = self._start_tick\n",
    "        observation = self._get_observation()\n",
    "        return observation, step_reward, self._terminated, self._truncated, info\n",
    "\n",
    "\n",
    "    def _calculate_reward(self, action):\n",
    "        step_reward = 0\n",
    "        if self._current_tick + 1 < len(self.signal_features):\n",
    "            label_action = self.signal_features[self._current_tick + 1][6]\n",
    "            print(\"internal action\")\n",
    "            print(action)\n",
    "            \n",
    "            print(\"label_action\")\n",
    "            print(label_action)\n",
    "            \n",
    "            print(\"self._current_tick\")\n",
    "            print(self._current_tick)\n",
    "            \n",
    "            print(\"self.signal_features[self._current_tick + 1]\")\n",
    "            print(self.signal_features[self._current_tick + 1])\n",
    "            \n",
    "            # print(\"self.signal_features[self._current_tick+1]\")\n",
    "            # print(self.signal_features[self._current_tick+1])\n",
    "            \n",
    "            # label_action = self.signal_features[self._current_tick+1][2]\n",
    "            # if action == label_action:\n",
    "            #     step_reward += 1\n",
    "        \n",
    "            if action == 1 and label_action == 0:\n",
    "                step_reward += -1\n",
    "                if self._prev_loss == True:\n",
    "                    step_reward += -2\n",
    "                self._prev_loss = True\n",
    "                # print(\"lost\")\n",
    "            else:\n",
    "                self._prev_loss = False\n",
    "            # else:\n",
    "                # step_reward += 0.1\n",
    "                # print(\"nothing\")\n",
    "                \n",
    "            if action == 1 and label_action == 1:\n",
    "                step_reward += 1.5\n",
    "                # print(\"won\")\n",
    "            print(\"step_reward\")\n",
    "            print(step_reward)    \n",
    "        return step_reward\n",
    "\n",
    "# env = TradeUpdateEnv(\n",
    "#        df = deepcopy(df),\n",
    "#        window_size = 3,\n",
    "#        frame_bound = (4, 3000)\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save a checkpoint every 1000 steps\n",
    "checkpoint_callback = CheckpointCallback(\n",
    "  save_freq=10_000,\n",
    "  save_path=\"./log_10_17/\",\n",
    "  name_prefix=\"trade_drl_model\",\n",
    "  save_replay_buffer=True,\n",
    "  save_vecnormalize=True,\n",
    ")\n",
    "\n",
    "# policy_kwargs = dict(activation_fn=th.nn.ReLU,\n",
    "#                      net_arch=dict(pi=[128, 128], vf=[128, 128]))\n",
    "\n",
    "# policy_kwargs = dict(\n",
    "#     net_arch=[256, 64, 64], # The number and size of the hidden layers\n",
    "#     activation_fn=th.nn.Tanh, # The activation function\n",
    "#     lstm=50 # The size of the LSTM cell\n",
    "# )\n",
    "\n",
    "policy_kwargs=dict(\n",
    "    net_arch=[256, 128, 128, 64],\n",
    "    activation_fn=th.nn.Tanh,\n",
    "          # lstm_hidden_size=64,\n",
    "      )\n",
    "\n",
    "model = RecurrentPPO('MlpLstmPolicy', \n",
    "                     env, \n",
    "                     verbose=1, \n",
    "                    gamma=0.01,\n",
    "                    gae_lambda = 0.9,\n",
    "                    #gamma=0,\n",
    "                    #n_epochs=1000,\n",
    "                    seed=1,                 \n",
    "                    #policy_kwargs = policy_kwargs, \n",
    "                    device=\"cuda\",\n",
    "                    tensorboard_log=\"./trade_env_tensorboard/\")\n",
    "# model.policy\n",
    "model.learn(total_timesteps=1_000_000_000,  \n",
    "            log_interval=10, \n",
    "            tb_log_name=\"trade_drl_experiment\",\n",
    "            callback=checkpoint_callback,\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sb3_contrib import RecurrentPPO\n",
    "\n",
    "# Load the model from a zip file\n",
    "# model = RecurrentPPO.load(\"C:/Users/da2343/Projects/robust_algo_trader/drl/log_10_17/trade_drl_model_44040000_steps.zip\")\n",
    "model = RecurrentPPO.load(\"/Users/newuser/Projects/robust-algo-trader/envs/trade_drl_model_9120000_steps.zip\")\n",
    "model\n",
    "# set the env of the saved_model\n",
    "#saved_model.set_env(env, force_reset=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "test_env = TradeUpdateEnv(\n",
    "         df = deepcopy(df),\n",
    "         window_size = 3,\n",
    "         frame_bound = (4, 10))\n",
    "\n",
    "# train_env = TradeUpdateEnv(\n",
    "#             df = deepcopy(df),\n",
    "#             window_size = 3,\n",
    "#             frame_bound = (4, 3000))\n",
    "\n",
    "algo_list = [\n",
    "    \n",
    "    #  {\n",
    "    #     \"name\" : \"Baseline_Frequent_Label\",\n",
    "    #     \"env\" : test_env,\n",
    "    #     \"get_action\" : lambda env, obs: np.bincount((obs[:,6]).astype(int)).argmax()\n",
    "        \n",
    "    # },\n",
    "    # {\n",
    "    #     \"name\" : \"Baseline_Random_Label\",\n",
    "    #     \"env\" : test_env,\n",
    "    #     \"get_action\" : lambda env, obs: env.action_space.sample()\n",
    "    # },\n",
    "    #   {\n",
    "    #     \"name\" : \"Baseline_Fixed_Label\",\n",
    "    #     \"env\" : test_env,\n",
    "    #     \"get_action\" : lambda env, obs: 1\n",
    "    # },\n",
    "    {\n",
    "        \"name\" : \"Baseline_Most_Recent_Label\",\n",
    "        \"env\" : test_env,\n",
    "        \"get_action\" : lambda env, obs: obs[-1][6]\n",
    "    },\n",
    "    # {\n",
    "    #     \"name\" : \"RecurrentPPO\",\n",
    "    #     \"env\" : test_env,\n",
    "    #     \"get_action\" : None   \n",
    "    # },\n",
    "    # {\n",
    "    #     \"name\" : \"Ensemble_Baseline_Most_Recent_Label_RecurrentPPO\",\n",
    "    #     \"env\" : test_env,\n",
    "    #     \"get_action\" : lambda env, obs: obs[-1][6]\n",
    "    # },\n",
    "    # {\n",
    "    #     \"name\" : \"Max_Reward\",\n",
    "    #     \"env\" : test_env,\n",
    "    #     \"get_action\" : lambda env, obs: 1,\n",
    "    # }\n",
    "    \n",
    "]\n",
    "\n",
    "reward_df_list = []\n",
    "for index, algo_dict in enumerate(algo_list):\n",
    "    env = algo_dict['env']\n",
    "    name = algo_dict['name']\n",
    "    get_action = algo_dict['get_action']\n",
    "    observation, info = env.reset()\n",
    "    episode_count = 0\n",
    "    step_count = 1\n",
    "    step_reward_list = []\n",
    "    lstm_states = None\n",
    "    num_envs = 1\n",
    "    # Episode start signals are used to reset the lstm states\n",
    "    episode_starts = np.ones((num_envs,), dtype=bool)\n",
    "    \n",
    "\n",
    "    while True:\n",
    "        if get_action == None or name == \"Ensemble_Baseline_Most_Recent_Label_RecurrentPPO\":\n",
    "            action, lstm_states = model.predict(observation,state=lstm_states, \n",
    "                                             episode_start=episode_starts, \n",
    "                                            deterministic=True)\n",
    "            if name == \"Ensemble_Baseline_Most_Recent_Label_RecurrentPPO\":\n",
    "                action_alt = get_action(env, observation)\n",
    "                # make action 1 if both action and action_alt are 1\n",
    "                if action == 1 and action_alt == 1:\n",
    "                    action = 1\n",
    "                else:\n",
    "                    action = 0\n",
    "        else:\n",
    "            action = get_action(env, observation)\n",
    "        \n",
    "        print(\"observation: \")\n",
    "        print(observation)\n",
    "        # print(\"action: \")\n",
    "        # print(action)\n",
    "        # print(type(observation))\n",
    "        # get action from the most recent label from observation\n",
    "        # action = observation[-1][6]\n",
    "             \n",
    "        #action = env.action_space.sample()\n",
    "        \n",
    "        m_obs = int(observation[-1][6])\n",
    "        observation, reward, terminated, truncated, info = env.step(action)\n",
    "        if reward == -3:\n",
    "            reward = -1\n",
    "        if name == \"Max_Reward\":            \n",
    "            reward = 1.5 if m_obs == 1 else 0\n",
    "        # print(action)\n",
    "        # print(reward)\n",
    "        step_reward_list.append({\n",
    "            \"step\": step_count,\n",
    "            f\"reward\": reward,\n",
    "        })\n",
    "        step_count += 1\n",
    "        \n",
    "        # print(\"info:\", info)\n",
    "        if terminated:\n",
    "            episode_count += 1\n",
    "        # observation, reward, done, info = env.step(action)\n",
    "        # env.render()\n",
    "        if truncated:\n",
    "            print(\"name:\", name)\n",
    "            print(\"info:\", info)\n",
    "            break\n",
    "    print(\"episode_count:\", episode_count)\n",
    "\n",
    "    reward_df = pd.DataFrame(step_reward_list)\n",
    "    reward_df[name] = reward_df[\"reward\"].cumsum()\n",
    "    # remove reward column\n",
    "    reward_df = reward_df.drop(columns=[\"reward\"])\n",
    "    reward_df_list.append(reward_df)\n",
    "    # add cumulative reward column\n",
    "reward_df_concat = pd.concat(reward_df_list, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reward_df_concat = pd.concat(reward_df_list, axis=1)\n",
    "# there are three columns with the same name \"step\", we only need one\n",
    "reward_df_concat = reward_df_concat.loc[:,~reward_df_concat.columns.duplicated()]\n",
    "reward_df_concat\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# plot the cumulative reward against the step\n",
    "plt.figure(figsize=(10, 7))\n",
    "plt.plot(reward_df_concat[\"step\"], reward_df_concat[\"Baseline_Most_Recent_Label\"], label=\"Baseline_Most_Recent_Label\")\n",
    "plt.plot(reward_df_concat[\"step\"], reward_df_concat[\"RecurrentPPO\"], label=\"RecurrentPPO\")\n",
    "plt.plot(reward_df_concat[\"step\"], reward_df_concat[\"Max_Reward\"], label=\"Max_Reward\")\n",
    "plt.plot(reward_df_concat[\"step\"], reward_df_concat[\"Baseline_Random_Label\"], label=\"Baseline_Random_Label\")\n",
    "plt.plot(reward_df_concat[\"step\"], reward_df_concat[\"Baseline_Fixed_Label\"], label=\"Baseline_Fixed_Label\")\n",
    "\n",
    "# plt.plot(reward_df_concat[\"step\"], reward_df_concat[\"Baseline_Frequent_Label\"], label=\"Baseline_Frequent_Label\")\n",
    "# plt.plot(reward_df_concat[\"step\"], reward_df_concat[\"Ensemble_Baseline_Most_Recent_Label_RecurrentPPO\"], label=\"Ensemble_Baseline_Most_Recent_Label_RecurrentPPO\")\n",
    "\n",
    "plt.xlabel(\"Step\")\n",
    "plt.ylabel(\"Cumulative Reward\")\n",
    "plt.title(\"Cumulative Reward vs Step\")\n",
    "\n",
    "# show grid lines\n",
    "plt.grid()\n",
    "\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reward_df_concat = pd.concat(reward_df_list, axis=1)\n",
    "# there are three columns with the same name \"step\", we only need one\n",
    "reward_df_concat = reward_df_concat.loc[:,~reward_df_concat.columns.duplicated()]\n",
    "reward_df_concat\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# plot the cumulative reward against the step\n",
    "plt.figure(figsize=(10, 7))\n",
    "\n",
    "plt.plot(reward_df_concat[\"step\"], reward_df_concat[\"Baseline_Frequent_Label\"], label=\"Baseline_Frequent_Label\")\n",
    "#plt.plot(reward_df_concat[\"step\"], reward_df_concat[\"Baseline_Random_Label\"], label=\"Baseline_Random_Label\")\n",
    "plt.plot(reward_df_concat[\"step\"], reward_df_concat[\"Baseline_Fixed_Label\"], label=\"Baseline_Fixed_Label\")\n",
    "plt.plot(reward_df_concat[\"step\"], reward_df_concat[\"Baseline_Most_Recent_Label\"], label=\"Baseline_Most_Recent_Label\")\n",
    "plt.plot(reward_df_concat[\"step\"], reward_df_concat[\"RecurrentPPO\"], label=\"RecurrentPPO\")\n",
    "plt.plot(reward_df_concat[\"step\"], reward_df_concat[\"Ensemble_Baseline_Most_Recent_Label_RecurrentPPO\"], label=\"Ensemble_Baseline_Most_Recent_Label_RecurrentPPO\")\n",
    "plt.plot(reward_df_concat[\"step\"], reward_df_concat[\"Max_Reward\"], label=\"Max_Reward\")\n",
    "\n",
    "plt.xlabel(\"Step\")\n",
    "plt.ylabel(\"Cumulative Reward\")\n",
    "plt.title(\"Cumulative Reward vs Step\")\n",
    "\n",
    "# show grid lines\n",
    "plt.grid()\n",
    "\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "# df = pd.read_csv(\"/Users/newuser/Projects/robust-algo-trader/envs/trades_2021.csv\")\n",
    "env = TradeUpdateEnv(\n",
    "       df = deepcopy(df),\n",
    "       window_size = 3,\n",
    "       frame_bound = (3000, 5000)\n",
    ")\n",
    "observation, info = env.reset()\n",
    "episode_count = 0\n",
    "step_count = 1\n",
    "step_reward_list = []\n",
    "print(\"got here\")\n",
    "lstm_states = None\n",
    "num_envs = 1\n",
    "# Episode start signals are used to reset the lstm states\n",
    "episode_starts = np.ones((num_envs,), dtype=bool)\n",
    "\n",
    "while True:\n",
    "    action, lstm_states = model.predict(observation,state=lstm_states, \n",
    "                                        episode_start=episode_starts, \n",
    "                                        deterministic=True)\n",
    "    # action = env.action_space.sample()\n",
    "    observation, reward, terminated, truncated, info = env.step(action)\n",
    "    # print(action)\n",
    "    # print(reward)\n",
    "    step_reward_list.append({\n",
    "        \"step\": step_count,\n",
    "        \"test_reward\": reward,\n",
    "    })\n",
    "    step_count += 1\n",
    "    \n",
    "    # print(\"info:\", info)\n",
    "    if terminated:\n",
    "        episode_count += 1\n",
    "    # observation, reward, done, info = env.step(action)\n",
    "    # env.render()\n",
    "    if truncated:\n",
    "        print(\"info:\", info)\n",
    "        break\n",
    "print(\"episode_count:\", episode_count)\n",
    "\n",
    "test_reward_df = pd.DataFrame(step_reward_list) \n",
    "# add cumulative reward column\n",
    "test_reward_df[\"RecurrentPPO_test_cum_reward\"] = test_reward_df[\"test_reward\"].cumsum()\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "# plot the cumulative reward against the step\n",
    "plt.plot(test_reward_df[\"step\"], test_reward_df[\"RecurrentPPO_test_cum_reward\"])\n",
    "# show labels\n",
    "plt.xlabel(\"step\")\n",
    "plt.ylabel(\"RecurrentPPO_test_cum_reward\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "# df = pd.read_csv(\"/Users/newuser/Projects/robust-algo-trader/envs/trades_2021.csv\")\n",
    "env = TradeUpdateEnv(\n",
    "       df = deepcopy(df),\n",
    "       window_size = 3,\n",
    "       frame_bound = (3000, 5000)\n",
    ")\n",
    "observation, info = env.reset()\n",
    "episode_count = 0\n",
    "step_count = 1\n",
    "step_reward_list = []\n",
    "print(\"got here\")\n",
    "lstm_states = None\n",
    "num_envs = 1\n",
    "# Episode start signals are used to reset the lstm states\n",
    "episode_starts = np.ones((num_envs,), dtype=bool)\n",
    "\n",
    "while True:\n",
    "    #action, lstm_states = model.predict(observation,state=lstm_states, \n",
    "            #                            episode_start=episode_starts, \n",
    "                 #                       deterministic=True)\n",
    "    #action = env.action_space.sample()\n",
    "    action = 1\n",
    "    observation, reward, terminated, truncated, info = env.step(action)\n",
    "    # print(action)\n",
    "    # print(reward)\n",
    "    step_reward_list.append({\n",
    "        \"step\": step_count,\n",
    "        \"test_reward\": reward,\n",
    "    })\n",
    "    step_count += 1\n",
    "    \n",
    "    # print(\"info:\", info)\n",
    "    if terminated:\n",
    "        episode_count += 1\n",
    "    # observation, reward, done, info = env.step(action)\n",
    "    # env.render()\n",
    "    if truncated:\n",
    "        print(\"info:\", info)\n",
    "        break\n",
    "print(\"episode_count:\", episode_count)\n",
    "\n",
    "test_reward_df = pd.DataFrame(step_reward_list) \n",
    "# add cumulative reward column\n",
    "test_reward_df[\"Featureless_test_cum_reward\"] = test_reward_df[\"test_reward\"].cumsum()\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "# plot the cumulative reward against the step\n",
    "plt.plot(test_reward_df[\"step\"], test_reward_df[\"Featureless_test_cum_reward\"])\n",
    "# show labels\n",
    "plt.xlabel(\"step\")\n",
    "plt.ylabel(\"Featureless_test_cum_reward\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "# df = pd.read_csv(\"/Users/newuser/Projects/robust-algo-trader/envs/trades_2021.csv\")\n",
    "env = TradeUpdateEnv(\n",
    "       df = deepcopy(df),\n",
    "       window_size = 3,\n",
    "    #    frame_bound = (4, 3000)\n",
    "       frame_bound = (4, 2004)\n",
    ")\n",
    "observation, info = env.reset()\n",
    "episode_count = 0\n",
    "step_count = 1\n",
    "step_reward_list = []\n",
    "print(\"got here\")\n",
    "lstm_states = None\n",
    "num_envs = 1\n",
    "# Episode start signals are used to reset the lstm states\n",
    "episode_starts = np.ones((num_envs,), dtype=bool)\n",
    "\n",
    "while True:\n",
    "    action, lstm_states = model.predict(observation,state=lstm_states, \n",
    "                                        episode_start=episode_starts, \n",
    "                                        deterministic=True)\n",
    "    # action = env.action_space.sample()\n",
    "    observation, reward, terminated, truncated, info = env.step(action)\n",
    "    #print(reward)\n",
    "    # if reward != 0:\n",
    "    step_reward_list.append({\n",
    "        \"step\": step_count,\n",
    "        \"train_reward\": reward,\n",
    "    })\n",
    "    step_count += 1\n",
    "    \n",
    "    # print(\"info:\", info)\n",
    "    if terminated:\n",
    "        episode_count += 1\n",
    "    # observation, reward, done, info = env.step(action)\n",
    "    # env.render()\n",
    "    if truncated:\n",
    "        print(\"info:\", info)\n",
    "        break\n",
    "print(\"episode_count:\", episode_count)\n",
    "\n",
    "train_reward_df = pd.DataFrame(step_reward_list) \n",
    "# add cumulative reward column\n",
    "train_reward_df[\"RecurrentPPO_train_cum_reward\"] = train_reward_df[\"train_reward\"].cumsum()\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "# plot the cumulative reward against the step\n",
    "plt.plot(train_reward_df[\"step\"], train_reward_df[\"RecurrentPPO_train_cum_reward\"])\n",
    "# show labels\n",
    "plt.xlabel(\"step\")\n",
    "plt.ylabel(\"RecurrentPPO_train_cum_reward\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_test_reward_df = pd.merge(train_reward_df, test_reward_df, on=\"step\", how=\"outer\")\n",
    "train_test_reward_df\n",
    "# choose first 1000 rows\n",
    "# train_test_reward_df = train_test_reward_df.iloc[40:50]\n",
    "\n",
    "# plot a graph of train_cum_reward and test_cum_reward against the step \n",
    "# give the train_cum_reward and test_cum_reward different colors\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming your dataframe is called df\n",
    "train_test_reward_df.plot(x=\"step\", y=[\"RecurrentPPO_train_cum_reward\", \"RecurrentPPO_test_cum_reward\"], color=[\"red\", \"blue\"])\n",
    "plt.show()\n",
    "# save the plot as a png file\n",
    "#plt.savefig(\"train_test_reward.png\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_vowel( word ):\n",
    "\n",
    "    try:\n",
    "        result = word[0].lower() in 'aeiou'\n",
    "\n",
    "    except TypeError:\n",
    "        print('word must start with a character')\n",
    "        return None\n",
    "\n",
    "    else:\n",
    "        return result\n",
    "    \n",
    "    \n",
    "entry = 'one'\n",
    "\n",
    "if is_vowel(word):\n",
    "    print(f\"{entry} starts with a vowel.\")\n",
    "else:\n",
    "    print(f\"{entry} does not start with a vowel.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs499f22",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
