{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gymnasium.envs.registration import register\n",
    "import gymnasium as gym\n",
    "from stable_baselines3 import *\n",
    "\n",
    "# from stable_baselines3 import A2C\n",
    "from sb3_contrib import RecurrentPPO\n",
    "from stable_baselines3.common.callbacks import CheckpointCallback\n",
    "from stable_baselines3.common.callbacks import EvalCallback, StopTrainingOnNoModelImprovement\n",
    "import torch as th\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "# from gymnasium.utils import seeding\n",
    "from enum import Enum\n",
    "import pandas as pd\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "from copy import deepcopy\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "from gymnasium.utils import seeding\n",
    "from enum import Enum\n",
    "from sklearn.preprocessing import StandardScaler, PowerTransformer\n",
    "import talib\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('/projects/genomic-ml/da2343/ml_project_2/data/EURUSD/trades_EURUSD_H1_2011_2022.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gpu\n",
    "device = th.device(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class TradeUpdateEnv(gym.Env):\n",
    "    def __init__(self,  df, window_size, frame_bound):\n",
    "        assert len(frame_bound) == 2\n",
    "        assert frame_bound[0] >= window_size\n",
    "        assert frame_bound[1] > frame_bound[0]\n",
    "        \n",
    "        self.frame_bound = frame_bound\n",
    "        \n",
    "        # self.seed()\n",
    "        self.df = df\n",
    "        self.window_size = window_size\n",
    "        self.signal_features = self._process_data()\n",
    "        self.shape = (window_size, self.signal_features.shape[1])\n",
    "\n",
    "        # spaces\n",
    "        self.observation_space = spaces.Box(low=-100, \n",
    "                                            high=100,\n",
    "                                            shape=self.shape, \n",
    "                                            dtype=np.float64)\n",
    "        self.action_space = spaces.Discrete(2)\n",
    "\n",
    "        # episode\n",
    "        self._start_tick = self.window_size\n",
    "        self._current_tick = self._start_tick\n",
    "        self._last_final_tick = len(self.signal_features) - 1\n",
    "        self._end_tick = None\n",
    "        self._terminated = None\n",
    "        self._truncated = False\n",
    "        self._total_reward = 0\n",
    "        self._total_profit = None\n",
    "        self._prev_loss = False\n",
    "\n",
    "\n",
    "    def _process_data(self):\n",
    "        features_df = self.df[[\"position\", \"RSI\", \"ATR\", \"ADX\", \"AROON_Oscillator\", \"MFI\",\"label\"]]\n",
    "        # features_df = self.df[[\"position\", \"RSI\", \"label\"]]\n",
    "        features_df['label'] = features_df['label'].shift(1)\n",
    "        features_df = features_df.iloc[1:]\n",
    "        features_np = features_df.to_numpy()\n",
    "        _start_index = self.frame_bound[0] - self.window_size\n",
    "        _end_index = self.frame_bound[1]\n",
    "        signal_features = features_np[_start_index:_end_index]\n",
    "        return signal_features\n",
    "\n",
    "    def _get_observation(self):\n",
    "        _start_index = self._current_tick - self.window_size + 1\n",
    "        _end_index = self._current_tick + 1\n",
    "        _obs = self.signal_features[_start_index:_end_index]\n",
    "        self._current_tick += 1\n",
    "        return _obs\n",
    "\n",
    "\n",
    "    def reset(self, seed=None, options=None):\n",
    "        super().reset(seed=1)\n",
    "        self._terminated = False\n",
    "        self._truncated = False\n",
    "        self._prev_loss = False\n",
    "        # self._current_tick = np.random.randint(\n",
    "        #     self._start_tick, len(self.signal_features) - 50\n",
    "        # )\n",
    "        self._end_tick = self._current_tick + 30\n",
    "        observation = self._get_observation()\n",
    "        info = dict(total_reward = self._total_reward)\n",
    "        return observation, info\n",
    "\n",
    "\n",
    "    def step(self, action):\n",
    "        step_reward = self._calculate_reward(action)\n",
    "        self._total_reward += step_reward\n",
    "        info = dict(total_reward = self._total_reward)\n",
    "        observation = self._get_observation()\n",
    "        # set done flag to True if episode ended because of tick limit\n",
    "        if self._current_tick == self._end_tick:\n",
    "            self._terminated = True\n",
    "        # set truncated flag to True if episode ended because of tick limit\n",
    "        if self._current_tick == self._last_final_tick:\n",
    "            self._truncated = True\n",
    "            # reset the current tick to the start tick\n",
    "            self._current_tick = self._start_tick\n",
    "        return observation, step_reward, self._terminated, self._truncated, info\n",
    "\n",
    "\n",
    "    def _calculate_reward(self, action):\n",
    "        step_reward = 0\n",
    "        label_action = self.signal_features[self._current_tick+1][6]\n",
    "        # label_action = self.signal_features[self._current_tick+1][2]\n",
    "        # if action == label_action:\n",
    "        #     step_reward += 1\n",
    "    \n",
    "        if action == 1 and label_action == 0:\n",
    "            step_reward += -1\n",
    "            if self._prev_loss == True:\n",
    "                step_reward += -2\n",
    "            self._prev_loss = True\n",
    "            print(\"lost\")\n",
    "        else:\n",
    "            self._prev_loss = False\n",
    "        # else:\n",
    "            # step_reward += 0.1\n",
    "            # print(\"nothing\")\n",
    "            \n",
    "        if action == 1 and label_action == 1:\n",
    "            step_reward += 1.5\n",
    "            print(\"won\")\n",
    "            \n",
    "        return step_reward\n",
    "\n",
    "env = TradeUpdateEnv(\n",
    "       df = deepcopy(df),\n",
    "       window_size = 3,\n",
    "       frame_bound = (4, 3000)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# Save a checkpoint every 1000 steps\n",
    "checkpoint_callback = CheckpointCallback(\n",
    "  save_freq=10_000,\n",
    "  save_path=\"./logs/\",\n",
    "  name_prefix=\"trade_drl_model\",\n",
    "  save_replay_buffer=True,\n",
    "  save_vecnormalize=True,\n",
    ")\n",
    "\n",
    "# policy_kwargs = dict(activation_fn=th.nn.ReLU,\n",
    "#                      net_arch=dict(pi=[128, 128], vf=[128, 128]))\n",
    "\n",
    "# policy_kwargs = dict(\n",
    "#     net_arch=[256, 64, 64], # The number and size of the hidden layers\n",
    "#     activation_fn=th.nn.Tanh, # The activation function\n",
    "#     lstm=50 # The size of the LSTM cell\n",
    "# )\n",
    "\n",
    "policy_kwargs=dict(\n",
    "    net_arch=[256, 128, 128, 64],\n",
    "    activation_fn=th.nn.Tanh,\n",
    "          # lstm_hidden_size=64,\n",
    "      )\n",
    "\n",
    "model = RecurrentPPO('MlpLstmPolicy', \n",
    "                     env, \n",
    "                     verbose=1, \n",
    "                    #  gamma=0.01,\n",
    "                     gamma=0,\n",
    "                    #  n_epochs=1000,\n",
    "                    policy_kwargs = policy_kwargs, \n",
    "                    device=\"cuda\",\n",
    "                    tensorboard_log=\"./trade_env_tensorboard/\")\n",
    "# model.policy\n",
    "model.learn(total_timesteps=10_000_000,  \n",
    "            log_interval=10, \n",
    "            tb_log_name=\"trade_drl_experiment\",\n",
    "            callback=checkpoint_callback,\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.backends.mps.is_available()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs499f22",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
