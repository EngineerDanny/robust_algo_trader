{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "import matplotlib.pyplot as plt\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.env_checker import check_env\n",
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "\n",
    "class PortfolioEnv(gym.Env):\n",
    "    \"\"\"\n",
    "    Portfolio allocation environment using Gymnasium interface.\n",
    "    \n",
    "    The agent makes sequential allocation decisions for 10 stocks,\n",
    "    with options of 0%, 10%, 20%, or 30% per stock, \n",
    "    with the constraint that allocations must sum to 100%.\n",
    "    \"\"\"\n",
    "    \n",
    "    metadata = {'render_modes': ['human']}\n",
    "    \n",
    "    def __init__(self, data_path: str, n_stocks: int = 10, episode_length: int = 12):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            data_path: Path to the CSV data file\n",
    "            n_stocks: Number of stocks to allocate (default: 10)\n",
    "            episode_length: Length of episode in months (default: 12)\n",
    "        \"\"\"\n",
    "        super(PortfolioEnv, self).__init__()\n",
    "        \n",
    "        # Load and prepare data\n",
    "        self.raw_data = pd.read_csv(data_path)\n",
    "        self.n_stocks = n_stocks\n",
    "        self.episode_length = episode_length\n",
    "        \n",
    "        # Create synthetic stocks (in practice, would use real data for each stock)\n",
    "        self.stocks = {}\n",
    "        for i in range(n_stocks):\n",
    "            # For demonstration, we create slightly different versions of the data\n",
    "            # In a real scenario, you would load separate data for each stock\n",
    "            stock_data = self.raw_data.copy()\n",
    "            # Add a small random variation to make each \"stock\" slightly different\n",
    "            stock_data['Close'] = stock_data['Close'] * (1 + np.random.normal(0, 0.05))\n",
    "            self.stocks[f'stock_{i}'] = stock_data\n",
    "        \n",
    "        # Key features for state representation\n",
    "        self.features = [\n",
    "            'Close_scaled', 'MA5_scaled', 'MA20_scaled', 'MA50_scaled', 'MA200_scaled',\n",
    "            'RSI_scaled', 'BB_width_scaled', 'ATR_scaled', 'Return_1W_scaled',\n",
    "            'Return_1M_scaled', 'Return_3M_scaled', 'CurrentDrawdown_scaled',\n",
    "            'MaxDrawdown_252d_scaled', 'Sharpe_20d_scaled', 'Sharpe_60d_scaled'\n",
    "        ]\n",
    "        \n",
    "        # Define observation and action spaces\n",
    "        # State: 15 features per stock + current stock index + remaining allocation\n",
    "        obs_dim = len(self.features) * n_stocks + 2\n",
    "        self.observation_space = spaces.Box(\n",
    "            low=-10, high=10, \n",
    "            shape=(obs_dim,), \n",
    "            dtype=np.float32\n",
    "        )\n",
    "        \n",
    "        # Action space: 4 options (0%, 10%, 20%, 30%)\n",
    "        self.action_space = spaces.Discrete(4)\n",
    "        \n",
    "        # Initialize\n",
    "        self.reset()\n",
    "    \n",
    "    def reset(self, seed=None, options=None):\n",
    "        \"\"\"Reset environment for new episode\"\"\"\n",
    "        super().reset(seed=seed)\n",
    "        \n",
    "        # Find a random starting point that allows for a full episode\n",
    "        data_length = len(self.raw_data)\n",
    "        max_start_idx = data_length - self.episode_length * 30 - 20  # 20-day lookback\n",
    "        self.current_step = np.random.randint(20, max_start_idx)\n",
    "        self.current_month = 0\n",
    "        \n",
    "        # Reset allocation process\n",
    "        self.allocation = np.zeros(self.n_stocks)\n",
    "        self.remaining_allocation = 100\n",
    "        self.current_stock_idx = 0\n",
    "        \n",
    "        # Get initial state\n",
    "        observation = self._get_observation()\n",
    "        info = {}\n",
    "        \n",
    "        return observation, info\n",
    "    \n",
    "    def _get_observation(self):\n",
    "        \"\"\"Get current state representation\"\"\"\n",
    "        observation = []\n",
    "        \n",
    "        # Get features for each stock\n",
    "        for stock_name, stock_data in self.stocks.items():\n",
    "            current_data = stock_data.iloc[self.current_step]\n",
    "            # Extract all required features\n",
    "            stock_features = [current_data[feature] for feature in self.features]\n",
    "            observation.extend(stock_features)\n",
    "        \n",
    "        # Add contextual information\n",
    "        observation.append(self.current_stock_idx / self.n_stocks)  # Normalized index\n",
    "        observation.append(self.remaining_allocation / 100)  # Remaining allocation %\n",
    "        \n",
    "        return np.array(observation, dtype=np.float32)\n",
    "    \n",
    "    def step(self, action):\n",
    "        \"\"\"\n",
    "        Take allocation action for current stock and move to next\n",
    "        \n",
    "        Args:\n",
    "            action: Integer in [0, 1, 2, 3] corresponding to [0%, 10%, 20%, 30%]\n",
    "            \n",
    "        Returns:\n",
    "            observation: New state observation\n",
    "            reward: Reward (only non-zero at end of month)\n",
    "            terminated: Whether episode is terminated\n",
    "            truncated: Whether episode is truncated\n",
    "            info: Additional information\n",
    "        \"\"\"\n",
    "        # Convert action to allocation percentage\n",
    "        allocation_pct = action * 10\n",
    "        \n",
    "        # Check if action is valid\n",
    "        if allocation_pct > self.remaining_allocation:\n",
    "            # Adjust invalid actions to the maximum possible\n",
    "            if self.current_stock_idx == self.n_stocks - 1:\n",
    "                # For the last stock, must use exactly what's left\n",
    "                allocation_pct = self.remaining_allocation\n",
    "            else:\n",
    "                # For other invalid actions, use the highest valid option\n",
    "                valid_options = [0, 10, 20, 30]\n",
    "                valid_options = [opt for opt in valid_options if opt <= self.remaining_allocation]\n",
    "                if valid_options:\n",
    "                    allocation_pct = max(valid_options)\n",
    "                else:\n",
    "                    allocation_pct = 0\n",
    "        \n",
    "        # Apply allocation for current stock\n",
    "        self.allocation[self.current_stock_idx] = allocation_pct\n",
    "        self.remaining_allocation -= allocation_pct\n",
    "        self.current_stock_idx += 1\n",
    "        \n",
    "        reward = 0\n",
    "        terminated = False\n",
    "        truncated = False\n",
    "        info = {}\n",
    "        \n",
    "        # If we've allocated to all stocks (completed a month)\n",
    "        if self.current_stock_idx == self.n_stocks:\n",
    "            # For the last stock, adjust allocation to ensure sum is 100%\n",
    "            actual_sum = np.sum(self.allocation)\n",
    "            if actual_sum != 100:\n",
    "                # Adjust the last allocation to make the sum 100%\n",
    "                self.allocation[-1] += (100 - actual_sum)\n",
    "                self.remaining_allocation = 0\n",
    "            \n",
    "            # Calculate portfolio return for the month\n",
    "            current_prices = np.array([\n",
    "                self.stocks[f'stock_{i}'].iloc[self.current_step]['Close'] \n",
    "                for i in range(self.n_stocks)\n",
    "            ])\n",
    "            \n",
    "            # Move to end of month (~30 trading days)\n",
    "            next_step = min(self.current_step + 30, len(self.raw_data) - 1)\n",
    "            next_prices = np.array([\n",
    "                self.stocks[f'stock_{i}'].iloc[next_step]['Close'] \n",
    "                for i in range(self.n_stocks)\n",
    "            ])\n",
    "            \n",
    "            # Calculate stock returns and portfolio return\n",
    "            stock_returns = (next_prices - current_prices) / current_prices\n",
    "            portfolio_return = np.sum((self.allocation / 100) * stock_returns)\n",
    "            \n",
    "            # Calculate Sharpe and drawdown metrics\n",
    "            sharpe = self._calculate_portfolio_metric('Sharpe_20d_scaled')\n",
    "            max_drawdown = self._calculate_portfolio_metric('MaxDrawdown_252d_scaled')\n",
    "            \n",
    "            # Calculate reward\n",
    "            reward = self._calculate_reward(portfolio_return, sharpe, max_drawdown)\n",
    "            \n",
    "            # Add portfolio info\n",
    "            info = {\n",
    "                'portfolio_return': portfolio_return,\n",
    "                'sharpe': sharpe,\n",
    "                'max_drawdown': max_drawdown,\n",
    "                'allocation': self.allocation.copy()\n",
    "            }\n",
    "            \n",
    "            # Move to next month\n",
    "            self.current_step = next_step\n",
    "            self.current_month += 1\n",
    "            \n",
    "            # Check if episode is done\n",
    "            terminated = (self.current_month >= self.episode_length)\n",
    "            \n",
    "            # Reset for next month's allocation if not done\n",
    "            self.current_stock_idx = 0\n",
    "            self.remaining_allocation = 100\n",
    "        \n",
    "        # Get new state\n",
    "        observation = self._get_observation()\n",
    "        \n",
    "        return observation, reward, terminated, truncated, info\n",
    "    \n",
    "    def _calculate_portfolio_metric(self, metric_name):\n",
    "        \"\"\"Calculate weighted portfolio metric\"\"\"\n",
    "        metric_values = np.array([\n",
    "            self.stocks[f'stock_{i}'].iloc[self.current_step][metric_name] \n",
    "            for i in range(self.n_stocks)\n",
    "        ])\n",
    "        return np.sum((self.allocation / 100) * metric_values)\n",
    "    \n",
    "    def _calculate_reward(self, portfolio_return, sharpe, max_drawdown):\n",
    "        \"\"\"Calculate reward based on return, Sharpe ratio, and drawdown\"\"\"\n",
    "        # Base reward centered around 1% monthly return target\n",
    "        base_reward = (portfolio_return - 0.01) * 100\n",
    "        \n",
    "        # Adjust for risk metrics\n",
    "        risk_adjustment = sharpe * 0.5\n",
    "        drawdown_penalty = max_drawdown * -2.0\n",
    "        \n",
    "        # Extra penalty for negative returns\n",
    "        if portfolio_return < 0:\n",
    "            base_reward *= 1.5\n",
    "        \n",
    "        return base_reward + risk_adjustment + drawdown_penalty\n",
    "    \n",
    "    def render(self, mode='human'):\n",
    "        \"\"\"Render the environment state\"\"\"\n",
    "        if self.current_stock_idx == 0:  # Just allocated a full portfolio\n",
    "            print(f\"Month {self.current_month}, Allocation: {self.allocation}\")\n",
    "    \n",
    "    def close(self):\n",
    "        \"\"\"Clean up resources\"\"\"\n",
    "        pass\n",
    "\n",
    "\n",
    "class TensorboardCallback(BaseCallback):\n",
    "    \"\"\"\n",
    "    Custom callback for logging useful metrics\n",
    "    \"\"\"\n",
    "    def __init__(self, verbose=0):\n",
    "        super(TensorboardCallback, self).__init__(verbose)\n",
    "    \n",
    "    def _on_step(self) -> bool:\n",
    "        # Log portfolio information if available\n",
    "        if 'portfolio_return' in self.locals['infos'][-1]:\n",
    "            info = self.locals['infos'][-1]\n",
    "            self.logger.record('portfolio/return', info['portfolio_return'])\n",
    "            self.logger.record('portfolio/sharpe', info['sharpe'])\n",
    "            self.logger.record('portfolio/max_drawdown', info['max_drawdown'])\n",
    "        return True\n",
    "\n",
    "\n",
    "def train_model(env, total_timesteps=100000, log_dir='./logs/'):\n",
    "    \"\"\"Train a PPO model on the portfolio environment\"\"\"\n",
    "    # Initialize the model\n",
    "    model = PPO(\n",
    "        \"MlpPolicy\", \n",
    "        env,\n",
    "        verbose=1,\n",
    "        tensorboard_log=log_dir,\n",
    "        learning_rate=3e-4,\n",
    "        gamma=0.99,\n",
    "        n_steps=2048,\n",
    "        ent_coef=0.01,\n",
    "        vf_coef=0.5,\n",
    "        max_grad_norm=0.5,\n",
    "        gae_lambda=0.95,\n",
    "        clip_range=0.2,\n",
    "        clip_range_vf=None,\n",
    "        normalize_advantage=True,\n",
    "        policy_kwargs={'net_arch': [256, 128, dict(vf=[64], pi=[64])]}\n",
    "    )\n",
    "    \n",
    "    # Train the model\n",
    "    model.learn(\n",
    "        total_timesteps=total_timesteps,\n",
    "        callback=TensorboardCallback(),\n",
    "        progress_bar=True\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "def evaluate_model(model, env, n_episodes=10):\n",
    "    \"\"\"Evaluate the trained model\"\"\"\n",
    "    # Evaluate the model\n",
    "    mean_reward, std_reward = evaluate_policy(\n",
    "        model, \n",
    "        env, \n",
    "        n_eval_episodes=n_episodes,\n",
    "        deterministic=True\n",
    "    )\n",
    "    \n",
    "    print(f\"Mean reward: {mean_reward:.3f} +/- {std_reward:.3f}\")\n",
    "    \n",
    "    # Run detailed evaluation for visualization\n",
    "    allocations = []\n",
    "    returns = []\n",
    "    sharpes = []\n",
    "    drawdowns = []\n",
    "    \n",
    "    obs, info = env.reset()\n",
    "    \n",
    "    for i in range(n_episodes):\n",
    "        episode_allocations = []\n",
    "        episode_returns = []\n",
    "        episode_sharpes = []\n",
    "        episode_drawdowns = []\n",
    "        done = False\n",
    "        \n",
    "        while not done:\n",
    "            action, _ = model.predict(obs, deterministic=True)\n",
    "            obs, reward, terminated, truncated, info = env.step(action)\n",
    "            \n",
    "            done = terminated or truncated\n",
    "            \n",
    "            if 'portfolio_return' in info:\n",
    "                episode_allocations.append(info['allocation'])\n",
    "                episode_returns.append(info['portfolio_return'])\n",
    "                episode_sharpes.append(info['sharpe'])\n",
    "                episode_drawdowns.append(info['max_drawdown'])\n",
    "        \n",
    "        allocations.append(episode_allocations)\n",
    "        returns.append(np.mean(episode_returns))\n",
    "        sharpes.append(np.mean(episode_sharpes))\n",
    "        drawdowns.append(np.mean(episode_drawdowns))\n",
    "        \n",
    "        obs, info = env.reset()\n",
    "    \n",
    "    # Plot average allocation\n",
    "    avg_allocations = np.mean([np.mean(ep_allocs, axis=0) for ep_allocs in allocations], axis=0)\n",
    "    \n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.bar(range(len(avg_allocations)), avg_allocations)\n",
    "    plt.xlabel('Stock')\n",
    "    plt.ylabel('Average Allocation (%)')\n",
    "    plt.title('Average Portfolio Allocation')\n",
    "    plt.xticks(range(len(avg_allocations)), [f'Stock {i}' for i in range(len(avg_allocations))])\n",
    "    plt.savefig('portfolio_allocation.png')\n",
    "    \n",
    "    # Plot returns distribution\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.hist(returns, bins=10)\n",
    "    plt.xlabel('Monthly Return')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.title(f'Distribution of Monthly Returns (Mean: {np.mean(returns):.4f})')\n",
    "    plt.savefig('returns_distribution.png')\n",
    "    \n",
    "    return {\n",
    "        'mean_return': np.mean(returns),\n",
    "        'mean_sharpe': np.mean(sharpes),\n",
    "        'mean_drawdown': np.mean(drawdowns),\n",
    "        'allocations': avg_allocations\n",
    "    }\n",
    "\n",
    "\n",
    "def validate_env(env):\n",
    "    \"\"\"Validate the environment works correctly\"\"\"\n",
    "    # Check the environment follows Gym API\n",
    "    check_env(env)\n",
    "    \n",
    "    # Test a few steps\n",
    "    obs, info = env.reset()\n",
    "    \n",
    "    print(\"Initial observation shape:\", obs.shape)\n",
    "    \n",
    "    for i in range(5):\n",
    "        action = env.action_space.sample()  # Random action\n",
    "        obs, reward, terminated, truncated, info = env.step(action)\n",
    "        print(f\"Step {i}: Action={action}, Reward={reward}\")\n",
    "    \n",
    "    print(\"Validation complete!\")\n",
    "\n",
    "\n",
    "def main():\n",
    "    # Path to the dataset\n",
    "    data_path = 'paste.txt'\n",
    "    \n",
    "    # Create environment\n",
    "    env = PortfolioEnv(data_path)\n",
    "    \n",
    "    # Validate environment\n",
    "    print(\"Validating environment...\")\n",
    "    validate_env(env)\n",
    "    \n",
    "    # Train model\n",
    "    print(\"Training PPO agent...\")\n",
    "    model = train_model(env, total_timesteps=100000)\n",
    "    \n",
    "    # Save the model\n",
    "    model.save(\"ppo_portfolio\")\n",
    "    \n",
    "    # Evaluate the model\n",
    "    print(\"Evaluating model...\")\n",
    "    results = evaluate_model(model, env)\n",
    "    \n",
    "    print(\"Training and evaluation complete!\")\n",
    "    print(f\"Average monthly return: {results['mean_return']:.4f}\")\n",
    "    print(f\"Average Sharpe ratio: {results['mean_sharpe']:.4f}\")\n",
    "    print(f\"Average max drawdown: {results['mean_drawdown']:.4f}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs499f22",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
