{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "import os\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.callbacks import CheckpointCallback\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from stable_baselines3.common.vec_env import SubprocVecEnv, VecNormalize\n",
    "import torch as th\n",
    "\n",
    "SAVE_DIR = f\"./models/model_{dt.datetime.now().strftime('%Y%m%d_%H')}\"\n",
    "os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "\n",
    "DATA_DIR = \"./data/gen_alpaca_data\"\n",
    "MASTER_SEED = 42\n",
    "\n",
    "\n",
    "class TradingProfitEnv(gym.Env):\n",
    "    metadata = {'render_modes': ['human']}\n",
    "    def __init__(self, signals_data, lookback_window=180, max_steps_per_trade=500,\n",
    "                 commission_rate=0.001, slippage_factor=0.0005):\n",
    "        super(TradingProfitEnv, self).__init__()\n",
    "\n",
    "        self.signals_data = signals_data\n",
    "        self.symbols = list(signals_data.keys())\n",
    "        self.current_symbol = None\n",
    "        self.lookback_window = lookback_window\n",
    "        self.max_steps_per_trade = max_steps_per_trade\n",
    "        \n",
    "        self.commission_rate = commission_rate\n",
    "        self.slippage_factor = slippage_factor\n",
    "        \n",
    "        self.features = [\n",
    "            'distance_from_mean',\n",
    "            # 'distance_from_upper',\n",
    "            # 'distance_from_lower',\n",
    "            'rsi',\n",
    "            'range_strength',\n",
    "            'mean_reversion_probability',\n",
    "            # 'is_range_market'\n",
    "        ]\n",
    "        \n",
    "        self.action_map = {\n",
    "            'HOLD': 0,\n",
    "            'CLOSE': 1\n",
    "        }\n",
    "        self.reverse_action_map = {v: k for k, v in self.action_map.items()}\n",
    "        self.action_space = spaces.Discrete(len(self.action_map))\n",
    "        \n",
    "        self.observation_space = spaces.Box(\n",
    "            low=-1, \n",
    "            high=1, \n",
    "            shape=(lookback_window, len(self.features) + 1),\n",
    "            dtype=np.float32\n",
    "        )\n",
    "\n",
    "        self.position = None\n",
    "        self.position_type = None\n",
    "        self.position_entry_step = None\n",
    "        self.entry_price = None\n",
    "        self.max_adverse_move = 0\n",
    "        self.steps_in_trade = 0\n",
    "        self.current_step = None\n",
    "        self.trade_history = []\n",
    "        \n",
    "    def _find_next_entry_signal(self):\n",
    "        \"\"\"Find the next BUY or SELL signal in the dataset sequentially\"\"\"\n",
    "        signals = self.signals_data[self.current_symbol]\n",
    "        \n",
    "        # If this is the first call (initialization), start after lookback window\n",
    "        if self.current_step is None:\n",
    "            start_idx = self.lookback_window\n",
    "        else:\n",
    "            # Otherwise, start from the step after current position\n",
    "            start_idx = self.current_step + 1\n",
    "        \n",
    "        # Look for the next entry signal in sequential order\n",
    "        for i in range(start_idx, len(signals) - 1):\n",
    "            action = signals.iloc[i]['action']\n",
    "            if action in ['BUY', 'SELL']:\n",
    "                self.current_step = i\n",
    "                self.position_type = action\n",
    "                self.position = 'LONG' if action == 'BUY' else 'SHORT'\n",
    "                self.position_entry_step = i\n",
    "                self.entry_price = signals.iloc[i]['price']\n",
    "                return\n",
    "        \n",
    "        # If no more signals are found, set a flag to indicate end of data\n",
    "        self.end_of_data = True\n",
    "    \n",
    "    def reset(self, seed=None, options=None):\n",
    "        super().reset(seed=seed)\n",
    "\n",
    "        # Only change symbol when explicitly told to or at the beginning\n",
    "        if self.current_symbol is None:\n",
    "            self.current_symbol = random.choice(self.symbols)\n",
    "        \n",
    "        self.position = None\n",
    "        self.position_type = None\n",
    "        self.position_entry_step = None\n",
    "        self.entry_price = None\n",
    "        self.max_adverse_move = 0\n",
    "        self.steps_in_trade = 0\n",
    "        self.end_of_data = False\n",
    "        \n",
    "        # Find next entry signal (sequential)\n",
    "        self._find_next_entry_signal()\n",
    "        \n",
    "        # If we've reached the end of data, wrap around to the beginning\n",
    "        if hasattr(self, 'end_of_data') and self.end_of_data:\n",
    "            self.current_step = self.lookback_window\n",
    "            self.end_of_data = False\n",
    "            self._find_next_entry_signal()\n",
    "        \n",
    "        return self._get_observation(), {}\n",
    "\n",
    "    def _get_observation(self):\n",
    "        signals = self.signals_data[self.current_symbol]\n",
    "\n",
    "        start_idx = self.current_step - self.lookback_window\n",
    "        end_idx = self.current_step\n",
    "        window_data = signals.iloc[start_idx:end_idx]\n",
    "\n",
    "        observations = np.zeros((self.lookback_window, len(self.features) + 1), dtype=np.float32)\n",
    "\n",
    "        for i, feature in enumerate(self.features):\n",
    "            window_values = window_data[feature].astype(float).values\n",
    "            \n",
    "            if feature == 'rsi':\n",
    "                scaled_values = 2 * (window_values / 100) - 1\n",
    "            elif feature == 'mean_reversion_probability':\n",
    "                scaled_values = 2 * window_values - 1\n",
    "            elif feature == 'is_range_market':\n",
    "                scaled_values = 2 * window_values - 1\n",
    "            elif feature in ['distance_from_mean']:\n",
    "                scaled_values = window_values / 10\n",
    "            elif feature in ['distance_from_upper', 'distance_from_lower']:\n",
    "                scaled_values = 2 * (window_values / 100) - 1\n",
    "            elif feature == 'range_strength':\n",
    "                scaled_values = 2 * window_values - 1\n",
    "            \n",
    "            scaled_values = np.clip(scaled_values, -1, 1)\n",
    "            observations[:, i] = scaled_values\n",
    "\n",
    "        observations[:, -1] = self._get_position_state()\n",
    "        return observations.astype(np.float32)\n",
    "\n",
    "    def _get_position_state(self):\n",
    "        if self.position is None:\n",
    "            return 0.0\n",
    "        elif self.position == 'LONG':\n",
    "            return 1.0\n",
    "        else:\n",
    "            return -1.0\n",
    "    \n",
    "    def _calculate_unrealized_pnl(self, current_price):\n",
    "        if self.position is None or self.entry_price is None:\n",
    "            return 0.0\n",
    "        pnl = 0.0\n",
    "        if self.position == 'LONG':\n",
    "            pnl = (current_price - self.entry_price) / self.entry_price\n",
    "        elif self.position == 'SHORT':\n",
    "            pnl = (self.entry_price - current_price) / self.entry_price\n",
    "            \n",
    "        return pnl\n",
    "    \n",
    "    def _calculate_reward(self, action, current_price):\n",
    "        action_str = self.reverse_action_map[action]\n",
    "        \n",
    "        if action_str == 'CLOSE':\n",
    "            if self.position == 'LONG':\n",
    "                pnl = (current_price - self.entry_price) / self.entry_price\n",
    "            elif self.position == 'SHORT':\n",
    "                pnl = (self.entry_price - current_price) / self.entry_price\n",
    "            else:\n",
    "                return 0.0\n",
    "                \n",
    "            entry_commission = self.entry_price * self.commission_rate\n",
    "            entry_slippage = self.entry_price * self.slippage_factor\n",
    "            exit_commission = current_price * self.commission_rate\n",
    "            total_costs = (entry_commission + entry_slippage + exit_commission) / self.entry_price\n",
    "            \n",
    "            net_pnl = pnl - total_costs\n",
    "            \n",
    "            # Simple reward based on scaled net P&L\n",
    "            reward = net_pnl * 25\n",
    "            \n",
    "            # Bonus for profitable trades\n",
    "            if net_pnl > 0:\n",
    "                reward += 1.0\n",
    "                \n",
    "            self.trade_history.append({\n",
    "                'symbol': self.current_symbol,\n",
    "                'position': self.position,\n",
    "                'entry_step': self.position_entry_step,\n",
    "                'exit_step': self.current_step,\n",
    "                'entry_price': self.entry_price,\n",
    "                'exit_price': current_price,\n",
    "                'pnl': pnl,\n",
    "                'net_pnl': net_pnl,\n",
    "                'costs': total_costs,\n",
    "                'max_drawdown': self.max_adverse_move,\n",
    "                'duration': self.current_step - self.position_entry_step\n",
    "            })\n",
    "            \n",
    "            return reward\n",
    "        else:  # HOLD action\n",
    "            # Start with a very small positive bias\n",
    "            hold_reward = 0.001\n",
    "            \n",
    "            unrealized_pnl = self._calculate_unrealized_pnl(current_price)\n",
    "            if unrealized_pnl < 0 and abs(unrealized_pnl) > self.max_adverse_move:\n",
    "                self.max_adverse_move = abs(unrealized_pnl)\n",
    "            \n",
    "            # Create a stronger, continuous incentive for profitable positions\n",
    "            if unrealized_pnl > 0:\n",
    "                # Scaled reward proportional to profit size\n",
    "                hold_reward += unrealized_pnl * 5  # Significant scaling\n",
    "            \n",
    "            return hold_reward\n",
    "\n",
    "    def step(self, action):\n",
    "        signals = self.signals_data[self.current_symbol]\n",
    "        current_price = signals.iloc[self.current_step]['price']\n",
    "        \n",
    "        reward = self._calculate_reward(action, current_price)\n",
    "        \n",
    "        action_str = self.reverse_action_map[action]\n",
    "        \n",
    "        # This is the important part:\n",
    "        # Set done=True when action is CLOSE\n",
    "        done = False\n",
    "        \n",
    "        if action_str == 'CLOSE':\n",
    "            self.position = None\n",
    "            self.position_type = None\n",
    "            self.position_entry_step = None\n",
    "            self.entry_price = None\n",
    "            self.max_adverse_move = 0\n",
    "            \n",
    "            # Set done=True to end the episode\n",
    "            done = True\n",
    "        else:\n",
    "            self.current_step += 1\n",
    "            self.steps_in_trade += 1\n",
    "            # Also end episode if max steps reached or end of data\n",
    "            done = (self.steps_in_trade >= self.max_steps_per_trade) or (self.current_step >= len(signals) - 1)\n",
    "        \n",
    "        obs = self._get_observation()\n",
    "        \n",
    "        info = {\n",
    "            \"Reward\": reward,\n",
    "            \"Position\": self.position,\n",
    "            \"Position Type\": self.position_type,\n",
    "            \"Action\": action_str,\n",
    "            \"Current Price\": current_price,\n",
    "            \"Entry Price\": self.entry_price,\n",
    "            \"Current Symbol\": self.current_symbol,\n",
    "            \"Steps in Trade\": self.steps_in_trade,\n",
    "            \"Unrealized PnL\": self._calculate_unrealized_pnl(current_price)\n",
    "        }\n",
    "        \n",
    "        return obs, reward, done, False, info\n",
    "\n",
    "    def render(self, mode='human'):\n",
    "        pass\n",
    "\n",
    "    def close(self):\n",
    "        pass\n",
    "\n",
    "\n",
    "def make_env(signals_data, rank):\n",
    "    def _init():\n",
    "        env = Monitor(TradingProfitEnv(signals_data))\n",
    "        return env\n",
    "    return _init\n",
    "\n",
    "\n",
    "def train_profit_model(signals_data, total_timesteps=1_000_000, n_envs=8):\n",
    "    env_fns = [make_env(signals_data, i) for i in range(n_envs)]\n",
    "    vec_env = SubprocVecEnv(env_fns)\n",
    "    vec_env = VecNormalize(vec_env, norm_obs=True, norm_reward=True)\n",
    "    \n",
    "    checkpoint_callback = CheckpointCallback(\n",
    "        save_freq=10000,\n",
    "        save_path=SAVE_DIR,\n",
    "        name_prefix=\"profit_model\",\n",
    "        save_replay_buffer=False,\n",
    "        save_vecnormalize=True,\n",
    "    )\n",
    "    \n",
    "    model = PPO(\n",
    "        \"MlpPolicy\",\n",
    "        vec_env,\n",
    "        tensorboard_log=\"/Users/newuser/Projects/robust_algo_trader/drl/mean_rev_env_logs\",\n",
    "        learning_rate=3e-4,\n",
    "        n_steps=1024,\n",
    "        batch_size=64,\n",
    "        n_epochs=5,\n",
    "        gamma=0.99,\n",
    "        gae_lambda=0.95,\n",
    "        clip_range=0.2,\n",
    "        ent_coef=0.01,\n",
    "        verbose=1,\n",
    "        policy_kwargs=dict(\n",
    "            net_arch=[256, 128, 64],\n",
    "            activation_fn=th.nn.Tanh,\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    model.learn(\n",
    "        total_timesteps=total_timesteps,\n",
    "        callback=checkpoint_callback,\n",
    "        progress_bar=True\n",
    "    )\n",
    "    \n",
    "    final_model_path = os.path.join(SAVE_DIR, \"final_profit_model\")\n",
    "    model.save(final_model_path)\n",
    "    print(f\"Model saved to {final_model_path}\")\n",
    "    return model\n",
    "\n",
    "\n",
    "def evaluate_profit_model(model, signals_data, symbol, n_eval_episodes=20):\n",
    "    eval_data = {symbol: signals_data[symbol]}\n",
    "    eval_env = Monitor(TradingProfitEnv(eval_data))\n",
    "    \n",
    "    mean_reward, std_reward = evaluate_policy(\n",
    "        model, \n",
    "        eval_env, \n",
    "        n_eval_episodes=n_eval_episodes,\n",
    "        deterministic=True\n",
    "    )\n",
    "    \n",
    "    print(f\"Mean reward: {mean_reward:.2f} +/- {std_reward:.2f}\")\n",
    "    \n",
    "    all_trades = eval_env.unwrapped.trade_history\n",
    "    \n",
    "    if all_trades:\n",
    "        total_trades = len(all_trades)\n",
    "        profitable_trades = sum(1 for t in all_trades if t['net_pnl'] > 0)\n",
    "        win_rate = profitable_trades / total_trades if total_trades > 0 else 0\n",
    "        \n",
    "        avg_profit = np.mean([t['net_pnl'] for t in all_trades if t['net_pnl'] > 0]) if profitable_trades > 0 else 0\n",
    "        avg_loss = np.mean([t['net_pnl'] for t in all_trades if t['net_pnl'] <= 0]) if total_trades - profitable_trades > 0 else 0\n",
    "        \n",
    "        profit_factor = abs(sum([t['net_pnl'] for t in all_trades if t['net_pnl'] > 0])) / abs(sum([t['net_pnl'] for t in all_trades if t['net_pnl'] < 0])) if sum([t['net_pnl'] for t in all_trades if t['net_pnl'] < 0]) != 0 else float('inf')\n",
    "        \n",
    "        avg_trade_duration = np.mean([t['duration'] for t in all_trades])\n",
    "        \n",
    "        print(\"\\nTrading Performance Summary:\")\n",
    "        print(f\"Total Trades: {total_trades}\")\n",
    "        print(f\"Win Rate: {win_rate:.2%}\")\n",
    "        print(f\"Average Profit: {avg_profit:.2%}\")\n",
    "        print(f\"Average Loss: {avg_loss:.2%}\")\n",
    "        print(f\"Profit Factor: {profit_factor:.2f}\")\n",
    "        print(f\"Average Trade Duration: {avg_trade_duration:.1f} steps\")\n",
    "        \n",
    "        cumulative_returns = np.cumsum([t['net_pnl'] for t in all_trades])\n",
    "        \n",
    "        plt.figure(figsize=(12, 6))\n",
    "        plt.plot(cumulative_returns * 100)\n",
    "        plt.title(f\"Cumulative Returns for {symbol}\")\n",
    "        plt.xlabel(\"Trade Number\")\n",
    "        plt.ylabel(\"Cumulative Return (%)\")\n",
    "        plt.grid(True)\n",
    "        plt.savefig(os.path.join(SAVE_DIR, f\"{symbol}_returns.png\"))\n",
    "        plt.show()\n",
    "    \n",
    "    return all_trades\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "signals_data = {\n",
    "    'CRM': pd.read_csv(\"/Users/newuser/Projects/robust_algo_trader/data/gen_alpaca_data/CRM_M1_signals.csv\")\n",
    "}\n",
    "model = PPO.load(\"/Users/newuser/Projects/robust_algo_trader/models/model_20250407_14/final_profit_model\")\n",
    "symbol = 'CRM'\n",
    "\n",
    "# Starting portfolio value\n",
    "initial_capital = 10000  # $10,000 starting capital\n",
    "portfolio_values = [initial_capital]\n",
    "current_capital = initial_capital\n",
    "\n",
    "eval_data = {symbol: signals_data[symbol]}\n",
    "eval_env = Monitor(TradingProfitEnv(eval_data))\n",
    "\n",
    "# Run evaluation\n",
    "mean_reward, std_reward = evaluate_policy(\n",
    "    model, \n",
    "    Monitor(TradingProfitEnv(eval_data)), \n",
    "    n_eval_episodes=20,\n",
    "    deterministic=True\n",
    ")\n",
    "\n",
    "print(f\"Mean reward: {mean_reward:.2f} +/- {std_reward:.2f}\")\n",
    "\n",
    "# Custom evaluation to collect trades\n",
    "all_trades = []\n",
    "current_capital = initial_capital\n",
    "\n",
    "# Reset and run manual evaluation to collect trades - run for multiple episodes to ensure we get trades\n",
    "max_episodes = 5\n",
    "for episode in range(max_episodes):\n",
    "    print(f\"\\nRunning episode {episode+1}/{max_episodes}\")\n",
    "    \n",
    "    # Reset for each episode\n",
    "    obs, _ = eval_env.reset()\n",
    "    done = False\n",
    "    step_count = 0\n",
    "    \n",
    "    # Track episode state\n",
    "    print(f\"Initial position: {eval_env.unwrapped.position}\")\n",
    "    print(f\"Initial position type: {eval_env.unwrapped.position_type}\")\n",
    "    \n",
    "    # Run until the episode ends\n",
    "    while not done and step_count < 1000:  # Add step limit as safeguard\n",
    "        step_count += 1\n",
    "        action, _ = model.predict(obs, deterministic=True)\n",
    "        action_scalar = action.item() if isinstance(action, np.ndarray) else int(action)\n",
    "        \n",
    "        print(f\"Step {step_count}: Action={action_scalar}, Position={eval_env.unwrapped.position}\")\n",
    "        \n",
    "        # Take step\n",
    "        obs, reward, terminated, truncated, info = eval_env.step(action_scalar)\n",
    "        done = terminated or truncated\n",
    "        \n",
    "        # If a trade was closed, update portfolio\n",
    "        if info.get(\"Action\") == \"CLOSE\":\n",
    "            print(f\"CLOSE action detected at step {step_count}\")\n",
    "            \n",
    "            # Make sure trade history exists and has entries\n",
    "            if hasattr(eval_env.unwrapped, 'trade_history') and len(eval_env.unwrapped.trade_history) > 0:\n",
    "                # Get trade details\n",
    "                trade = eval_env.unwrapped.trade_history[-1]  # Most recent trade\n",
    "                all_trades.append(trade)\n",
    "                \n",
    "                # Apply trade P&L to portfolio\n",
    "                trade_pnl_percentage = trade['net_pnl']  # This is already a percentage\n",
    "                \n",
    "                # Assume we use 50% of our capital per trade\n",
    "                position_size = current_capital * 0.5\n",
    "                trade_pnl_dollars = position_size * trade_pnl_percentage\n",
    "                \n",
    "                # Update capital\n",
    "                current_capital += trade_pnl_dollars\n",
    "                \n",
    "                # Record portfolio value after this trade\n",
    "                portfolio_values.append(current_capital)\n",
    "                \n",
    "                print(f\"Trade {len(all_trades)}: {trade_pnl_percentage:.2%} profit, Portfolio: ${current_capital:.2f}\")\n",
    "            else:\n",
    "                print(f\"Warning: CLOSE action but no trade in history. trade_history exists: {hasattr(eval_env.unwrapped, 'trade_history')}\")\n",
    "    \n",
    "    print(f\"Episode {episode+1} completed after {step_count} steps\")\n",
    "    if hasattr(eval_env.unwrapped, 'trade_history'):\n",
    "        print(f\"Episode trade history length: {len(eval_env.unwrapped.trade_history)}\")\n",
    "\n",
    "print(f\"\\nEvaluation complete. Total trades collected: {len(all_trades)}\")\n",
    "\n",
    "if all_trades:\n",
    "    total_trades = len(all_trades)\n",
    "    profitable_trades = sum(1 for t in all_trades if t['net_pnl'] > 0)\n",
    "    win_rate = profitable_trades / total_trades if total_trades > 0 else 0\n",
    "    \n",
    "    avg_profit = np.mean([t['net_pnl'] for t in all_trades if t['net_pnl'] > 0]) if profitable_trades > 0 else 0\n",
    "    avg_loss = np.mean([t['net_pnl'] for t in all_trades if t['net_pnl'] <= 0]) if total_trades - profitable_trades > 0 else 0\n",
    "    \n",
    "    profit_factor = abs(sum([t['net_pnl'] for t in all_trades if t['net_pnl'] > 0])) / abs(sum([t['net_pnl'] for t in all_trades if t['net_pnl'] < 0])) if sum([t['net_pnl'] for t in all_trades if t['net_pnl'] < 0]) != 0 else float('inf')\n",
    "    \n",
    "    avg_trade_duration = np.mean([t['duration'] for t in all_trades])\n",
    "    \n",
    "    # Calculate portfolio metrics\n",
    "    total_return_pct = (current_capital - initial_capital) / initial_capital * 100\n",
    "    \n",
    "    print(\"\\nTrading Performance Summary:\")\n",
    "    print(f\"Total Trades: {total_trades}\")\n",
    "    print(f\"Win Rate: {win_rate:.2%}\")\n",
    "    print(f\"Average Profit: {avg_profit:.2%}\")\n",
    "    print(f\"Average Loss: {avg_loss:.2%}\")\n",
    "    print(f\"Profit Factor: {profit_factor:.2f}\")\n",
    "    print(f\"Average Trade Duration: {avg_trade_duration:.1f} steps\")\n",
    "    print(f\"\\nPortfolio Performance:\")\n",
    "    print(f\"Starting Capital: ${initial_capital:.2f}\")\n",
    "    print(f\"Ending Capital: ${current_capital:.2f}\")\n",
    "    print(f\"Total Return: {total_return_pct:.2f}%\")\n",
    "    \n",
    "    # Plot portfolio growth\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(portfolio_values)\n",
    "    plt.title(f\"Portfolio Growth - {symbol}\")\n",
    "    plt.xlabel(\"Trade Number\")\n",
    "    plt.ylabel(\"Portfolio Value ($)\")\n",
    "    plt.grid(True)\n",
    "    plt.savefig(os.path.join(SAVE_DIR, f\"{symbol}_portfolio.png\"))\n",
    "    plt.show()\n",
    "    \n",
    "    # Plot cumulative returns percentage\n",
    "    pct_changes = [(pv / initial_capital - 1) * 100 for pv in portfolio_values]\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(pct_changes)\n",
    "    plt.title(f\"Cumulative Percentage Return - {symbol}\")\n",
    "    plt.xlabel(\"Trade Number\")\n",
    "    plt.ylabel(\"Return (%)\")\n",
    "    plt.grid(True)\n",
    "    # plt.savefig(os.path.join(SAVE_DIR, f\"{symbol}_pct_return.png\"))\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"\\nNo trades were executed during evaluation.\")\n",
    "    print(\"Possible issues:\")\n",
    "    print(\"1. The agent may not be choosing CLOSE actions\")\n",
    "    print(\"2. The environment might not be recording trades properly\")\n",
    "    print(\"3. The trade_history attribute might be missing or not working as expected\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add these modifications to test if CLOSE actions work\n",
    "print(\"\\nTesting with forced CLOSE actions\")\n",
    "obs, _ = eval_env.reset()\n",
    "done = False\n",
    "step_count = 0\n",
    "\n",
    "# Force a CLOSE action after 20 steps\n",
    "hold_steps = 20\n",
    "\n",
    "while not done and step_count < 500:\n",
    "    step_count += 1\n",
    "    \n",
    "    # Force CLOSE action every 20 steps\n",
    "    if step_count % hold_steps == 0:\n",
    "        action_scalar = 1  # CLOSE\n",
    "        print(f\"Step {step_count}: FORCING Action=1 (CLOSE)\")\n",
    "    else:\n",
    "        action, _ = model.predict(obs, deterministic=True)\n",
    "        action_scalar = action.item() if isinstance(action, np.ndarray) else int(action)\n",
    "    \n",
    "    print(f\"Step {step_count}: Action={action_scalar}, Position={eval_env.unwrapped.position}\")\n",
    "    \n",
    "    # Take step\n",
    "    obs, reward, terminated, truncated, info = eval_env.step(action_scalar)\n",
    "    done = terminated or truncated\n",
    "    \n",
    "    # Print more info if we close a position\n",
    "    if action_scalar == 1:\n",
    "        print(f\"CLOSE results - trade_history length: {len(eval_env.unwrapped.trade_history) if hasattr(eval_env.unwrapped, 'trade_history') else 'N/A'}\")\n",
    "        print(f\"Info: {info}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs499f22",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
