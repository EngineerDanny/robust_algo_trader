{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "import os\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "from scipy.special import softmax\n",
    "import matplotlib.pyplot as plt\n",
    "from stable_baselines3 import PPO\n",
    "from sb3_contrib import RecurrentPPO\n",
    "from stable_baselines3.common.env_checker import check_env\n",
    "from stable_baselines3.common.callbacks import CheckpointCallback\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from stable_baselines3.common.vec_env import SubprocVecEnv, VecNormalize\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import torch as th\n",
    "import sys\n",
    "import tensorflow as tf\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save dir should add the current date and time\n",
    "SAVE_DIR = f\"/Users/newuser/Projects/robust_algo_trader/drl/models/model_{dt.datetime.now().strftime('%Y%m%d_%H')}\"\n",
    "os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "\n",
    "DATA_DIR = \"/Users/newuser/Projects/robust_algo_trader/data/gen_alpaca_data\"\n",
    "MASTER_SEED = 42\n",
    "\n",
    "\n",
    "class TradingImitationEnv(gym.Env):\n",
    "    metadata = {'render_modes': ['human']}\n",
    "    def __init__(self, datasets, lookback_window=60, max_episode_steps=1000):\n",
    "        super(TradingImitationEnv, self).__init__()\n",
    "\n",
    "        # Store datasets (dictionary of {symbol: {'data': df, 'actions': series}})\n",
    "        self.datasets = datasets\n",
    "        self.symbols = list(datasets.keys())\n",
    "        self.current_symbol = random.choice(self.symbols)\n",
    "        self.lookback_window = lookback_window\n",
    "\n",
    "        # Define features we want to use from the dataset\n",
    "        self.features = [\n",
    "            'distance_from_mean',\n",
    "            'distance_from_upper',\n",
    "            'distance_from_lower',\n",
    "            'rsi',\n",
    "            'bollinger_upper',\n",
    "            'bollinger_lower',\n",
    "            'bollinger_mid',\n",
    "            'range_strength',\n",
    "            'mean_reversion_probability',\n",
    "            'is_range_market'\n",
    "        ]\n",
    "        # Action mapping\n",
    "        self.action_map = {\n",
    "            'NOTHING': 0,\n",
    "            'HOLD': 1,\n",
    "            'CLOSE': 2,\n",
    "            'BUY': 3,\n",
    "            'SELL': 4\n",
    "        }\n",
    "        self.reverse_action_map = {v: k for k, v in self.action_map.items()}\n",
    "        self.action_space = spaces.Discrete(len(self.action_map))\n",
    "        self.observation_space = spaces.Box(\n",
    "            low=-1, \n",
    "            high=1, \n",
    "            shape=(lookback_window, len(self.features) + 1),\n",
    "            dtype=np.float32\n",
    "        )\n",
    "\n",
    "        self.current_step = self.lookback_window + 60\n",
    "        self.position = None\n",
    "        self.position_entry_step = None\n",
    "        self.last_action = None\n",
    "        self.max_episode_steps = max_episode_steps\n",
    "        self.steps_in_episode = 0\n",
    "\n",
    "    def reset(self, seed=None, options=None):\n",
    "        super().reset(seed=seed)\n",
    "\n",
    "        # Choose a random dataset for this episode\n",
    "        self.current_symbol = random.choice(self.symbols)\n",
    "        self.position = None\n",
    "        self.position_entry_step = None\n",
    "        self.last_action = None\n",
    "        self.steps_in_episode = 0\n",
    "\n",
    "        return self._get_observation(), {}\n",
    "\n",
    "    def _get_observation(self):\n",
    "        # Get current dataset\n",
    "        current_data = self.datasets[self.current_symbol]['data']\n",
    "\n",
    "        # Extract lookback window\n",
    "        start_idx = self.current_step - self.lookback_window\n",
    "        end_idx = self.current_step\n",
    "        window_data = current_data.iloc[start_idx:end_idx]\n",
    "\n",
    "        # Create features array [lookback_window, features]\n",
    "        observations = np.zeros((self.lookback_window, len(self.features) + 1), dtype=np.float32)\n",
    "\n",
    "        # Process each feature\n",
    "        for i, feature in enumerate(self.features):\n",
    "            feature_values = window_data[feature].astype(float).values.reshape(-1, 1)\n",
    "            scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "            scaled_values = scaler.fit_transform(feature_values)\n",
    "\n",
    "            # Add to observations\n",
    "            observations[:, i] = scaled_values.flatten()\n",
    "        # Add position state as the last feature for all timesteps\n",
    "        position_value = 0.0  # No position\n",
    "        if self.position == 'LONG':\n",
    "            position_value = 1.0\n",
    "        elif self.position == 'SHORT':\n",
    "            position_value = -1.0\n",
    "\n",
    "        observations[:, -1] = position_value\n",
    "        return observations.astype(np.float32)\n",
    "\n",
    "    def _get_position_state(self):\n",
    "        if self.position is None:\n",
    "            return 0.0\n",
    "        elif self.position == 'LONG':\n",
    "            return 1.0\n",
    "        else:  # SHORT\n",
    "            return -1.0\n",
    "\n",
    "    def get_expert_action(self):\n",
    "        expert_actions = self.datasets[self.current_symbol]['actions']\n",
    "        # Handle both DataFrame and Series cases\n",
    "        action_str = expert_actions.iloc[self.current_step]['action']\n",
    "        return self.action_map.get(action_str, 0) \n",
    "\n",
    "    def _check_valid_transition(self, action):\n",
    "        action_str = self.reverse_action_map[action]\n",
    "\n",
    "        # Valid transitions\n",
    "        if self.position is None:  # No position\n",
    "            if action_str in ['NOTHING', 'BUY', 'SELL']:\n",
    "                return True\n",
    "            return False\n",
    "\n",
    "        elif self.position == 'LONG':  # Long position\n",
    "            if action_str in ['HOLD', 'CLOSE']:\n",
    "                return True\n",
    "            return False\n",
    "\n",
    "        elif self.position == 'SHORT':  # Short position\n",
    "            if action_str in ['HOLD', 'CLOSE']:\n",
    "                return True\n",
    "            return False\n",
    "\n",
    "        return False\n",
    "\n",
    "    def _calculate_reward(self, action):\n",
    "        expert_action = self.get_expert_action()\n",
    "        match_reward = 1.0 if action == expert_action else -1.0\n",
    "        flow_reward = 0.5 if self._check_valid_transition(action) else -1.0\n",
    "        total_reward = match_reward + flow_reward\n",
    "        return total_reward\n",
    "\n",
    "    def step(self, action):\n",
    "        reward = self._calculate_reward(action)\n",
    "        action_str = self.reverse_action_map[action]\n",
    "\n",
    "        if action_str == 'BUY':\n",
    "            self.position = 'LONG'\n",
    "            self.position_entry_step = self.current_step\n",
    "        elif action_str == 'SELL':\n",
    "            self.position = 'SHORT'\n",
    "            self.position_entry_step = self.current_step\n",
    "        elif action_str == 'CLOSE':\n",
    "            self.position = None\n",
    "            self.position_entry_step = None\n",
    "\n",
    "        # Store last action\n",
    "        self.last_action = action_str\n",
    "        self.current_step += 1\n",
    "        self.steps_in_episode += 1\n",
    "\n",
    "        current_data = self.datasets[self.current_symbol]['data']\n",
    "        truncated = self.current_step >= len(current_data) - 1\n",
    "        terminated = self.steps_in_episode >= self.max_episode_steps\n",
    "        done = terminated or truncated\n",
    "        \n",
    "        if truncated:\n",
    "            self.current_step = self.lookback_window + 60\n",
    "\n",
    "        obs = self._get_observation() if not done else None\n",
    "        info = {\n",
    "            \"Reward\": reward,\n",
    "            \"Position\": self.position,\n",
    "            \"Action\": action_str,\n",
    "            \"Current Symbol\": self.current_symbol,\n",
    "            \"Done\": done\n",
    "        }\n",
    "        return obs, reward, terminated, truncated, info\n",
    "\n",
    "    def render(self, mode='human'):\n",
    "        pass\n",
    "\n",
    "    def close(self):\n",
    "        pass\n",
    "\n",
    "def make_env(datasets, lookback_window, rank):\n",
    "    def _init():\n",
    "        env = TradingImitationEnv(datasets, lookback_window)\n",
    "        env = Monitor(env)\n",
    "        return env\n",
    "    return _init\n",
    "\n",
    "def train_imitation_model(datasets, lookback_window=10, total_timesteps=1000000, n_envs=4):\n",
    "    # Create vectorized environment\n",
    "    env_fns = [make_env(datasets, lookback_window, i) for i in range(n_envs)]\n",
    "    vec_env = SubprocVecEnv(env_fns)\n",
    "    # vec_env = VecNormalize(vec_env, norm_obs=False, norm_reward=False)\n",
    "    \n",
    "    # Checkpoint callback\n",
    "    checkpoint_callback = CheckpointCallback(\n",
    "        save_freq=1_000,\n",
    "        save_path=SAVE_DIR,\n",
    "        name_prefix=\"imitation_model\",\n",
    "        save_replay_buffer=False,\n",
    "        save_vecnormalize=True,\n",
    "    )\n",
    "    \n",
    "    # Initialize model\n",
    "    model = RecurrentPPO(\n",
    "        \"MlpLstmPolicy\",\n",
    "        vec_env,\n",
    "        tensorboard_log=\"/Users/newuser/Projects/robust_algo_trader/drl/mean_rev_env_logs\",\n",
    "        learning_rate=3e-4,\n",
    "        n_steps=1024,\n",
    "        batch_size=64,\n",
    "        n_epochs=10,\n",
    "        gamma=0.99,\n",
    "        gae_lambda=0.95,\n",
    "        clip_range=0.2,\n",
    "        ent_coef=0.01,\n",
    "        verbose=1,\n",
    "        policy_kwargs=dict(\n",
    "            net_arch=dict(pi=[256, 256, 128, 64], \n",
    "                          vf=[256, 256, 256, 128]),\n",
    "            activation_fn=th.nn.Tanh,\n",
    "            lstm_hidden_size=128,  # Size of LSTM hidden states\n",
    "            n_lstm_layers=1, \n",
    "        )\n",
    "    )\n",
    "    \n",
    "    # Train the model\n",
    "    model.learn(\n",
    "        total_timesteps=total_timesteps,\n",
    "        callback=checkpoint_callback,\n",
    "        progress_bar=True\n",
    "    )\n",
    "    \n",
    "    # Save final model\n",
    "    final_model_path = os.path.join(SAVE_DIR, \"final_imitation_model\")\n",
    "    model.save(final_model_path)\n",
    "    print(f\"Model saved to {final_model_path}\")\n",
    "    return model\n",
    "\n",
    "def evaluate_imitation(model, datasets, symbol, lookback_window=60, n_eval_episodes=10):\n",
    "    # Create single environment for evaluation\n",
    "    eval_datasets = {symbol: datasets[symbol]}\n",
    "    eval_env = Monitor(TradingImitationEnv(eval_datasets, lookback_window))\n",
    "    \n",
    "    # Evaluate\n",
    "    mean_reward, std_reward = evaluate_policy(\n",
    "        model, \n",
    "        eval_env, \n",
    "        n_eval_episodes=n_eval_episodes,\n",
    "        deterministic=True\n",
    "    )\n",
    "    \n",
    "    print(f\"Mean reward: {mean_reward:.2f} +/- {std_reward:.2f}\")\n",
    "    \n",
    "    # Collect model actions for comparison\n",
    "    obs, _ = eval_env.reset()\n",
    "    done = False\n",
    "    actions = []\n",
    "    expert_actions = []\n",
    "    \n",
    "    while not done:\n",
    "        action, _ = model.predict(obs, deterministic=True)\n",
    "        expert_action = eval_env.unwrapped.get_expert_action()\n",
    "        \n",
    "        actions.append(eval_env.reverse_action_map[action])\n",
    "        expert_actions.append(eval_env.reverse_action_map[expert_action])\n",
    "        \n",
    "        obs, _, done, _, _ = eval_env.step(action)\n",
    "        if done:\n",
    "            break\n",
    "    \n",
    "    # Compare actions\n",
    "    correct = sum(1 for a, e in zip(actions, expert_actions) if a == e)\n",
    "    total = len(actions)\n",
    "    accuracy = correct / total if total > 0 else 0\n",
    "    \n",
    "    print(f\"Action matching accuracy: {accuracy:.2%} ({correct}/{total})\")\n",
    "    \n",
    "    # Print confusion matrix\n",
    "    action_types = ['NOTHING', 'HOLD', 'CLOSE', 'BUY', 'SELL']\n",
    "    confusion = np.zeros((len(action_types), len(action_types)), dtype=int)\n",
    "    \n",
    "    for pred, true in zip(actions, expert_actions):\n",
    "        pred_idx = action_types.index(pred)\n",
    "        true_idx = action_types.index(true)\n",
    "        confusion[true_idx, pred_idx] += 1\n",
    "    \n",
    "    print(\"\\nConfusion Matrix (rows: true, cols: predicted):\")\n",
    "    print(\"               \" + \" \".join(f\"{a:>8}\" for a in action_types))\n",
    "    for i, a in enumerate(action_types):\n",
    "        print(f\"{a:>8}\", end=\"\")\n",
    "        for j in range(len(action_types)):\n",
    "            print(f\"{confusion[i, j]:>9}\", end=\"\")\n",
    "        print()\n",
    "    \n",
    "    return actions, expert_actions, accuracy\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    datasets = {\n",
    "        'CRM': {\n",
    "            'data': pd.read_csv(os.path.join(DATA_DIR, 'CRM_M1_train_data.csv')), \n",
    "            'actions': pd.read_csv(os.path.join(DATA_DIR, 'CRM_M1_signals.csv'))   \n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Train model\n",
    "    model = train_imitation_model(datasets, lookback_window=60, total_timesteps=2_000)\n",
    "    \n",
    "    # Evaluate on one of the datasets\n",
    "    eval_symbol = 'CRM'\n",
    "    actions, expert_actions, accuracy = evaluate_imitation(model, datasets, eval_symbol)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs499f22",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
