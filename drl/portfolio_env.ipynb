{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "import matplotlib.pyplot as plt\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.env_checker import check_env\n",
    "from stable_baselines3.common.callbacks import CheckpointCallback\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "\n",
    "class PortfolioEnv(gym.Env):\n",
    "    \"\"\"\n",
    "    Portfolio allocation environment for reinforcement learning.\n",
    "    \n",
    "    This environment simulates a portfolio manager making monthly allocation decisions:\n",
    "    - Each month, the agent allocates capital to 10 stocks\n",
    "    - For each stock, it can allocate 0%, 10%, 20%, or 30%\n",
    "    - Total allocation must equal 100%\n",
    "    - The goal is to maximize returns while managing risk\n",
    "    \n",
    "    A single step represents allocating to one stock.\n",
    "    A full allocation cycle (10 steps) represents a complete portfolio allocation for one month.\n",
    "    The reward is calculated at the end of each month based on portfolio performance.\n",
    "    \"\"\"\n",
    "    \n",
    "    metadata = {'render_modes': ['human']}\n",
    "    \n",
    "    def __init__(self, data_dir: str, episode_length: int = 12):\n",
    "        \"\"\"\n",
    "        Initialize the portfolio environment.\n",
    "        \n",
    "        Args:\n",
    "            data_dir: Directory containing CSV files for each stock (stock_0.csv, stock_1.csv, etc.)\n",
    "            episode_length: Number of months in an episode (default: 12)\n",
    "        \"\"\"\n",
    "        super(PortfolioEnv, self).__init__()\n",
    "        \n",
    "        # Load stock data from individual CSV files\n",
    "        self.stocks = self._load_stock_data(data_dir)\n",
    "        self.n_stocks = len(self.stocks)\n",
    "        self.episode_length = episode_length\n",
    "        \n",
    "        # Key features used for state representation\n",
    "        self.features = [\n",
    "            'Close_scaled', 'MA5_scaled', 'MA20_scaled', 'MA50_scaled', 'MA200_scaled',\n",
    "            'RSI_scaled', 'BB_width_scaled', 'ATR_scaled', 'Return_1W_scaled',\n",
    "            'Return_1M_scaled', 'Return_3M_scaled', 'CurrentDrawdown_scaled',\n",
    "            'MaxDrawdown_252d_scaled', 'Sharpe_20d_scaled', 'Sharpe_60d_scaled'\n",
    "        ]\n",
    "        \n",
    "        # Define observation and action spaces\n",
    "        # State: 15 features per stock + current stock index + remaining allocation\n",
    "        obs_dim = len(self.features) * self.n_stocks + 2\n",
    "        self.observation_space = spaces.Box(\n",
    "            low=-10, high=10, \n",
    "            shape=(obs_dim,), \n",
    "            dtype=np.float32\n",
    "        )\n",
    "        \n",
    "        # Action space: 4 options (0%, 10%, 20%, 30%)\n",
    "        self.action_space = spaces.Discrete(4)\n",
    "        \n",
    "        # Determine the valid date range across all stocks\n",
    "        self.start_date, self.end_date = self._get_common_date_range()\n",
    "        \n",
    "        # Initialize\n",
    "        self.reset()\n",
    "    \n",
    "    def _load_stock_data(self, data_dir: str) -> Dict[str, pd.DataFrame]:\n",
    "        \"\"\"\n",
    "        Load stock data from individual CSV files.\n",
    "        \n",
    "        Args:\n",
    "            data_dir: Directory containing CSV files for each stock\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary mapping stock names to DataFrames\n",
    "        \"\"\"\n",
    "        stocks = {}\n",
    "        \n",
    "        # Look for CSV files in the directory\n",
    "        for i in range(10):  # Assuming 10 stocks\n",
    "            file_path = os.path.join(data_dir, f\"stock_{i}.csv\")\n",
    "            \n",
    "            # If the specific file doesn't exist, look for any CSV in the directory\n",
    "            if not os.path.exists(file_path):\n",
    "                # For demo purposes, we'll use the same data for all stocks if individual files don't exist\n",
    "                csv_files = [f for f in os.listdir(data_dir) if f.endswith('.csv')]\n",
    "                if not csv_files:\n",
    "                    raise FileNotFoundError(f\"No CSV files found in {data_dir}\")\n",
    "                \n",
    "                file_path = os.path.join(data_dir, csv_files[0])\n",
    "                print(f\"Warning: stock_{i}.csv not found, using {csv_files[0]} instead\")\n",
    "            \n",
    "            # Load and parse the data\n",
    "            df = pd.read_csv(file_path)\n",
    "            \n",
    "            # Ensure the DataFrame has a Date column and is sorted\n",
    "            if 'Date' in df.columns:\n",
    "                df['Date'] = pd.to_datetime(df['Date'])\n",
    "                df = df.sort_values('Date')\n",
    "            \n",
    "            stocks[f'stock_{i}'] = df\n",
    "        \n",
    "        return stocks\n",
    "    \n",
    "    def _get_common_date_range(self) -> Tuple[str, str]:\n",
    "        \"\"\"\n",
    "        Find the common date range across all stock data.\n",
    "        \n",
    "        Returns:\n",
    "            Tuple of (start_date, end_date) as string dates\n",
    "        \"\"\"\n",
    "        # Get the date range for each stock\n",
    "        start_dates = []\n",
    "        end_dates = []\n",
    "        \n",
    "        for stock_name, df in self.stocks.items():\n",
    "            if 'Date' in df.columns:\n",
    "                start_dates.append(df['Date'].min())\n",
    "                end_dates.append(df['Date'].max())\n",
    "            else:\n",
    "                # If no Date column, use index range\n",
    "                start_dates.append(0)\n",
    "                end_dates.append(len(df) - 1)\n",
    "        \n",
    "        # Find the common range\n",
    "        if all(isinstance(date, (pd.Timestamp, np.datetime64)) for date in start_dates):\n",
    "            start_date = max(start_dates)\n",
    "            end_date = min(end_dates)\n",
    "            \n",
    "            # Ensure there's enough data for a full episode\n",
    "            min_length = (self.episode_length + 1) * 30  # +1 for lookback\n",
    "            if (end_date - start_date).days < min_length:\n",
    "                raise ValueError(f\"Common date range too short for episode length {self.episode_length}\")\n",
    "            \n",
    "            return start_date.strftime('%Y-%m-%d'), end_date.strftime('%Y-%m-%d')\n",
    "        else:\n",
    "            # For index-based data, find common range\n",
    "            min_length = (self.episode_length + 1) * 30\n",
    "            max_start = max(start_dates)\n",
    "            min_end = min(end_dates)\n",
    "            \n",
    "            if min_end - max_start < min_length:\n",
    "                raise ValueError(f\"Common date range too short for episode length {self.episode_length}\")\n",
    "            \n",
    "            return str(max_start), str(min_end)\n",
    "    \n",
    "    def reset(self, seed=None, options=None):\n",
    "        \"\"\"\n",
    "        Reset environment for new episode.\n",
    "        \n",
    "        This method is called at the beginning of each episode. It:\n",
    "        1. Selects a random starting point in the data\n",
    "        2. Resets all portfolio allocations\n",
    "        3. Prepares the initial state\n",
    "        \n",
    "        Returns:\n",
    "            Initial observation and empty info dict\n",
    "        \"\"\"\n",
    "        super().reset(seed=seed)\n",
    "        \n",
    "        # Select a random starting point that allows for a full episode\n",
    "        data_length = len(next(iter(self.stocks.values())))\n",
    "        max_start_idx = data_length - self.episode_length * 30 - 20  # 20-day lookback\n",
    "        self.current_step = np.random.randint(20, max_start_idx)\n",
    "        self.current_month = 0\n",
    "        \n",
    "        # Reset allocation process\n",
    "        self.allocation = np.zeros(self.n_stocks)\n",
    "        self.remaining_allocation = 100\n",
    "        self.current_stock_idx = 0\n",
    "        \n",
    "        # Reset performance tracking\n",
    "        self.monthly_returns = []\n",
    "        \n",
    "        # Get initial state\n",
    "        observation = self._get_observation()\n",
    "        info = {}\n",
    "        \n",
    "        return observation, info\n",
    "    \n",
    "    def _get_observation(self):\n",
    "        \"\"\"\n",
    "        Create the state observation vector.\n",
    "        \n",
    "        The observation includes:\n",
    "        1. Features for each stock at the current time step\n",
    "        2. Which stock we're currently allocating to (normalized index)\n",
    "        3. How much allocation percentage remains\n",
    "        \n",
    "        Returns:\n",
    "            Numpy array of shape (obs_dim,) containing the observation\n",
    "        \"\"\"\n",
    "        observation = []\n",
    "        \n",
    "        # Get features for each stock\n",
    "        for stock_name, stock_data in self.stocks.items():\n",
    "            current_data = stock_data.iloc[self.current_step]\n",
    "            # Extract all required features\n",
    "            stock_features = []\n",
    "            for feature in self.features:\n",
    "                if feature in current_data:\n",
    "                    stock_features.append(current_data[feature])\n",
    "                else:\n",
    "                    # Use a default value if feature is missing\n",
    "                    stock_features.append(0.0)\n",
    "                    \n",
    "            observation.extend(stock_features)\n",
    "        \n",
    "        # Add contextual information\n",
    "        observation.append(self.current_stock_idx / self.n_stocks)  # Normalized index\n",
    "        observation.append(self.remaining_allocation / 100)  # Remaining allocation %\n",
    "        \n",
    "        return np.array(observation, dtype=np.float32)\n",
    "    \n",
    "    def step(self, action):\n",
    "        \"\"\"\n",
    "        Take an allocation action and advance the environment.\n",
    "        \n",
    "        For each step, the agent decides how much to allocate to the current stock.\n",
    "        After 10 steps (full allocation to all stocks), the environment advances by one month,\n",
    "        calculates portfolio performance, and gives a reward.\n",
    "        \n",
    "        Args:\n",
    "            action: Integer in [0, 1, 2, 3] corresponding to [0%, 10%, 20%, 30%]\n",
    "            \n",
    "        Returns:\n",
    "            observation: New state observation\n",
    "            reward: Reward (only non-zero at end of month)\n",
    "            terminated: Whether episode is terminated\n",
    "            truncated: Whether episode is truncated\n",
    "            info: Additional information\n",
    "        \"\"\"\n",
    "        # Convert action to allocation percentage\n",
    "        allocation_pct = action * 10\n",
    "        \n",
    "        # Check if action is valid\n",
    "        if allocation_pct > self.remaining_allocation:\n",
    "            # Adjust invalid actions to the maximum possible\n",
    "            if self.current_stock_idx == self.n_stocks - 1:\n",
    "                # For the last stock, must use exactly what's left\n",
    "                allocation_pct = self.remaining_allocation\n",
    "            else:\n",
    "                # For other invalid actions, use the highest valid option\n",
    "                valid_options = [0, 10, 20, 30]\n",
    "                valid_options = [opt for opt in valid_options if opt <= self.remaining_allocation]\n",
    "                if valid_options:\n",
    "                    allocation_pct = max(valid_options)\n",
    "                else:\n",
    "                    allocation_pct = 0\n",
    "        \n",
    "        # Apply allocation for current stock\n",
    "        self.allocation[self.current_stock_idx] = allocation_pct\n",
    "        self.remaining_allocation -= allocation_pct\n",
    "        self.current_stock_idx += 1\n",
    "        \n",
    "        reward = 0\n",
    "        terminated = False\n",
    "        truncated = False\n",
    "        info = {}\n",
    "        \n",
    "        # If we've allocated to all stocks (completed a month)\n",
    "        if self.current_stock_idx == self.n_stocks:\n",
    "            # For the last stock, adjust allocation to ensure sum is 100%\n",
    "            actual_sum = np.sum(self.allocation)\n",
    "            if actual_sum != 100:\n",
    "                # Adjust the last allocation to make the sum 100%\n",
    "                self.allocation[-1] += (100 - actual_sum)\n",
    "                self.remaining_allocation = 0\n",
    "            \n",
    "            # Calculate portfolio return for the month\n",
    "            portfolio_return, stock_returns = self._calculate_monthly_performance()\n",
    "            \n",
    "            # Calculate Sharpe and drawdown metrics\n",
    "            sharpe = self._calculate_portfolio_metric('Sharpe_20d_scaled')\n",
    "            max_drawdown = self._calculate_portfolio_metric('MaxDrawdown_252d_scaled')\n",
    "            \n",
    "            # Calculate reward\n",
    "            reward = self._calculate_reward(portfolio_return, sharpe, max_drawdown)\n",
    "            \n",
    "            # Track this month's return\n",
    "            self.monthly_returns.append(portfolio_return)\n",
    "            \n",
    "            # Add portfolio info\n",
    "            info = {\n",
    "                'portfolio_return': portfolio_return,\n",
    "                'sharpe': sharpe,\n",
    "                'max_drawdown': max_drawdown,\n",
    "                'allocation': self.allocation.copy(),\n",
    "                'stock_returns': stock_returns\n",
    "            }\n",
    "            \n",
    "            # Advance to next month\n",
    "            self.current_step += 30\n",
    "            self.current_month += 1\n",
    "            \n",
    "            # Check if episode is done\n",
    "            terminated = (self.current_month >= self.episode_length)\n",
    "            \n",
    "            # Reset for next month's allocation if not done\n",
    "            self.current_stock_idx = 0\n",
    "            self.remaining_allocation = 100\n",
    "        \n",
    "        # Get new state\n",
    "        observation = self._get_observation()\n",
    "        \n",
    "        return observation, reward, terminated, truncated, info\n",
    "    \n",
    "    def _calculate_monthly_performance(self):\n",
    "        \"\"\"\n",
    "        Calculate the portfolio return for the current month.\n",
    "        \n",
    "        Returns:\n",
    "            Tuple of (portfolio_return, stock_returns)\n",
    "        \"\"\"\n",
    "        # Get current prices\n",
    "        current_prices = np.array([\n",
    "            self.stocks[f'stock_{i}'].iloc[self.current_step]['Close'] \n",
    "            for i in range(self.n_stocks)\n",
    "        ])\n",
    "        \n",
    "        # Get prices at the end of the month\n",
    "        next_step = min(self.current_step + 30, len(next(iter(self.stocks.values()))) - 1)\n",
    "        next_prices = np.array([\n",
    "            self.stocks[f'stock_{i}'].iloc[next_step]['Close'] \n",
    "            for i in range(self.n_stocks)\n",
    "        ])\n",
    "        \n",
    "        # Calculate individual stock returns\n",
    "        stock_returns = (next_prices - current_prices) / current_prices\n",
    "        \n",
    "        # Calculate weighted portfolio return\n",
    "        portfolio_return = np.sum((self.allocation / 100) * stock_returns)\n",
    "        \n",
    "        return portfolio_return, stock_returns\n",
    "    \n",
    "    def _calculate_portfolio_metric(self, metric_name):\n",
    "        \"\"\"\n",
    "        Calculate a weighted portfolio metric based on current allocations.\n",
    "        \n",
    "        Args:\n",
    "            metric_name: Name of the metric to calculate (e.g., 'Sharpe_20d_scaled')\n",
    "            \n",
    "        Returns:\n",
    "            Weighted average of the metric across all stocks\n",
    "        \"\"\"\n",
    "        # Check if all stocks have this metric\n",
    "        if not all(metric_name in stock_df.columns for stock_df in self.stocks.values()):\n",
    "            return 0.0  # Default value if metric not available\n",
    "        \n",
    "        metric_values = np.array([\n",
    "            self.stocks[f'stock_{i}'].iloc[self.current_step][metric_name] \n",
    "            for i in range(self.n_stocks)\n",
    "        ])\n",
    "        return np.sum((self.allocation / 100) * metric_values)\n",
    "    \n",
    "    def _calculate_reward(self, portfolio_return, sharpe, max_drawdown):\n",
    "        \"\"\"\n",
    "        Calculate the reward based on portfolio performance.\n",
    "        \n",
    "        The reward function balances:\n",
    "        1. Return above target (1% monthly)\n",
    "        2. Risk-adjusted performance (Sharpe ratio)\n",
    "        3. Downside protection (avoiding drawdowns)\n",
    "        \n",
    "        Args:\n",
    "            portfolio_return: Monthly portfolio return\n",
    "            sharpe: Sharpe ratio\n",
    "            max_drawdown: Maximum drawdown\n",
    "            \n",
    "        Returns:\n",
    "            Calculated reward value\n",
    "        \"\"\"\n",
    "        # Base reward centered around 1% monthly return target\n",
    "        base_reward = (portfolio_return - 0.01) * 100\n",
    "        \n",
    "        # Adjust for risk metrics\n",
    "        risk_adjustment = sharpe * 0.5\n",
    "        drawdown_penalty = max_drawdown * -2.0\n",
    "        \n",
    "        # Extra penalty for negative returns\n",
    "        if portfolio_return < 0:\n",
    "            base_reward *= 1.5\n",
    "        \n",
    "        return base_reward + risk_adjustment + drawdown_penalty\n",
    "    \n",
    "    def render(self, mode='human'):\n",
    "        \"\"\"Render the current state of the environment\"\"\"\n",
    "        if self.current_stock_idx == 0:  # Just completed a month's allocation\n",
    "            print(f\"Month {self.current_month}\")\n",
    "            print(f\"Allocation: {self.allocation}\")\n",
    "            if self.monthly_returns:\n",
    "                print(f\"Last month return: {self.monthly_returns[-1]:.4f}\")\n",
    "    \n",
    "    def close(self):\n",
    "        \"\"\"Clean up resources\"\"\"\n",
    "        pass\n",
    "\n",
    "\n",
    "def train_and_evaluate(data_dir, save_dir='./models', total_timesteps=100000, eval_episodes=10):\n",
    "    \"\"\"\n",
    "    Train and evaluate a PPO agent on the portfolio allocation problem.\n",
    "    \n",
    "    This function:\n",
    "    1. Creates the portfolio environment\n",
    "    2. Validates that the environment works properly\n",
    "    3. Trains a PPO agent with the specified hyperparameters\n",
    "    4. Evaluates the trained agent\n",
    "    5. Plots the performance and allocation results\n",
    "    \n",
    "    Args:\n",
    "        data_dir: Directory containing stock CSV files\n",
    "        save_dir: Directory to save model checkpoints\n",
    "        total_timesteps: Number of timesteps to train for\n",
    "        eval_episodes: Number of episodes to use for evaluation\n",
    "        \n",
    "    Returns:\n",
    "        Trained model and evaluation results\n",
    "    \"\"\"\n",
    "    # Create and validate environment\n",
    "    print(\"Creating environment...\")\n",
    "    env = PortfolioEnv(data_dir)\n",
    "    \n",
    "    # Basic environment validation\n",
    "    check_env(env)\n",
    "    \n",
    "    # Initialize agent\n",
    "    print(\"Initializing PPO agent...\")\n",
    "    model = PPO(\n",
    "        \"MlpPolicy\", \n",
    "        env,\n",
    "        verbose=1,\n",
    "        learning_rate=3e-4,\n",
    "        gamma=0.99,  # Discount factor\n",
    "        n_steps=2048,  # Steps to collect before updating\n",
    "        ent_coef=0.01,  # Entropy coefficient (exploration)\n",
    "        vf_coef=0.5,  # Value function coefficient\n",
    "        max_grad_norm=0.5,  # Gradient clipping\n",
    "        policy_kwargs={'net_arch': [256, 128, dict(vf=[64], pi=[64])]}  # Network architecture\n",
    "    )\n",
    "    \n",
    "    # Create checkpoint callback for saving models\n",
    "    checkpoint_callback = CheckpointCallback(\n",
    "        save_freq=10000,  # Save every 10000 steps\n",
    "        save_path=save_dir,\n",
    "        name_prefix=\"ppo_portfolio\",\n",
    "        save_replay_buffer=False,\n",
    "        save_vecnormalize=True,\n",
    "    )\n",
    "    \n",
    "    # Train the agent\n",
    "    print(f\"Training for {total_timesteps} timesteps...\")\n",
    "    model.learn(\n",
    "        total_timesteps=total_timesteps,\n",
    "        callback=checkpoint_callback,\n",
    "        progress_bar=True\n",
    "    )\n",
    "    \n",
    "    # Save the final model\n",
    "    final_model_path = os.path.join(save_dir, \"ppo_portfolio_final\")\n",
    "    model.save(final_model_path)\n",
    "    print(f\"Final model saved to {final_model_path}\")\n",
    "    \n",
    "    # Evaluate the trained agent\n",
    "    print(f\"Evaluating agent over {eval_episodes} episodes...\")\n",
    "    eval_env = PortfolioEnv(data_dir)  # Create a separate env for evaluation\n",
    "    \n",
    "    # Basic evaluation for average reward\n",
    "    mean_reward, std_reward = evaluate_policy(\n",
    "        model, \n",
    "        eval_env, \n",
    "        n_eval_episodes=eval_episodes,\n",
    "        deterministic=True\n",
    "    )\n",
    "    \n",
    "    print(f\"Mean reward: {mean_reward:.4f} Â± {std_reward:.4f}\")\n",
    "    \n",
    "    # Detailed evaluation for visualization\n",
    "    results = detailed_evaluation(model, eval_env, n_episodes=eval_episodes)\n",
    "    \n",
    "    # Print summary statistics\n",
    "    print(\"\\nPerformance Summary:\")\n",
    "    print(f\"Average Monthly Return: {results['mean_return']:.4f}\")\n",
    "    print(f\"Average Sharpe Ratio: {results['mean_sharpe']:.4f}\")\n",
    "    print(f\"Average Max Drawdown: {results['mean_drawdown']:.4f}\")\n",
    "    \n",
    "    return model, results\n",
    "\n",
    "\n",
    "def detailed_evaluation(model, env, n_episodes=10):\n",
    "    \"\"\"\n",
    "    Perform a detailed evaluation of the trained model.\n",
    "    \n",
    "    This function:\n",
    "    1. Runs the model for multiple episodes\n",
    "    2. Collects detailed performance metrics\n",
    "    3. Generates visualizations of allocations and returns\n",
    "    \n",
    "    Args:\n",
    "        model: Trained PPO model\n",
    "        env: Portfolio environment\n",
    "        n_episodes: Number of episodes to evaluate\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary containing evaluation results\n",
    "    \"\"\"\n",
    "    # Storage for results\n",
    "    all_allocations = []\n",
    "    all_returns = []\n",
    "    all_sharpes = []\n",
    "    all_drawdowns = []\n",
    "    monthly_allocations = []\n",
    "    \n",
    "    for episode in range(n_episodes):\n",
    "        obs, info = env.reset()\n",
    "        episode_allocations = []\n",
    "        episode_returns = []\n",
    "        episode_sharpes = []\n",
    "        episode_drawdowns = []\n",
    "        done = False\n",
    "        \n",
    "        while not done:\n",
    "            # Get action from model\n",
    "            action, _ = model.predict(obs, deterministic=True)\n",
    "            \n",
    "            # Take step in environment\n",
    "            obs, reward, terminated, truncated, info = env.step(action)\n",
    "            \n",
    "            done = terminated or truncated\n",
    "            \n",
    "            # If we just completed a month, save the results\n",
    "            if 'portfolio_return' in info:\n",
    "                episode_allocations.append(info['allocation'])\n",
    "                episode_returns.append(info['portfolio_return'])\n",
    "                episode_sharpes.append(info['sharpe'])\n",
    "                episode_drawdowns.append(info['max_drawdown'])\n",
    "        \n",
    "        # Save episode results\n",
    "        all_allocations.append(episode_allocations)\n",
    "        monthly_allocations.extend(episode_allocations)\n",
    "        all_returns.append(np.mean(episode_returns))\n",
    "        all_sharpes.append(np.mean(episode_sharpes))\n",
    "        all_drawdowns.append(np.mean(episode_drawdowns))\n",
    "        \n",
    "        print(f\"Episode {episode+1}: Return = {np.mean(episode_returns):.4f}, Sharpe = {np.mean(episode_sharpes):.4f}\")\n",
    "    \n",
    "    # Calculate average allocation across all months\n",
    "    avg_allocation = np.mean(monthly_allocations, axis=0)\n",
    "    \n",
    "    # Create visualizations\n",
    "    create_visualizations(avg_allocation, all_returns, all_sharpes, all_drawdowns)\n",
    "    \n",
    "    # Return summary results\n",
    "    return {\n",
    "        'mean_return': np.mean(all_returns),\n",
    "        'mean_sharpe': np.mean(all_sharpes),\n",
    "        'mean_drawdown': np.mean(all_drawdowns),\n",
    "        'avg_allocation': avg_allocation\n",
    "    }\n",
    "\n",
    "\n",
    "def create_visualizations(avg_allocation, returns, sharpes, drawdowns):\n",
    "    \"\"\"\n",
    "    Create and save visualizations of the evaluation results.\n",
    "    \n",
    "    Args:\n",
    "        avg_allocation: Average allocation percentages across all episodes\n",
    "        returns: List of average returns for each episode\n",
    "        sharpes: List of average Sharpe ratios for each episode\n",
    "        drawdowns: List of average maximum drawdowns for each episode\n",
    "    \"\"\"\n",
    "    # Create output directory if it doesn't exist\n",
    "    os.makedirs('results', exist_ok=True)\n",
    "    \n",
    "    # Plot 1: Average allocation\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    bars = plt.bar(range(len(avg_allocation)), avg_allocation)\n",
    "    \n",
    "    # Add value labels on top of each bar\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        plt.text(bar.get_x() + bar.get_width()/2., height + 1,\n",
    "                 f'{height:.1f}%',\n",
    "                 ha='center', va='bottom', rotation=0)\n",
    "    \n",
    "    plt.xlabel('Stock')\n",
    "    plt.ylabel('Average Allocation (%)')\n",
    "    plt.title('Average Portfolio Allocation')\n",
    "    plt.xticks(range(len(avg_allocation)), [f'Stock {i}' for i in range(len(avg_allocation))])\n",
    "    plt.ylim(0, max(avg_allocation) * 1.2)  # Give some headroom for labels\n",
    "    plt.savefig('results/portfolio_allocation.png')\n",
    "    \n",
    "    # Plot 2: Returns distribution\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.hist(returns, bins=10, alpha=0.7)\n",
    "    plt.axvline(np.mean(returns), color='r', linestyle='dashed', linewidth=2)\n",
    "    plt.text(np.mean(returns)*1.1, plt.ylim()[1]*0.9, f'Mean: {np.mean(returns):.4f}')\n",
    "    plt.xlabel('Monthly Return')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.title('Distribution of Monthly Returns')\n",
    "    plt.savefig('results/returns_distribution.png')\n",
    "    \n",
    "    # Plot 3: Performance metrics comparison\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    metrics = ['Return', 'Sharpe', 'Drawdown']\n",
    "    values = [np.mean(returns), np.mean(sharpes), np.mean(drawdowns)]\n",
    "    colors = ['green', 'blue', 'red']\n",
    "    \n",
    "    bars = plt.bar(metrics, values, color=colors)\n",
    "    plt.title('Average Performance Metrics')\n",
    "    \n",
    "    # Add value labels\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        plt.text(bar.get_x() + bar.get_width()/2., height + 0.001,\n",
    "                 f'{height:.4f}',\n",
    "                 ha='center', va='bottom')\n",
    "    \n",
    "    plt.savefig('results/performance_metrics.png')\n",
    "    \n",
    "    print(\"Visualizations saved to 'results' directory\")\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Main entry point for the program.\n",
    "    \n",
    "    This function:\n",
    "    1. Gets the path to stock data\n",
    "    2. Trains and evaluates the model\n",
    "    \"\"\"\n",
    "    # Get directory containing stock CSV files\n",
    "    data_dir = input(\"Enter path to directory containing stock CSV files: \")\n",
    "    \n",
    "    if not os.path.exists(data_dir):\n",
    "        print(f\"Error: Directory {data_dir} does not exist\")\n",
    "        return\n",
    "    \n",
    "    # Create directory for model checkpoints\n",
    "    save_dir = './models'\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    \n",
    "    # Train and evaluate model\n",
    "    train_and_evaluate(\n",
    "        data_dir=data_dir,\n",
    "        save_dir=save_dir,\n",
    "        total_timesteps=100000,  # Adjust based on available computational resources\n",
    "        eval_episodes=10\n",
    "    )\n",
    "    \n",
    "    print(\"Training and evaluation complete!\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs499f22",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
