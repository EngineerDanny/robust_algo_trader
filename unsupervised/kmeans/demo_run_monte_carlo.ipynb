{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "from sktime.forecasting.model_selection import SlidingWindowSplitter\n",
    "import os\n",
    "import sys\n",
    "\n",
    "sys.path.append(\n",
    "    os.path.abspath(\n",
    "        \"/projects/genomic-ml/da2343/ml_project_2/unsupervised/kmeans/utils.py\"\n",
    "    )\n",
    ")\n",
    "from utils import *\n",
    "\n",
    "\n",
    "# Assume the RandomStartSlidingWindowSplitter class is defined here or imported\n",
    "\n",
    "# Generate synthetic time series data\n",
    "np.random.seed(42)\n",
    "dates = pd.date_range(start='2020-01-01', end='2022-12-31', freq='D')\n",
    "n = len(dates)\n",
    "trend = np.linspace(0, 100, n)\n",
    "seasonality = 10 * np.sin(2 * np.pi * np.arange(n) / 365.25)\n",
    "noise = np.random.normal(0, 5, n)\n",
    "y = trend + seasonality + noise\n",
    "\n",
    "# Create features (using lag features for this example)\n",
    "def create_features(y, lag=30):\n",
    "    df = pd.DataFrame({'y': y, 'ds': dates})\n",
    "    for i in range(1, lag+1):\n",
    "        df[f'lag_{i}'] = df['y'].shift(i)\n",
    "    df['month'] = df['ds'].dt.month\n",
    "    df['day'] = df['ds'].dt.day\n",
    "    return df.dropna().reset_index(drop=True)\n",
    "\n",
    "df = create_features(y)\n",
    "X = df.drop(['y', 'ds'], axis=1)\n",
    "y = df['y']\n",
    "\n",
    "# Initialize our custom splitter\n",
    "splitter = RandomStartSlidingWindowSplitter(n_splits=10, \n",
    "                                            train_size=500, \n",
    "                                            test_size=10, \n",
    "                                            randomness=0.5)\n",
    "\n",
    "# Prepare for storing results\n",
    "mse_scores = []\n",
    "predictions = []\n",
    "true_values = []\n",
    "\n",
    "# Perform cross-validation\n",
    "for fold, (train_index, test_index) in enumerate(splitter.split(X)):\n",
    "    print()\n",
    "    print(f\"Fold {fold}\")\n",
    "    print(f\"Length of train set: {len(train_index)}\")\n",
    "    print(f\"Length of test set: {len(test_index)}\")\n",
    "    \n",
    "    print(f\"Train indices: {train_index}\")\n",
    "    print(f\"Test indices: {test_index}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import shutil\n",
    "import sys\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "# Load the config file\n",
    "config_path = \"/projects/genomic-ml/da2343/ml_project_2/settings/config.json\"\n",
    "with open(config_path) as f:\n",
    "  config = json.load(f) \n",
    "config_settings = config[\"trading_settings\"]\n",
    "\n",
    "params_df_list = []\n",
    "params_dict = {\n",
    "    'max_cluster_labels': [1, 2, 5],\n",
    "    'price_history_length': [24],\n",
    "    'num_perceptually_important_points': [5],\n",
    "    'distance_measure': [1],\n",
    "    'num_clusters': [70, 80, 90, 100, 110, 120],\n",
    "    'atr_multiplier': [10],\n",
    "    'clustering_algorithm': ['kmeans', 'gaussian_mixture'],\n",
    "    # 'random_seed': np.arange(1, 100),\n",
    "    'random_seed': [1, 2, 4, 7, 10, 12, 15, 18, 20, 21, 42, 50, 80, 90, 100, 200, 300],\n",
    "    'train_period': [30, 40, 50, 60, 70, 80, 90], # days   \n",
    "    'test_period': [10] # days\n",
    "}\n",
    "params_df = pd.MultiIndex.from_product(\n",
    "    params_dict.values(),\n",
    "    names=params_dict.keys()\n",
    ").to_frame().reset_index(drop=True)\n",
    "params_df_list.append(params_df)\n",
    "params_concat_df = pd.concat(params_df_list, ignore_index=True)\n",
    "params_concat_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict = {\n",
    "    'train_period': [3840, 4800, 4800, 4800, 4800, 8640, 8640, 8640, 8640],\n",
    "    'random_seed': [20, 10, 20, 42, 200, 7, 7, 10, 90],\n",
    "    'num_clusters': [110, 80, 80, 70, 90, 80, 80, 90, 70],\n",
    "    'clustering_algorithm': 8 * ['gaussian_mixture'] + ['kmeans'],\n",
    "    'max_cluster_labels': [2, 2, 1, 1, 1, 1, 2, 2, 2],\n",
    "    \n",
    "    'num_perceptually_important_points': 9 * [5],\n",
    "    'distance_measure': 9 *[1],\n",
    "    'atr_multiplier': 9 * [10],\n",
    "    'price_history_length': 9 * [24],\n",
    "    'test_period': 9 * [960],\n",
    "}\n",
    "params_concat_df = pd.DataFrame(data_dict)\n",
    "params_concat_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_concat_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_tasks, ncol = params_concat_df.shape\n",
    "date_time = datetime.now().strftime(\"%Y-%m-%d_%H:%M\")\n",
    "job_name = f\"ml_project_2_{date_time}\"\n",
    "job_dir = \"/scratch/da2343/\" + job_name\n",
    "results_dir = os.path.join(job_dir, \"results\")\n",
    "os.system(\"mkdir -p \" + results_dir)\n",
    "params_concat_df.to_csv(os.path.join(job_dir, \"params.csv\"), index=False)\n",
    "\n",
    "run_one_contents = f\"\"\"#!/bin/bash\n",
    "#SBATCH --array=0-{n_tasks-1}\n",
    "#SBATCH --time=24:00:00\n",
    "#SBATCH --mem=2GB\n",
    "#SBATCH --cpus-per-task=1\n",
    "#SBATCH --error={job_dir}/slurm-%A_%a.out\n",
    "#SBATCH --output={job_dir}/slurm-%A_%a.out\n",
    "#SBATCH --job-name={job_name}\n",
    "cd {job_dir}\n",
    "python run_one.py $SLURM_ARRAY_TASK_ID\n",
    "\"\"\"\n",
    "run_one_sh = os.path.join(job_dir, \"run_one.sh\")\n",
    "with open(run_one_sh, \"w\") as run_one_f:\n",
    "    run_one_f.write(run_one_contents)\n",
    "\n",
    "# run_orig_py = \"demo_run.py\"\n",
    "run_orig_py = \"demo_run_optimized.py\"\n",
    "run_one_py = os.path.join(job_dir, \"run_one.py\")\n",
    "shutil.copyfile(run_orig_py, run_one_py)\n",
    "orig_dir = os.path.dirname(run_orig_py)\n",
    "orig_results = os.path.join(orig_dir, \"results\")\n",
    "os.system(\"mkdir -p \" + orig_results)\n",
    "orig_csv = os.path.join(orig_dir, \"params.csv\")\n",
    "params_concat_df.to_csv(orig_csv, index=False)\n",
    "\n",
    "msg = f\"\"\"created params CSV files and job scripts, test with\n",
    "python {run_orig_py}\n",
    "SLURM_ARRAY_TASK_ID=0 bash {run_one_sh}\"\"\"\n",
    "print(msg)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs685",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
