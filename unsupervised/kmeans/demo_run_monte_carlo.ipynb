{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import Tuple, Union, Iterator\n",
    "\n",
    "class RandomStartWindowSplitter:\n",
    "    def __init__(self, window_length: int, fh: Union[int, list, np.ndarray], \n",
    "                 n_splits: int = None, random_state: int = None):\n",
    "        \"\"\"\n",
    "        Initialize the RandomStartWindowSplitter.\n",
    "        \n",
    "        Args:\n",
    "            window_length (int): The size of each window.\n",
    "            fh (int, list, or np.ndarray): Forecasting horizon, relative time points to forecast.\n",
    "            n_splits (int, optional): Number of random splits to generate. If None, uses max possible unique splits.\n",
    "            random_state (int, optional): Seed for random number generator.\n",
    "        \"\"\"\n",
    "        if window_length <= 0:\n",
    "            raise ValueError(\"window_length must be a positive integer\")\n",
    "        self.window_length = window_length\n",
    "        \n",
    "        if isinstance(fh, (int, list, np.ndarray)):\n",
    "            self.fh = np.array([fh] if isinstance(fh, int) else fh)\n",
    "        else:\n",
    "            raise ValueError(\"fh must be an int, list, or numpy array\")\n",
    "        \n",
    "        if len(self.fh) == 0:\n",
    "            raise ValueError(\"fh must not be empty\")\n",
    "        \n",
    "        self.min_fh = min(self.fh)\n",
    "        self.max_fh = max(self.fh)\n",
    "        \n",
    "        if self.min_fh <= 0:\n",
    "            raise ValueError(\"All values in fh must be positive\")\n",
    "        \n",
    "        self.n_splits = n_splits\n",
    "        self.random_state = random_state\n",
    "        self.rng = np.random.default_rng(self.random_state)\n",
    "\n",
    "    def split(self, X: Union[np.ndarray, pd.DataFrame, pd.Series], \n",
    "              y: Union[np.ndarray, pd.Series, None] = None) -> Iterator[Tuple[np.ndarray, np.ndarray]]:\n",
    "        \"\"\"\n",
    "        Split the input data into random windows, allowing duplicates if necessary.\n",
    "        \n",
    "        Args:\n",
    "            X (np.ndarray, pd.DataFrame, or pd.Series): Input time series data.\n",
    "            y (np.ndarray, pd.Series, optional): Target values. If provided, should have the same length as X.\n",
    "        \n",
    "        Yields:\n",
    "            Tuple of train and test indices for each split.\n",
    "        \"\"\"\n",
    "        n_samples = self._get_n_samples(X)\n",
    "        indices = self._get_indices(X)\n",
    "\n",
    "        if n_samples < self.window_length + self.max_fh:\n",
    "            raise ValueError(f\"Insufficient data: n_samples ({n_samples}) must be at least window_length ({self.window_length}) + max(fh) ({self.max_fh})\")\n",
    "\n",
    "        min_start = self.window_length\n",
    "        max_start = n_samples - self.window_length - self.max_fh + 1\n",
    "        available_starts = max_start - min_start\n",
    "\n",
    "        if self.n_splits is None:\n",
    "            self.n_splits = available_starts\n",
    "\n",
    "        # Generate all possible start indices and shuffle them\n",
    "        all_start_indices = np.arange(min_start, max_start)\n",
    "        self.rng.shuffle(all_start_indices)\n",
    "        \n",
    "        # Use modulo to allow for wrapping around when n_splits > available_starts\n",
    "        for i in range(self.n_splits):\n",
    "            start_idx = all_start_indices[i % available_starts]\n",
    "            train_start = start_idx\n",
    "            train_end = train_start + self.window_length\n",
    "            test_indices = train_end - 1 + self.fh\n",
    "            \n",
    "            yield indices[train_start:train_end], indices[test_indices]\n",
    "\n",
    "    def get_n_splits(self, X: Union[np.ndarray, pd.DataFrame, pd.Series], \n",
    "                     y: Union[np.ndarray, pd.Series, None] = None) -> int:\n",
    "        \"\"\"\n",
    "        Returns the number of splitting iterations.\n",
    "        \n",
    "        Args:\n",
    "            X (np.ndarray, pd.DataFrame, or pd.Series): Input time series data.\n",
    "            y (np.ndarray, pd.Series, optional): Target values. Not used, present for API consistency.\n",
    "        \n",
    "        Returns:\n",
    "            int: Number of splits (same as n_splits or max possible unique splits).\n",
    "        \"\"\"\n",
    "        if self.n_splits is None:\n",
    "            n_samples = self._get_n_samples(X)\n",
    "            min_start = self.window_length\n",
    "            max_start = n_samples - self.window_length - self.max_fh + 1\n",
    "            return max_start - min_start\n",
    "        return self.n_splits\n",
    "\n",
    "    def _get_n_samples(self, X: Union[np.ndarray, pd.DataFrame, pd.Series]) -> int:\n",
    "        \"\"\"Helper method to get the number of samples in X.\"\"\"\n",
    "        if isinstance(X, (pd.DataFrame, pd.Series)):\n",
    "            return len(X)\n",
    "        elif isinstance(X, np.ndarray):\n",
    "            return X.shape[0] if X.ndim > 1 else len(X)\n",
    "        else:\n",
    "            raise ValueError(\"X must be a numpy array, pandas DataFrame, or pandas Series\")\n",
    "\n",
    "    def _get_indices(self, X: Union[np.ndarray, pd.DataFrame, pd.Series]) -> np.ndarray:\n",
    "        \"\"\"Helper method to get the indices of X.\"\"\"\n",
    "        if isinstance(X, (pd.DataFrame, pd.Series)):\n",
    "            return X.index.values\n",
    "        elif isinstance(X, np.ndarray):\n",
    "            return np.arange(self._get_n_samples(X))\n",
    "        else:\n",
    "            raise ValueError(\"X must be a numpy array, pandas DataFrame, or pandas Series\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "from sktime.forecasting.model_selection import SlidingWindowSplitter\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# sys.path.append(\n",
    "#     os.path.abspath(\n",
    "#         \"/projects/genomic-ml/da2343/ml_project_2/unsupervised/kmeans/utils.py\"\n",
    "#     )\n",
    "# )\n",
    "# from utils import *\n",
    "\n",
    "\n",
    "# Assume the RandomStartSlidingWindowSplitter class is defined here or imported\n",
    "\n",
    "# Generate synthetic time series data\n",
    "np.random.seed(42)\n",
    "dates = pd.date_range(start='2020-01-01', end='2022-12-31', freq='D')\n",
    "n = len(dates)\n",
    "trend = np.linspace(0, 100, n)\n",
    "seasonality = 10 * np.sin(2 * np.pi * np.arange(n) / 365.25)\n",
    "noise = np.random.normal(0, 5, n)\n",
    "y = trend + seasonality + noise\n",
    "\n",
    "# Create features (using lag features for this example)\n",
    "def create_features(y, lag=30):\n",
    "    df = pd.DataFrame({'y': y, 'ds': dates})\n",
    "    for i in range(1, lag+1):\n",
    "        df[f'lag_{i}'] = df['y'].shift(i)\n",
    "    df['month'] = df['ds'].dt.month\n",
    "    df['day'] = df['ds'].dt.day\n",
    "    return df.dropna().reset_index(drop=True)\n",
    "\n",
    "df = create_features(y)\n",
    "X = df.drop(['y', 'ds'], axis=1)\n",
    "y = df['y']\n",
    "\n",
    "\n",
    "# Initialize our custom splitter\n",
    "splitter = RandomStartWindowSplitter(\n",
    "    window_length=500,\n",
    "    fh=np.arange(1, 10 + 1),\n",
    "    n_splits=10,\n",
    "    random_state=42  # optional, for reproducibility\n",
    ")\n",
    "\n",
    "# Prepare for storing results\n",
    "mse_scores = []\n",
    "predictions = []\n",
    "true_values = []\n",
    "\n",
    "# Perform cross-validation\n",
    "for fold, (train_index, test_index) in enumerate(splitter.split(X)):\n",
    "    print()\n",
    "    print(f\"Fold {fold}\")\n",
    "    print(f\"Length of train set: {len(train_index)}\")\n",
    "    print(f\"Length of test set: {len(test_index)}\")\n",
    "    \n",
    "    print(f\"Train indices: {train_index}\")\n",
    "    print(f\"Test indices: {test_index}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import *\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sktime.forecasting.model_selection import SlidingWindowSplitter\n",
    "import talib\n",
    "import warnings\n",
    "from numba import jit\n",
    "import joblib\n",
    "import os\n",
    "import sys\n",
    "import multiprocessing\n",
    "from functools import partial\n",
    "\n",
    "# sys.path.append(\n",
    "#     os.path.abspath(\n",
    "#         \"/projects/genomic-ml/da2343/ml_project_2/unsupervised/kmeans/utils.py\"\n",
    "#     )\n",
    "# )\n",
    "# from utils import *\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "@jit(nopython=True)\n",
    "def calculate_sharpe_ratio(returns, risk_free_rate):\n",
    "    excess_returns = returns - risk_free_rate\n",
    "    return np.mean(excess_returns) / np.std(returns)\n",
    "\n",
    "\n",
    "@jit(nopython=True)\n",
    "def find_perceptually_important_points(price_data, num_points, distance_measure):\n",
    "    point_indices = np.zeros(num_points, dtype=np.int64)\n",
    "    point_prices = np.zeros(num_points, dtype=np.float64)\n",
    "    point_indices[0], point_indices[1] = 0, len(price_data) - 1\n",
    "    point_prices[0], point_prices[1] = price_data[0], price_data[-1]\n",
    "\n",
    "    for current_point in range(2, num_points):\n",
    "        max_distance, max_distance_index, insert_index = 0.0, -1, -1\n",
    "        for i in range(1, len(price_data) - 1):\n",
    "            left_adj = (\n",
    "                np.searchsorted(point_indices[:current_point], i, side=\"right\") - 1\n",
    "            )\n",
    "            right_adj = left_adj + 1\n",
    "            distance = calculate_point_distance(\n",
    "                price_data,\n",
    "                point_indices[:current_point],\n",
    "                point_prices[:current_point],\n",
    "                i,\n",
    "                left_adj,\n",
    "                right_adj,\n",
    "                distance_measure,\n",
    "            )\n",
    "            if distance > max_distance:\n",
    "                max_distance, max_distance_index, insert_index = distance, i, right_adj\n",
    "\n",
    "        point_indices[insert_index + 1 : current_point + 1] = point_indices[\n",
    "            insert_index:current_point\n",
    "        ]\n",
    "        point_prices[insert_index + 1 : current_point + 1] = point_prices[\n",
    "            insert_index:current_point\n",
    "        ]\n",
    "        point_indices[insert_index], point_prices[insert_index] = (\n",
    "            max_distance_index,\n",
    "            price_data[max_distance_index],\n",
    "        )\n",
    "\n",
    "    return point_indices, point_prices\n",
    "\n",
    "\n",
    "@jit(nopython=True)\n",
    "def calculate_point_distance(\n",
    "    data, point_indices, point_prices, index, left_adj, right_adj, distance_measure\n",
    "):\n",
    "    time_diff = point_indices[right_adj] - point_indices[left_adj]\n",
    "    price_diff = point_prices[right_adj] - point_prices[left_adj]\n",
    "    slope = price_diff / time_diff\n",
    "    intercept = point_prices[left_adj] - point_indices[left_adj] * slope\n",
    "    x, y = index, data[index]\n",
    "\n",
    "    if distance_measure == 1:\n",
    "        return (\n",
    "            (point_indices[left_adj] - x) ** 2 + (point_prices[left_adj] - y) ** 2\n",
    "        ) ** 0.5 + (\n",
    "            (point_indices[right_adj] - x) ** 2 + (point_prices[right_adj] - y) ** 2\n",
    "        ) ** 0.5\n",
    "    elif distance_measure == 2:\n",
    "        return abs((slope * x + intercept) - y) / (slope**2 + 1) ** 0.5\n",
    "    else:  # distance_measure == 3\n",
    "        return abs((slope * x + intercept) - y)\n",
    "\n",
    "\n",
    "@jit(nopython=True)\n",
    "def determine_trade_outcome(future_highs, future_lows, take_profit, stop_loss):\n",
    "    if future_highs[0] >= take_profit:\n",
    "        return 1\n",
    "    if future_lows[0] <= stop_loss:\n",
    "        return -1\n",
    "\n",
    "    tp_hit = np.argmax(future_highs >= take_profit)\n",
    "    sl_hit = np.argmax(future_lows <= stop_loss)\n",
    "\n",
    "    if tp_hit == 0 and sl_hit == 0:\n",
    "        return 0\n",
    "    elif (tp_hit < sl_hit and tp_hit != 0) or (tp_hit != 0 and sl_hit == 0):\n",
    "        return 1\n",
    "    elif (sl_hit < tp_hit and sl_hit != 0) or (sl_hit != 0 and tp_hit == 0):\n",
    "        return -1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "\n",
    "def prepare_training_data(price_subset, params):\n",
    "    training_data_list = []\n",
    "    scaler = StandardScaler()\n",
    "\n",
    "    for index in range(params[\"price_history_length\"], len(price_subset)):\n",
    "        price_history = (\n",
    "            price_subset[\"close\"]\n",
    "            .iloc[max(0, index - params[\"price_history_length\"]) : index]\n",
    "            .values\n",
    "        )\n",
    "        if len(price_history) < params[\"price_history_length\"]:\n",
    "            break\n",
    "\n",
    "        _, important_points = find_perceptually_important_points(\n",
    "            price_history,\n",
    "            params[\"num_perceptually_important_points\"],\n",
    "            params[\"distance_measure\"],\n",
    "        )\n",
    "        scaled_points = scaler.fit_transform(important_points.reshape(-1, 1)).flatten()\n",
    "\n",
    "        data_point = {\n",
    "            f\"price_point_{i}\": scaled_points[i]\n",
    "            for i in range(params[\"num_perceptually_important_points\"])\n",
    "        }\n",
    "        data_point.update(\n",
    "            price_subset.iloc[index - 1][\n",
    "                [\"year\", \"month\", \"day_of_week\", \"hour\", \"minute\"]\n",
    "            ].to_dict()\n",
    "        )\n",
    "\n",
    "        current_price = price_subset[\"close\"].iloc[index - 1]\n",
    "        current_atr = price_subset[\"atr_clipped\"].iloc[index - 1]\n",
    "        take_profit = current_price + (params[\"atr_multiplier\"] * current_atr)\n",
    "        stop_loss = current_price - (params[\"atr_multiplier\"] * current_atr)\n",
    "\n",
    "        future_highs = price_subset[\"high\"].iloc[index:].values\n",
    "        future_lows = price_subset[\"low\"].iloc[index:].values\n",
    "\n",
    "        if len(future_highs) > 0:\n",
    "            data_point[\"trade_outcome\"] = determine_trade_outcome(\n",
    "                future_highs, future_lows, take_profit, stop_loss\n",
    "            )\n",
    "        else:\n",
    "            data_point[\"trade_outcome\"] = 0\n",
    "\n",
    "        training_data_list.append(data_point)\n",
    "\n",
    "    return pd.DataFrame(training_data_list)\n",
    "\n",
    "\n",
    "def cluster_and_evaluate_price_data(price_data_df, params):\n",
    "    price_features = price_data_df[\n",
    "        [f\"price_point_{i}\" for i in range(params[\"num_perceptually_important_points\"])]\n",
    "        + [\"day_of_week\", \"hour\", \"minute\"]\n",
    "    ].values\n",
    "\n",
    "    clustering_models = {\n",
    "        \"kmeans\": KMeans(\n",
    "            n_clusters=params[\"num_clusters\"], random_state=params[\"random_seed\"]\n",
    "        ),\n",
    "        \"gaussian_mixture\": GaussianMixture(\n",
    "            n_components=params[\"num_clusters\"],\n",
    "            covariance_type=\"tied\",\n",
    "            random_state=params[\"random_seed\"],\n",
    "        ),\n",
    "    }\n",
    "\n",
    "    clustering_model = clustering_models[params[\"clustering_algorithm\"]]\n",
    "    clustering_model.fit(price_features)\n",
    "    price_data_df[\"cluster_label\"] = clustering_model.predict(price_features)\n",
    "\n",
    "    top_clusters_df = (\n",
    "        price_data_df.groupby(\"cluster_label\")[\"trade_outcome\"]\n",
    "        .sum()\n",
    "        .abs()\n",
    "        .nlargest(params[\"max_cluster_labels\"])\n",
    "        .reset_index()\n",
    "    )\n",
    "\n",
    "    best_clusters_list = []\n",
    "    for cluster_label in top_clusters_df[\"cluster_label\"]:\n",
    "        cluster_trade_outcomes = price_data_df[\n",
    "            price_data_df[\"cluster_label\"] == cluster_label\n",
    "        ][\"trade_outcome\"].values\n",
    "        metrics = calculate_trading_metrics(\n",
    "            cluster_trade_outcomes, params[\"initial_capital\"]\n",
    "        )\n",
    "        best_clusters_list.append(\n",
    "            {\n",
    "                \"signal\": metrics[0],\n",
    "                \"cluster_label\": cluster_label,\n",
    "                \"calmar_ratio\": metrics[1],\n",
    "                \"annualized_return\": metrics[2],\n",
    "                \"max_drawdown\": metrics[3],\n",
    "                \"actual_return\": metrics[4],\n",
    "                \"num_trades\": metrics[5],\n",
    "            }\n",
    "        )\n",
    "\n",
    "    return pd.DataFrame(best_clusters_list), clustering_model\n",
    "\n",
    "\n",
    "@jit(nopython=True)\n",
    "def calculate_max_drawdown(portfolio_values):\n",
    "    peak = portfolio_values[0]\n",
    "    max_drawdown = 0.0\n",
    "\n",
    "    for value in portfolio_values[1:]:\n",
    "        if value > peak:\n",
    "            peak = value\n",
    "        drawdown = (peak - value) / peak\n",
    "        if drawdown > max_drawdown:\n",
    "            max_drawdown = drawdown\n",
    "\n",
    "    return max_drawdown\n",
    "\n",
    "\n",
    "@jit(nopython=True)\n",
    "def calculate_trading_metrics(trade_outcomes, initial_capital):\n",
    "    if len(trade_outcomes) == 0:\n",
    "        return 0, 0.0, 0.0, 0.0, 0.0, 0\n",
    "\n",
    "    cumulative_return = np.cumsum(trade_outcomes)\n",
    "    signal = 1 if cumulative_return[-1] > 0 else 0\n",
    "    if signal == 0:\n",
    "        cumulative_return = -cumulative_return\n",
    "\n",
    "    portfolio_values = np.zeros(len(cumulative_return) + 1)\n",
    "    portfolio_values[0] = initial_capital\n",
    "    portfolio_values[1:] = cumulative_return + initial_capital\n",
    "\n",
    "    start_value, end_value = portfolio_values[0], portfolio_values[-1]\n",
    "    annualized_return = (end_value / start_value) - 1\n",
    "    max_drawdown = calculate_max_drawdown(portfolio_values)\n",
    "    calmar_ratio = annualized_return / (max_drawdown + 1e-6)\n",
    "    actual_return = end_value - start_value\n",
    "    return (\n",
    "        signal,\n",
    "        calmar_ratio,\n",
    "        annualized_return,\n",
    "        max_drawdown,\n",
    "        actual_return,\n",
    "        len(trade_outcomes),\n",
    "    )\n",
    "\n",
    "\n",
    "def prepare_test_data(price_subset, full_price_data, last_test_index, params):\n",
    "    test_data_list = []\n",
    "    scaler = StandardScaler()\n",
    "\n",
    "    for index in range(params[\"price_history_length\"], len(price_subset)):\n",
    "        price_history = (\n",
    "            price_subset[\"close\"]\n",
    "            .iloc[index - params[\"price_history_length\"] : index]\n",
    "            .values\n",
    "        )\n",
    "        if len(price_history) < params[\"price_history_length\"]:\n",
    "            continue\n",
    "\n",
    "        _, important_points = find_perceptually_important_points(\n",
    "            price_history,\n",
    "            params[\"num_perceptually_important_points\"],\n",
    "            params[\"distance_measure\"],\n",
    "        )\n",
    "        scaled_points = scaler.fit_transform(important_points.reshape(-1, 1)).flatten()\n",
    "\n",
    "        data_point = {\n",
    "            f\"price_point_{i}\": scaled_points[i]\n",
    "            for i in range(params[\"num_perceptually_important_points\"])\n",
    "        }\n",
    "        data_point.update(\n",
    "            price_subset.iloc[index - 1][\n",
    "                [\"year\", \"month\", \"day_of_week\", \"hour\", \"minute\"]\n",
    "            ].to_dict()\n",
    "        )\n",
    "\n",
    "        current_price = price_subset[\"close\"].iloc[index - 1]\n",
    "        current_atr = price_subset[\"atr_clipped\"].iloc[index - 1]\n",
    "        take_profit = current_price + (params[\"atr_multiplier\"] * current_atr)\n",
    "        stop_loss = current_price - (params[\"atr_multiplier\"] * current_atr)\n",
    "\n",
    "        future_highs = price_subset[\"high\"].iloc[index:].values\n",
    "        future_lows = price_subset[\"low\"].iloc[index:].values\n",
    "\n",
    "        data_point[\"trade_outcome\"] = determine_trade_outcome(\n",
    "            future_highs, future_lows, take_profit, stop_loss\n",
    "        )\n",
    "        if data_point[\"trade_outcome\"] == 0:\n",
    "            future_highs_full = full_price_data[\"high\"].iloc[last_test_index:].values\n",
    "            future_lows_full = full_price_data[\"low\"].iloc[last_test_index:].values\n",
    "            data_point[\"trade_outcome\"] = determine_trade_outcome(\n",
    "                future_highs_full, future_lows_full, take_profit, stop_loss\n",
    "            )\n",
    "\n",
    "        test_data_list.append(data_point)\n",
    "\n",
    "    return pd.DataFrame(test_data_list)\n",
    "\n",
    "\n",
    "def evaluate_cluster_performance_df(\n",
    "    price_data_df, train_best_clusters_df, clustering_model, params\n",
    "):\n",
    "    price_data = price_data_df[\n",
    "        [\n",
    "            \"price_point_0\",\n",
    "            \"price_point_1\",\n",
    "            \"price_point_2\",\n",
    "            \"price_point_3\",\n",
    "            \"price_point_4\",\n",
    "            \"day_of_week\",\n",
    "            \"hour\",\n",
    "            \"minute\",\n",
    "            \"trade_outcome\",\n",
    "        ]\n",
    "    ].values\n",
    "    train_best_clusters = train_best_clusters_df[[\"cluster_label\", \"signal\"]].values\n",
    "\n",
    "    predicted_labels = clustering_model.predict(price_data[:, :-1])\n",
    "    cluster_performance_list = []\n",
    "\n",
    "    for cluster_label, signal in train_best_clusters:\n",
    "        mask = predicted_labels == cluster_label\n",
    "        cluster_cumulative_return = np.cumsum(price_data[mask, -1])\n",
    "        if signal == 0:\n",
    "            cluster_cumulative_return = -cluster_cumulative_return\n",
    "\n",
    "        cluster_trade_outcomes = price_data[mask, -1]\n",
    "        metrics = calculate_trading_metrics(\n",
    "            cluster_trade_outcomes, params[\"initial_capital\"]\n",
    "        )\n",
    "        cluster_performance_list.append(\n",
    "            {\n",
    "                \"signal\": signal,\n",
    "                \"cluster_label\": cluster_label,\n",
    "                \"calmar_ratio\": metrics[1],\n",
    "                \"annualized_return\": metrics[2],\n",
    "                \"max_drawdown\": metrics[3],\n",
    "                \"actual_return\": metrics[4],\n",
    "                \"num_trades\": metrics[5],\n",
    "            }\n",
    "        )\n",
    "\n",
    "    return pd.DataFrame(cluster_performance_list)\n",
    "\n",
    "\n",
    "def process_window(window, train_indices, test_indices, price_data, params):\n",
    "    print(f\"Processing window {window}...\")\n",
    "    train_data = price_data.iloc[train_indices, :]\n",
    "    test_data = price_data.iloc[test_indices, :]\n",
    "    last_test_index = test_indices[-1]\n",
    "\n",
    "    # Prepare training data and perform clustering\n",
    "    train_price_data = prepare_training_data(train_data, params)\n",
    "    train_best_clusters, clustering_model = cluster_and_evaluate_price_data(\n",
    "        train_price_data, params\n",
    "    )\n",
    "    if train_best_clusters.empty:\n",
    "        return None\n",
    "\n",
    "    # Prepare test data and evaluate cluster performance\n",
    "    test_price_data = prepare_test_data(test_data, price_data, last_test_index, params)\n",
    "    test_cluster_performance = evaluate_cluster_performance_df(\n",
    "        test_price_data, train_best_clusters, clustering_model, params\n",
    "    )\n",
    "    if test_cluster_performance.empty:\n",
    "        return None\n",
    "\n",
    "    # Compile results for this window\n",
    "    return {\n",
    "        \"window\": window,\n",
    "        \"train_total_annualized_return\": train_best_clusters[\"annualized_return\"].sum(),\n",
    "        \"train_total_actual_return\": train_best_clusters[\"actual_return\"].sum(),\n",
    "        \"train_total_trades\": train_best_clusters[\"num_trades\"].sum(),\n",
    "        \"test_total_annualized_return\": test_cluster_performance[\n",
    "            \"annualized_return\"\n",
    "        ].sum(),\n",
    "        \"test_total_actual_return\": test_cluster_performance[\"actual_return\"].sum(),\n",
    "        \"test_total_trades\": test_cluster_performance[\"num_trades\"].sum(),\n",
    "    }\n",
    "\n",
    "\n",
    "def main():\n",
    "    # Load trading parameters from CSV\n",
    "    trading_params = pd.read_csv(\"params.csv\")\n",
    "    param_row = 0 if len(sys.argv) != 2 else int(sys.argv[1])\n",
    "    param_dict = dict(trading_params.iloc[param_row, :])\n",
    "\n",
    "    # Extract trading parameters\n",
    "    params = {\n",
    "        \"max_cluster_labels\": int(param_dict[\"max_cluster_labels\"]),\n",
    "        \"price_history_length\": int(param_dict[\"price_history_length\"]),\n",
    "        \"num_perceptually_important_points\": int(\n",
    "            param_dict[\"num_perceptually_important_points\"]\n",
    "        ),\n",
    "        \"distance_measure\": int(param_dict[\"distance_measure\"]),\n",
    "        \"num_clusters\": int(param_dict[\"num_clusters\"]),\n",
    "        \"atr_multiplier\": int(param_dict[\"atr_multiplier\"]),\n",
    "        \"clustering_algorithm\": param_dict[\"clustering_algorithm\"],\n",
    "        \"random_seed\": int(param_dict[\"random_seed\"]),\n",
    "        \"train_period\": int(param_dict[\"train_period\"]),\n",
    "        \"test_period\": int(param_dict[\"test_period\"]),\n",
    "        \"initial_capital\": 100,\n",
    "        \"risk_free_rate\": 0.01,\n",
    "    }\n",
    "\n",
    "    # Load and preprocess data\n",
    "    time_scaler = joblib.load(\n",
    "        \"/projects/genomic-ml/da2343/ml_project_2/unsupervised/kmeans/ts_scaler_2018.joblib\"\n",
    "    )\n",
    "    price_data = pd.read_csv(\n",
    "        \"/projects/genomic-ml/da2343/ml_project_2/data/gen_oanda_data/GBP_USD_M15_raw_data.csv\",\n",
    "        parse_dates=[\"time\"],\n",
    "        index_col=\"time\",\n",
    "    )\n",
    "\n",
    "    price_data[\"year\"] = price_data.index.year\n",
    "    price_data[\"month\"] = price_data.index.month\n",
    "    price_data[\"day_of_week\"] = price_data.index.dayofweek\n",
    "    price_data[\"hour\"] = price_data.index.hour\n",
    "    price_data[\"minute\"] = price_data.index.minute\n",
    "    price_data[\"atr\"] = talib.ATR(\n",
    "        price_data[\"high\"].values,\n",
    "        price_data[\"low\"].values,\n",
    "        price_data[\"close\"].values,\n",
    "        timeperiod=1,\n",
    "    )\n",
    "    price_data[\"atr_clipped\"] = np.clip(price_data[\"atr\"], 0.00068, 0.00176)\n",
    "\n",
    "    # Filter date range and apply time scaling\n",
    "    # price_data = price_data.loc[\"2019-01-01\":\"2024-05-01\"]\n",
    "    price_data = price_data.loc[\"2019-01-01\":\"2019-05-01\"]\n",
    "    time_columns = [\"day_of_week\", \"hour\", \"minute\"]\n",
    "    price_data[time_columns] = np.round(\n",
    "        time_scaler.transform(price_data[time_columns]), 6\n",
    "    )\n",
    "    price_data[[\"atr\", \"atr_clipped\"]] = price_data[[\"atr\", \"atr_clipped\"]].round(6)\n",
    "\n",
    "    # Initialize the sliding window splitter for backtesting\n",
    "    window_splitter = SlidingWindowSplitter(\n",
    "        window_length=params[\"train_period\"],\n",
    "        fh=np.arange(1, params[\"test_period\"] + 1),\n",
    "        step_length=1,\n",
    "    )\n",
    "\n",
    "    # Prepare the arguments for multiprocessing\n",
    "    window_args = [\n",
    "        (window, train_indices, test_indices, price_data, params)\n",
    "        for window, (train_indices, test_indices) in enumerate(\n",
    "            window_splitter.split(price_data)\n",
    "        )\n",
    "    ]\n",
    "\n",
    "    # Determine the number of processes\n",
    "    num_processes = 30\n",
    "    # Create a multiprocessing pool and map the process_window function to all windows\n",
    "    try:\n",
    "        with multiprocessing.Pool(processes=num_processes) as pool:\n",
    "            backtest_results = pool.starmap(process_window, window_args)\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during multiprocessing: {e}\")\n",
    "        return\n",
    "\n",
    "    # Filter out None results and create DataFrame\n",
    "    backtest_results = [result for result in backtest_results if result is not None]\n",
    "    if not backtest_results:\n",
    "        print(\"No valid results were produced.\")\n",
    "        return\n",
    "\n",
    "    results_df = pd.DataFrame(backtest_results)\n",
    "\n",
    "    # Compile final results\n",
    "    results_df[\"train_cumulative_annualized_return\"] = results_df[\n",
    "        \"train_total_annualized_return\"\n",
    "    ].cumsum()\n",
    "    results_df[\"train_cumulative_actual_return\"] = results_df[\n",
    "        \"train_total_actual_return\"\n",
    "    ].cumsum()\n",
    "    results_df[\"train_sharpe_ratio\"] = calculate_sharpe_ratio(\n",
    "        results_df[\"train_total_annualized_return\"].values, params[\"risk_free_rate\"]\n",
    "    )\n",
    "\n",
    "    results_df[\"test_cumulative_annualized_return\"] = results_df[\n",
    "        \"test_total_annualized_return\"\n",
    "    ].cumsum()\n",
    "    results_df[\"test_cumulative_actual_return\"] = results_df[\n",
    "        \"test_total_actual_return\"\n",
    "    ].cumsum()\n",
    "    results_df[\"test_sharpe_ratio\"] = calculate_sharpe_ratio(\n",
    "        results_df[\"test_total_annualized_return\"].values, params[\"risk_free_rate\"]\n",
    "    )\n",
    "    results_df[\"test_inverse_sharpe_ratio\"] = calculate_sharpe_ratio(\n",
    "        -1 * results_df[\"test_total_annualized_return\"].values, params[\"risk_free_rate\"]\n",
    "    )\n",
    "\n",
    "    # Add constant parameters to the results\n",
    "    for key, value in params.items():\n",
    "        results_df[key] = value\n",
    "\n",
    "    # save results to csv\n",
    "    out_file = f\"results/{param_row}.csv\"\n",
    "    results_df.to_csv(out_file, encoding=\"utf-8\", index=False)\n",
    "    print(\"Backtesting completed.\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "sys.path.append(\n",
    "    os.path.abspath(\n",
    "        \"/projects/genomic-ml/da2343/ml_project_2/unsupervised/kmeans/utils.py\"\n",
    "    )\n",
    ")\n",
    "\n",
    "print(sys.path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# path = \"/projects/genomic-ml/da2343/ml_project_2/unsupervised/kmeans/\"\n",
    "# print(os.path.exists(path))\n",
    "\n",
    "mdir = os.path.dirname(\n",
    "        os.path.abspath(\"/projects/genomic-ml/da2343/ml_project_2/unsupervised/kmeans/utils.py\")\n",
    "    )\n",
    "\n",
    "print(mdir)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs685",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
