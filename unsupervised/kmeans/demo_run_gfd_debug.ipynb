{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import *\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from numba import jit\n",
    "import joblib\n",
    "import os\n",
    "import shutil\n",
    "import json\n",
    "\n",
    "sys.path.append(\n",
    "    os.path.abspath(\n",
    "        \"/projects/genomic-ml/da2343/ml_project_2/unsupervised/kmeans\"\n",
    "    )\n",
    ")\n",
    "from utils import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "# Define large value for cases with no losses (instead of infinity)\n",
    "LARGE_VALUE = 1000.0 \n",
    "\n",
    "# Load trading parameters from CSV\n",
    "trading_params = pd.read_csv(\"params.csv\")\n",
    "param_row = 10\n",
    "param_dict = dict(\n",
    "    trading_params.iloc[param_row, :]\n",
    ")  # Assume first row of trading_params.csv\n",
    "\n",
    "# Extract trading parameters\n",
    "INSTRUMENT = param_dict[\"instrument\"]\n",
    "MAX_CLUSTER_LABELS = int(param_dict[\"max_cluster_labels\"])\n",
    "PRICE_HISTORY_LENGTH = int(param_dict[\"price_history_length\"])\n",
    "# PRICE_HISTORY_LENGTH = 10\n",
    "NUM_PERCEPTUALLY_IMPORTANT_POINTS = 4\n",
    "NUM_CLUSTERS = int(param_dict[\"num_clusters\"])\n",
    "ATR_MULTIPLIER = int(param_dict[\"atr_multiplier\"])\n",
    "# CLUSTERING_ALGORITHM = param_dict[\"clustering_algorithm\"]\n",
    "# RANDOM_SEED = int(param_dict[\"random_seed\"])\n",
    "# TEST_PERIOD = int(param_dict[\"test_period\"])\n",
    "CLUSTERING_ALGORITHM = \"kmeans\"\n",
    "RANDOM_SEED = 10234\n",
    "TEST_PERIOD = 2\n",
    "\n",
    "\n",
    "NUM_CLUSTERS = 5\n",
    "TRAIN_PERIOD = 4\n",
    "REVERSE_TEST = False\n",
    "# REVERSE_TEST = True\n",
    "\n",
    "\n",
    "# Define clustering algorithms\n",
    "clustering_estimator_dict = {\n",
    "    \"kmeans\": KMeans(n_clusters=NUM_CLUSTERS, \n",
    "                     random_state=RANDOM_SEED,\n",
    "                        n_init=1000),\n",
    "    \"gaussian_mixture\": GaussianMixture(\n",
    "        n_components=NUM_CLUSTERS, \n",
    "        covariance_type=\"tied\", \n",
    "        random_state=RANDOM_SEED\n",
    "    ),\n",
    "    \"birch\": Birch(n_clusters=NUM_CLUSTERS)\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_test_data(price_subset, instrument_dict, max_trades_per_day=5):\n",
    "    data_list = []\n",
    "    scaler = StandardScaler()\n",
    "    \n",
    "    # Get instrument's high liquidity hours\n",
    "    liquidity_start = instrument_dict['market_hours']['high_liquidity_start']\n",
    "    liquidity_end = instrument_dict['market_hours']['high_liquidity_end']\n",
    "    \n",
    "    # Keep track of number of trades per day\n",
    "    daily_trade_count = {}\n",
    "\n",
    "    for index in range(PRICE_HISTORY_LENGTH, len(price_subset)):\n",
    "        # Get price history for PIP calculation\n",
    "        price_history = (\n",
    "            price_subset[\"close\"]\n",
    "            .iloc[max(0, index - PRICE_HISTORY_LENGTH) : index]\n",
    "            .values\n",
    "        )\n",
    "        if len(price_history) < PRICE_HISTORY_LENGTH:\n",
    "            break\n",
    "\n",
    "        # Current row index\n",
    "        j = index - 1\n",
    "        current_time = price_subset.index[j]\n",
    "        current_hour = current_time.hour\n",
    "        current_date = current_time.date()\n",
    "        \n",
    "        # Skip if we already took maximum trades for this day\n",
    "        if daily_trade_count.get(current_date, 0) >= max_trades_per_day:\n",
    "            continue\n",
    "        \n",
    "        # Check if current time is within high liquidity period\n",
    "        # Handle cases where high liquidity period crosses midnight\n",
    "        is_liquid_time = False\n",
    "        if liquidity_start <= liquidity_end:\n",
    "            is_liquid_time = liquidity_start <= current_hour < liquidity_end\n",
    "        else:  # Period crosses midnight\n",
    "            is_liquid_time = current_hour >= liquidity_start or current_hour < liquidity_end\n",
    "            \n",
    "        if not is_liquid_time:\n",
    "            continue\n",
    "            \n",
    "        # Find current day's end time (15 mins before actual EOD)\n",
    "        eod_time = pd.Timestamp.combine(\n",
    "            current_time.date(), \n",
    "            pd.Timestamp('23:45').time()  # 15 mins before midnight\n",
    "        ).tz_localize('UTC')\n",
    "\n",
    "        # Get the EOD price (23:45 current day)\n",
    "        eod_data = price_subset[price_subset.index <= eod_time]\n",
    "        if len(eod_data) == 0:\n",
    "            continue\n",
    "        eod_row = eod_data.iloc[-1]\n",
    "        \n",
    "        # Calculate PIPs and scale them\n",
    "        _, important_points = find_perceptually_important_points(\n",
    "            price_history, NUM_PERCEPTUALLY_IMPORTANT_POINTS\n",
    "        )\n",
    "        scaled_points = scaler.fit_transform(important_points.reshape(-1, 1)).flatten()\n",
    "        \n",
    "        # Create data point with scaled PIPs\n",
    "        data_point = {\n",
    "            f\"price_point_{i}\": scaled_points[i]\n",
    "            for i in range(NUM_PERCEPTUALLY_IMPORTANT_POINTS)\n",
    "        }\n",
    "\n",
    "        # Add time features\n",
    "        data_point.update(\n",
    "            price_subset.iloc[j][\n",
    "                [\"year\", \"month\", \"day_of_week\", \"hour\", \"minute\"]\n",
    "            ].to_dict()\n",
    "        )\n",
    "        \n",
    "        # Calculate log return from current point to EOD\n",
    "        data_point[\"trade_outcome\"] = (eod_row[\"log_close\"] - price_subset[\"log_close\"].iloc[j])\n",
    "        \n",
    "        # Increment trade count for this day\n",
    "        daily_trade_count[current_date] = daily_trade_count.get(current_date, 0) + 1\n",
    "        \n",
    "        data_list.append(data_point)\n",
    "\n",
    "    return pd.DataFrame(data_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# @jit(nopython=True)\n",
    "def find_perceptually_important_points(price_data, num_points):\n",
    "    point_indices = np.zeros(num_points, dtype=np.int64)\n",
    "    point_prices = np.zeros(num_points, dtype=np.float64)\n",
    "    point_indices[0], point_indices[1] = 0, len(price_data) - 1\n",
    "    point_prices[0], point_prices[1] = price_data[0], price_data[-1]\n",
    "\n",
    "    for current_point in range(2, num_points):\n",
    "        max_distance, max_distance_index, insert_index = 0.0, -1, -1\n",
    "        for i in range(1, len(price_data) - 1):\n",
    "            left_adj = (\n",
    "                np.searchsorted(point_indices[:current_point], i, side=\"right\") - 1\n",
    "            )\n",
    "            right_adj = left_adj + 1\n",
    "            distance = calculate_point_distance(\n",
    "                price_data,\n",
    "                point_indices[:current_point],\n",
    "                point_prices[:current_point],\n",
    "                i,\n",
    "                left_adj,\n",
    "                right_adj,\n",
    "            )\n",
    "            if distance > max_distance:\n",
    "                max_distance, max_distance_index, insert_index = distance, i, right_adj\n",
    "\n",
    "        point_indices[insert_index + 1 : current_point + 1] = point_indices[\n",
    "            insert_index:current_point\n",
    "        ]\n",
    "        point_prices[insert_index + 1 : current_point + 1] = point_prices[\n",
    "            insert_index:current_point\n",
    "        ]\n",
    "        point_indices[insert_index], point_prices[insert_index] = (\n",
    "            max_distance_index,\n",
    "            price_data[max_distance_index],\n",
    "        )\n",
    "\n",
    "    return point_indices, point_prices\n",
    "\n",
    "# @jit(nopython=True)\n",
    "def calculate_point_distance(\n",
    "    data, point_indices, point_prices, index, left_adj, right_adj\n",
    "):\n",
    "    time_diff = point_indices[right_adj] - point_indices[left_adj]\n",
    "    price_diff = point_prices[right_adj] - point_prices[left_adj]\n",
    "    slope = price_diff / time_diff\n",
    "    x, y = index, data[index]\n",
    "    return (\n",
    "        (point_indices[left_adj] - x) ** 2 + (point_prices[left_adj] - y) ** 2\n",
    "    ) ** 0.5 + (\n",
    "        (point_indices[right_adj] - x) ** 2 + (point_prices[right_adj] - y) ** 2\n",
    "    ) ** 0.5\n",
    "\n",
    "\n",
    "def prepare_data(price_subset, instrument_dict):\n",
    "    data_list = []\n",
    "    scaler = StandardScaler()\n",
    "    \n",
    "    # Get instrument's high liquidity hours\n",
    "    liquidity_start = instrument_dict['market_hours']['high_liquidity_start']\n",
    "    liquidity_end = instrument_dict['market_hours']['high_liquidity_end']\n",
    "\n",
    "    for index in range(PRICE_HISTORY_LENGTH, len(price_subset)):\n",
    "        # Get price history for PIP calculation\n",
    "        price_history = (\n",
    "            price_subset[\"close\"]\n",
    "            .iloc[max(0, index - PRICE_HISTORY_LENGTH) : index]\n",
    "            .values\n",
    "        )\n",
    "        if len(price_history) < PRICE_HISTORY_LENGTH:\n",
    "            break\n",
    "\n",
    "        # Current row index\n",
    "        j = index - 1\n",
    "        current_time = price_subset.index[j]\n",
    "        current_hour = current_time.hour\n",
    "        \n",
    "        # Check if current time is within high liquidity period\n",
    "        # Handle cases where high liquidity period crosses midnight\n",
    "        is_liquid_time = False\n",
    "        if liquidity_start <= liquidity_end:\n",
    "            is_liquid_time = liquidity_start <= current_hour < liquidity_end\n",
    "        else:  # Period crosses midnight\n",
    "            is_liquid_time = current_hour >= liquidity_start or current_hour < liquidity_end\n",
    "            \n",
    "        if not is_liquid_time:\n",
    "            continue\n",
    "            \n",
    "        # Find current day's end time (15 mins before actual EOD)\n",
    "        eod_time = pd.Timestamp.combine(\n",
    "            current_time.date(), \n",
    "            pd.Timestamp('23:45').time()  # 15 mins before midnight\n",
    "        ).tz_localize('UTC')\n",
    "\n",
    "        # Get the EOD price (23:45 current day)\n",
    "        eod_data = price_subset[price_subset.index <= eod_time]\n",
    "        if len(eod_data) == 0:\n",
    "            continue\n",
    "        eod_row = eod_data.iloc[-1]\n",
    "        \n",
    "        # Calculate PIPs and scale them\n",
    "        _, important_points = find_perceptually_important_points(\n",
    "            price_history, NUM_PERCEPTUALLY_IMPORTANT_POINTS\n",
    "        )\n",
    "        scaled_points = scaler.fit_transform(important_points.reshape(-1, 1)).flatten()\n",
    "        \n",
    "        # Create data point with scaled PIPs\n",
    "        data_point = {\n",
    "            f\"price_point_{i}\": scaled_points[i]\n",
    "            for i in range(NUM_PERCEPTUALLY_IMPORTANT_POINTS)\n",
    "        }\n",
    "\n",
    "        # Add time features\n",
    "        data_point.update(\n",
    "            price_subset.iloc[j][\n",
    "                [\"year\", \"month\", \"day_of_week\", \"hour\", \"minute\"]\n",
    "            ].to_dict()\n",
    "        )\n",
    "        \n",
    "        # Calculate log return from current point to EOD (next day 00:00)\n",
    "        data_point[\"trade_outcome\"] = (eod_row[\"log_close\"] -  price_subset[\"log_close\"].iloc[j])\n",
    "        # print(f\"Processing {current_time} to {eod_time}...\")\n",
    "        data_list.append(data_point)\n",
    "\n",
    "    return pd.DataFrame(data_list)\n",
    "\n",
    "def evaluate_cluster_performance_df(price_data_df, train_best_cluster, clustering_model):\n",
    "    # Prepare features for prediction\n",
    "    price_point_columns = [f\"price_point_{i}\" for i in range(NUM_PERCEPTUALLY_IMPORTANT_POINTS)]\n",
    "    feature_columns = price_point_columns + [\"day_of_week\", \"hour\", \"minute\"]\n",
    "    \n",
    "    # Predict clusters for test data\n",
    "    price_features = price_data_df[feature_columns].values\n",
    "    \n",
    "    # scale features to 2 decimal places\n",
    "    price_features = np.round(price_features, 2)\n",
    "    price_data_df[\"cluster_label\"] = clustering_model.predict(price_features)\n",
    "    \n",
    "    # Get the best cluster label and its direction from training\n",
    "    cluster_label = train_best_cluster['cluster_label']\n",
    "    trade_direction = train_best_cluster['trade_direction']\n",
    "    \n",
    "    # Get trades for the best cluster\n",
    "    cluster_data = price_data_df[price_data_df['cluster_label'] == cluster_label]\n",
    "    \n",
    "    # Get trade outcomes and adjust for direction\n",
    "    cluster_trades = cluster_data['trade_outcome']\n",
    "    \n",
    "    if REVERSE_TEST:\n",
    "        trade_direction = 'short' if trade_direction == 'long' else 'long'\n",
    "    \n",
    "    if trade_direction == 'short':\n",
    "        cluster_trades = -cluster_trades\n",
    "    \n",
    "    # Basic performance metrics\n",
    "    total_return = cluster_trades.sum()\n",
    "    num_trades = len(cluster_trades)\n",
    "    \n",
    "    # Create performance dictionary\n",
    "    cluster_performance = {\n",
    "        \"cluster_label\": cluster_label,\n",
    "        \"trade_direction\": trade_direction,\n",
    "        \"actual_return\": total_return,\n",
    "        \"num_trades\": num_trades\n",
    "    }\n",
    "    return cluster_performance\n",
    "\n",
    "def cluster_and_evaluate_price_data(price_data_df):\n",
    "    price_point_columns = [f\"price_point_{i}\" for i in range(NUM_PERCEPTUALLY_IMPORTANT_POINTS)]\n",
    "    feature_columns = price_point_columns + [\"day_of_week\", \"hour\", \"minute\"]\n",
    "    price_features = price_data_df[feature_columns].values\n",
    "    \n",
    "    # scale features to 2 decimal places\n",
    "    price_features = np.round(price_features, 2)\n",
    "    \n",
    "    clustering_model = clustering_estimator_dict[CLUSTERING_ALGORITHM]\n",
    "    clustering_model.fit(price_features)\n",
    "    price_data_df[\"cluster_label\"] = clustering_model.predict(price_features)\n",
    "\n",
    "    cluster_metrics = []\n",
    "    for cluster in price_data_df['cluster_label'].unique():\n",
    "        cluster_data = price_data_df[price_data_df['cluster_label'] == cluster]\n",
    "        \n",
    "        if len(cluster_data) < 5:  # Skip clusters with too few trades\n",
    "            continue\n",
    "            \n",
    "        # Determine if cluster is long or short based on mean return\n",
    "        cluster_mean = cluster_data['trade_outcome'].mean()\n",
    "        is_long_cluster = cluster_mean >= 0\n",
    "        \n",
    "        # Adjust trade outcomes for short trades\n",
    "        cluster_trades = cluster_data['trade_outcome']\n",
    "        if not is_long_cluster:\n",
    "            cluster_trades = -cluster_trades  # Invert returns for short trades\n",
    "            \n",
    "        # Basic metrics\n",
    "        wins = cluster_trades > 0\n",
    "        losses = cluster_trades < 0\n",
    "        \n",
    "        # 1. Win Rate\n",
    "        win_rate = np.mean(wins) if len(cluster_trades) > 0 else 0\n",
    "        \n",
    "        # 2. Risk-adjusted return\n",
    "        # Consider dividing by another factor to normalize\n",
    "        returns_mean = cluster_trades.mean()\n",
    "        returns_std = cluster_trades.std()\n",
    "        annualization_factor = np.sqrt(252)\n",
    "        actual_returns_mean = (np.exp(returns_mean) - 1)\n",
    "        actual_returns_std = returns_std\n",
    "        # Add scaling factor for 15-min returns\n",
    "        scale_factor = 0.5  # This can be adjusted\n",
    "        sharpe = (actual_returns_mean / actual_returns_std) * annualization_factor * scale_factor if actual_returns_std != 0 else 0\n",
    "                \n",
    "        # 3. Maximum Drawdown\n",
    "        cumulative = cluster_trades.cumsum()\n",
    "        running_max = cumulative.expanding().max()\n",
    "        drawdowns = cumulative - running_max\n",
    "        max_drawdown = abs(drawdowns.min()) if len(drawdowns) > 0 else LARGE_VALUE\n",
    "        \n",
    "        # 4. Profit Factor\n",
    "        gross_profits = cluster_trades[wins].sum() if any(wins) else 0\n",
    "        gross_losses = abs(cluster_trades[losses].sum()) if any(losses) else 0\n",
    "        profit_factor = gross_profits / gross_losses if gross_losses != 0 else LARGE_VALUE\n",
    "        \n",
    "        # 5. Win/Loss Ratio\n",
    "        avg_win = cluster_trades[wins].mean() if any(wins) else 0\n",
    "        avg_loss = abs(cluster_trades[losses].mean()) if any(losses) else LARGE_VALUE\n",
    "        win_loss_ratio = avg_win / avg_loss if avg_loss != 0 else LARGE_VALUE\n",
    "        \n",
    "        # 6. Consistency Score (lower is better)\n",
    "        returns_volatility = cluster_trades.std()\n",
    "        \n",
    "        cluster_metrics.append({\n",
    "            'cluster_label': cluster,\n",
    "            'trade_direction': 'long' if is_long_cluster else 'short',\n",
    "            'num_trades': len(cluster_trades),\n",
    "            'win_rate': win_rate,\n",
    "            'sharpe': sharpe,\n",
    "            'max_drawdown': max_drawdown,\n",
    "            'profit_factor': profit_factor,\n",
    "            'win_loss_ratio': win_loss_ratio,\n",
    "            'returns_volatility': returns_volatility,\n",
    "            'avg_return': returns_mean,\n",
    "            'actual_return': cluster_trades.sum(),\n",
    "            'mean_return': cluster_mean\n",
    "        })\n",
    "    metrics_df = pd.DataFrame(cluster_metrics)\n",
    "    \n",
    "    print(f\"Found {sharpe} sharpe.\")\n",
    "        \n",
    "#     # Filter for good clusters with realistic performance metrics\n",
    "#     mask = (\n",
    "#         (metrics_df['win_rate'].between(0.5, 0.65)) &     # 55-65% win rate\n",
    "#         (metrics_df['profit_factor'].between(1.5, 2.0)) &  # 1.6-2.0 profit factor\n",
    "#         (metrics_df['win_loss_ratio'].between(1.2, 2.0)) &  # 1.2-2.0 win/loss ratio\n",
    "#         (metrics_df['sharpe'].between(0.5, 2.0)) \n",
    "#     )\n",
    "#     # Create an explicit copy of the filtered DataFrame\n",
    "#     good_clusters = metrics_df[mask].copy()   \n",
    "#     # Return empty DataFrame if no valid clusters\n",
    "#     if len(good_clusters) == 0:\n",
    "#        return {}, clustering_model \n",
    "#    # Calculate composite score for filtered clusters\n",
    "#     good_clusters['consistency_score'] = (\n",
    "#         good_clusters['win_rate'] / 0.65 * 0.3 +            # Normalized to max 0.65, 30% weight\n",
    "#         good_clusters['profit_factor'] / 2.0 * 0.2 +        # Normalized to max 2.0, 30% weight\n",
    "#         good_clusters['win_loss_ratio'] / 2.0 * 0.2 +       # Normalized to max 2.0, 20% weight\n",
    "#         good_clusters['sharpe'] / 3.0 * 0.2                 # Normalized to max 2.0, 20% weight\n",
    "#     )\n",
    "#     print(f\"Found {len(good_clusters)} good clusters.\")\n",
    "\n",
    "    mask = (\n",
    "        # (metrics_df['sharpe'] > 0)          # Win rate above 55%\n",
    "        (metrics_df['profit_factor'] > 1.5) \n",
    "        # (metrics_df['sharpe'] <= 5)             # Sharpe above 0.5\n",
    "        # (metrics_df['win_loss_ratio'] > 1.2)    # Win/loss ratio above 1.2\n",
    "    )\n",
    "    # Create an explicit copy of the filtered DataFrame\n",
    "    good_clusters = metrics_df[mask].copy()\n",
    "    # Return empty DataFrame if no valid clusters\n",
    "    if len(good_clusters) == 0:\n",
    "        return {}, clustering_model \n",
    "    # Calculate composite score for filtered clusters\n",
    "    good_clusters['consistency_score'] = (\n",
    "         good_clusters['sharpe']\n",
    "        # good_clusters['win_rate'] / 0.8 * 0.5 +          # Normalized to max 0.8, 30% weight\n",
    "        # good_clusters['profit_factor'] / 2.0 * 0.5      # Normalized to max 2.0, 20% weight\n",
    "        # good_clusters['win_loss_ratio'] / 2.0 * 0.2     # Normalized to max 2.0, 20% weight\n",
    "        # good_clusters['sharpe'] / 3.0 * 0.3              # Normalized to max 3.0, 30% weight\n",
    "    )\n",
    "    print(f\"Found {len(good_clusters)} good clusters.\")\n",
    "            \n",
    "    # Get single best cluster by consistency score\n",
    "    best_cluster_df = (\n",
    "        good_clusters\n",
    "        .sort_values('consistency_score', ascending=False)\n",
    "        .head(1)\n",
    "        .reset_index(drop=True)\n",
    "    )\n",
    "    best_cluster_dict = best_cluster_df.iloc[0].to_dict()\n",
    "    return best_cluster_dict, clustering_model\n",
    "\n",
    "\n",
    "INSTRUMENT = \"GBP_JPY_M15\"\n",
    "PROJECT_DIR = \"/projects/genomic-ml/da2343/ml_project_2\"\n",
    "# PROJECT_DIR = \"/Users/newuser/Projects/robust_algo_trader\"\n",
    "# Load the config file\n",
    "config_path = f\"{PROJECT_DIR}/settings/config_gfd.json\"\n",
    "with open(config_path) as f:\n",
    "    config = json.load(f)\n",
    "\n",
    "instrument_dict = config[\"traded_instruments\"][INSTRUMENT.split(\"_M15\")[0]]\n",
    "time_scaler = joblib.load(f\"{PROJECT_DIR}/unsupervised/kmeans/ts_scaler_2018.joblib\")\n",
    "price_data = pd.read_csv(\n",
    "    f\"{PROJECT_DIR}/data/gen_oanda_data/{INSTRUMENT}_raw_data.csv\",\n",
    "    parse_dates=[\"time\"],\n",
    "    index_col=\"time\",\n",
    ")\n",
    "\n",
    "# Filter date range and apply time scaling\n",
    "price_data = price_data.loc[\"2019-01-01\":\"2024-01-01\"]\n",
    "price_data['log_close'] = np.log(price_data['close'])\n",
    "price_data['log_open'] = np.log(price_data['open'])\n",
    "price_data[\"year\"] = price_data.index.year\n",
    "price_data[\"month\"] = price_data.index.month\n",
    "price_data[\"day_of_week\"] = price_data.index.dayofweek\n",
    "price_data[\"hour\"] = price_data.index.hour\n",
    "price_data[\"minute\"] = price_data.index.minute\n",
    "time_columns = [\"day_of_week\", \"hour\", \"minute\"]\n",
    "price_data[time_columns] = time_scaler.transform(price_data[time_columns])\n",
    "# Round price columns\n",
    "columns_to_round = ['open', 'high', 'low', 'close', 'log_close', \"log_open\", \"day_of_week\", \"hour\", \"minute\"]\n",
    "price_data[columns_to_round] = price_data[columns_to_round].round(6)\n",
    "\n",
    "# Initialize the sliding window splitter for backtesting\n",
    "window_splitter = OrderedSlidingWindowSplitter(\n",
    "    train_weeks=TRAIN_PERIOD, test_weeks=TEST_PERIOD, step_size=1\n",
    ")\n",
    "\n",
    "backtest_results = []\n",
    "for window, (train_indices, test_indices) in enumerate(window_splitter.split(price_data), 1):\n",
    "    if window < 100:\n",
    "        continue\n",
    "    \n",
    "    print(f\"Processing window {window}...\")\n",
    "    train_data = price_data.iloc[train_indices, :]\n",
    "    test_data = price_data.iloc[test_indices, :]\n",
    "\n",
    "    # Prepare training data and perform clustering\n",
    "    print(\"Preparing training data and clustering...\")\n",
    "    train_price_data = prepare_data(train_data, instrument_dict)\n",
    "    train_cluster_perf, clustering_model = cluster_and_evaluate_price_data(train_price_data)\n",
    "    if not train_cluster_perf:\n",
    "        continue\n",
    "\n",
    "    # Prepare test data and evaluate cluster performance\n",
    "    test_price_data = prepare_data(test_data, instrument_dict)\n",
    "    print(\"Preparing test data and evaluating cluster performance...\")\n",
    "    test_cluster_perf = evaluate_cluster_performance_df(test_price_data, train_cluster_perf, clustering_model)\n",
    "    \n",
    "    # check if test_cluster_perf dict is empty\n",
    "    if not test_cluster_perf:\n",
    "        continue\n",
    "\n",
    "    # Compile results for this window\n",
    "    print(\"Compiling results...\")\n",
    "    window_result = {\n",
    "        \"window\": window,\n",
    "        # Training metrics (single best cluster)\n",
    "        \"train_actual_return\": train_cluster_perf[\"actual_return\"], \n",
    "        \"train_num_trades\": train_cluster_perf[\"num_trades\"], \n",
    "        \"train_direction\": train_cluster_perf[\"trade_direction\"],\n",
    "        \n",
    "        # Test metrics (single cluster performance)\n",
    "        \"test_actual_return\": test_cluster_perf[\"actual_return\"],\n",
    "        \"test_num_trades\": test_cluster_perf[\"num_trades\"], \n",
    "        \"test_direction\": test_cluster_perf[\"trade_direction\"]\n",
    "    }\n",
    "    backtest_results.append(window_result)\n",
    "    # break\n",
    "    if window >= 200:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>window</th>\n",
       "      <th>train_actual_return</th>\n",
       "      <th>train_num_trades</th>\n",
       "      <th>train_direction</th>\n",
       "      <th>test_actual_return</th>\n",
       "      <th>test_num_trades</th>\n",
       "      <th>test_direction</th>\n",
       "      <th>train_average_return</th>\n",
       "      <th>train_sharpe_ratio</th>\n",
       "      <th>train_profit_factor</th>\n",
       "      <th>...</th>\n",
       "      <th>total_windows</th>\n",
       "      <th>reverse_test</th>\n",
       "      <th>max_cluster_labels</th>\n",
       "      <th>num_clusters</th>\n",
       "      <th>clustering_algorithm</th>\n",
       "      <th>train_period</th>\n",
       "      <th>test_period</th>\n",
       "      <th>random_seed</th>\n",
       "      <th>instrument</th>\n",
       "      <th>num_pips</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>100</td>\n",
       "      <td>0.350844</td>\n",
       "      <td>154</td>\n",
       "      <td>long</td>\n",
       "      <td>-0.027664</td>\n",
       "      <td>69</td>\n",
       "      <td>long</td>\n",
       "      <td>0.183459</td>\n",
       "      <td>1.402166</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>...</td>\n",
       "      <td>98</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>kmeans</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>10234</td>\n",
       "      <td>GBP_JPY_M15</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>101</td>\n",
       "      <td>0.368330</td>\n",
       "      <td>109</td>\n",
       "      <td>long</td>\n",
       "      <td>-0.019693</td>\n",
       "      <td>83</td>\n",
       "      <td>long</td>\n",
       "      <td>0.183459</td>\n",
       "      <td>1.402166</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>...</td>\n",
       "      <td>98</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>kmeans</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>10234</td>\n",
       "      <td>GBP_JPY_M15</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>102</td>\n",
       "      <td>0.219817</td>\n",
       "      <td>154</td>\n",
       "      <td>long</td>\n",
       "      <td>0.087733</td>\n",
       "      <td>108</td>\n",
       "      <td>long</td>\n",
       "      <td>0.183459</td>\n",
       "      <td>1.402166</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>...</td>\n",
       "      <td>98</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>kmeans</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>10234</td>\n",
       "      <td>GBP_JPY_M15</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>103</td>\n",
       "      <td>0.202898</td>\n",
       "      <td>187</td>\n",
       "      <td>long</td>\n",
       "      <td>0.025969</td>\n",
       "      <td>88</td>\n",
       "      <td>long</td>\n",
       "      <td>0.183459</td>\n",
       "      <td>1.402166</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>...</td>\n",
       "      <td>98</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>kmeans</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>10234</td>\n",
       "      <td>GBP_JPY_M15</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>104</td>\n",
       "      <td>0.040212</td>\n",
       "      <td>86</td>\n",
       "      <td>long</td>\n",
       "      <td>0.179541</td>\n",
       "      <td>35</td>\n",
       "      <td>long</td>\n",
       "      <td>0.183459</td>\n",
       "      <td>1.402166</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>...</td>\n",
       "      <td>98</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>kmeans</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>10234</td>\n",
       "      <td>GBP_JPY_M15</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>195</td>\n",
       "      <td>0.626205</td>\n",
       "      <td>133</td>\n",
       "      <td>long</td>\n",
       "      <td>0.008385</td>\n",
       "      <td>40</td>\n",
       "      <td>long</td>\n",
       "      <td>0.183459</td>\n",
       "      <td>1.402166</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>...</td>\n",
       "      <td>98</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>kmeans</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>10234</td>\n",
       "      <td>GBP_JPY_M15</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>196</td>\n",
       "      <td>0.314481</td>\n",
       "      <td>111</td>\n",
       "      <td>long</td>\n",
       "      <td>0.038804</td>\n",
       "      <td>16</td>\n",
       "      <td>long</td>\n",
       "      <td>0.183459</td>\n",
       "      <td>1.402166</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>...</td>\n",
       "      <td>98</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>kmeans</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>10234</td>\n",
       "      <td>GBP_JPY_M15</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>197</td>\n",
       "      <td>0.231130</td>\n",
       "      <td>152</td>\n",
       "      <td>long</td>\n",
       "      <td>-0.074994</td>\n",
       "      <td>62</td>\n",
       "      <td>long</td>\n",
       "      <td>0.183459</td>\n",
       "      <td>1.402166</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>...</td>\n",
       "      <td>98</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>kmeans</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>10234</td>\n",
       "      <td>GBP_JPY_M15</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>198</td>\n",
       "      <td>0.182991</td>\n",
       "      <td>220</td>\n",
       "      <td>short</td>\n",
       "      <td>-0.169776</td>\n",
       "      <td>89</td>\n",
       "      <td>short</td>\n",
       "      <td>0.183459</td>\n",
       "      <td>1.402166</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>...</td>\n",
       "      <td>98</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>kmeans</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>10234</td>\n",
       "      <td>GBP_JPY_M15</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>200</td>\n",
       "      <td>0.078040</td>\n",
       "      <td>117</td>\n",
       "      <td>short</td>\n",
       "      <td>0.216386</td>\n",
       "      <td>92</td>\n",
       "      <td>short</td>\n",
       "      <td>0.183459</td>\n",
       "      <td>1.402166</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>...</td>\n",
       "      <td>98</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>kmeans</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>10234</td>\n",
       "      <td>GBP_JPY_M15</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>98 rows × 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    window  train_actual_return  train_num_trades train_direction  \\\n",
       "0      100             0.350844               154            long   \n",
       "1      101             0.368330               109            long   \n",
       "2      102             0.219817               154            long   \n",
       "3      103             0.202898               187            long   \n",
       "4      104             0.040212                86            long   \n",
       "..     ...                  ...               ...             ...   \n",
       "93     195             0.626205               133            long   \n",
       "94     196             0.314481               111            long   \n",
       "95     197             0.231130               152            long   \n",
       "96     198             0.182991               220           short   \n",
       "97     200             0.078040               117           short   \n",
       "\n",
       "    test_actual_return  test_num_trades test_direction  train_average_return  \\\n",
       "0            -0.027664               69           long              0.183459   \n",
       "1            -0.019693               83           long              0.183459   \n",
       "2             0.087733              108           long              0.183459   \n",
       "3             0.025969               88           long              0.183459   \n",
       "4             0.179541               35           long              0.183459   \n",
       "..                 ...              ...            ...                   ...   \n",
       "93            0.008385               40           long              0.183459   \n",
       "94            0.038804               16           long              0.183459   \n",
       "95           -0.074994               62           long              0.183459   \n",
       "96           -0.169776               89          short              0.183459   \n",
       "97            0.216386               92          short              0.183459   \n",
       "\n",
       "    train_sharpe_ratio  train_profit_factor  ...  total_windows  reverse_test  \\\n",
       "0             1.402166               1000.0  ...             98         False   \n",
       "1             1.402166               1000.0  ...             98         False   \n",
       "2             1.402166               1000.0  ...             98         False   \n",
       "3             1.402166               1000.0  ...             98         False   \n",
       "4             1.402166               1000.0  ...             98         False   \n",
       "..                 ...                  ...  ...            ...           ...   \n",
       "93            1.402166               1000.0  ...             98         False   \n",
       "94            1.402166               1000.0  ...             98         False   \n",
       "95            1.402166               1000.0  ...             98         False   \n",
       "96            1.402166               1000.0  ...             98         False   \n",
       "97            1.402166               1000.0  ...             98         False   \n",
       "\n",
       "    max_cluster_labels  num_clusters  clustering_algorithm  train_period  \\\n",
       "0                    1             5                kmeans             4   \n",
       "1                    1             5                kmeans             4   \n",
       "2                    1             5                kmeans             4   \n",
       "3                    1             5                kmeans             4   \n",
       "4                    1             5                kmeans             4   \n",
       "..                 ...           ...                   ...           ...   \n",
       "93                   1             5                kmeans             4   \n",
       "94                   1             5                kmeans             4   \n",
       "95                   1             5                kmeans             4   \n",
       "96                   1             5                kmeans             4   \n",
       "97                   1             5                kmeans             4   \n",
       "\n",
       "    test_period  random_seed   instrument  num_pips  \n",
       "0             2        10234  GBP_JPY_M15         4  \n",
       "1             2        10234  GBP_JPY_M15         4  \n",
       "2             2        10234  GBP_JPY_M15         4  \n",
       "3             2        10234  GBP_JPY_M15         4  \n",
       "4             2        10234  GBP_JPY_M15         4  \n",
       "..          ...          ...          ...       ...  \n",
       "93            2        10234  GBP_JPY_M15         4  \n",
       "94            2        10234  GBP_JPY_M15         4  \n",
       "95            2        10234  GBP_JPY_M15         4  \n",
       "96            2        10234  GBP_JPY_M15         4  \n",
       "97            2        10234  GBP_JPY_M15         4  \n",
       "\n",
       "[98 rows x 29 columns]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create base DataFrame from backtest results\n",
    "backtest_results_df = pd.DataFrame(backtest_results)\n",
    "\n",
    "# Get returns series\n",
    "# Calculate metrics for train returns\n",
    "train_returns = backtest_results_df['train_actual_return']\n",
    "train_sharpe = (\n",
    "    np.mean(train_returns) / np.std(train_returns)\n",
    "    if np.std(train_returns) != 0 else LARGE_VALUE\n",
    ")\n",
    "train_winning_trades = train_returns[train_returns > 0]\n",
    "train_losing_trades = train_returns[train_returns < 0]\n",
    "train_gross_profits = train_winning_trades.sum() if len(train_winning_trades) > 0 else 0\n",
    "train_gross_losses = abs(train_losing_trades.sum()) if len(train_losing_trades) > 0 else 0\n",
    "train_profit_factor = train_gross_profits / train_gross_losses if train_gross_losses != 0 else LARGE_VALUE\n",
    "\n",
    "# Calculate metrics for test returns\n",
    "test_returns = backtest_results_df['test_actual_return']\n",
    "test_sharpe = (\n",
    "    np.mean(test_returns) / np.std(test_returns)\n",
    "    if np.std(test_returns) != 0 else LARGE_VALUE\n",
    ")\n",
    "test_winning_trades = test_returns[test_returns > 0]\n",
    "test_losing_trades = test_returns[test_returns < 0]\n",
    "test_gross_profits = test_winning_trades.sum() if len(test_winning_trades) > 0 else 0\n",
    "test_gross_losses = abs(test_losing_trades.sum()) if len(test_losing_trades) > 0 else 0\n",
    "test_profit_factor = test_gross_profits / test_gross_losses if test_gross_losses != 0 else LARGE_VALUE\n",
    "\n",
    "# Train Performance metrics\n",
    "backtest_results_df['train_actual_return'] = round(backtest_results_df['train_actual_return'], 6)\n",
    "backtest_results_df[\"train_average_return\"] = round(train_returns.mean(), 6)\n",
    "backtest_results_df[\"train_sharpe_ratio\"] = round(train_sharpe, 6)\n",
    "backtest_results_df[\"train_profit_factor\"] = round(train_profit_factor, 6)\n",
    "backtest_results_df[\"train_total_trades\"] = backtest_results_df['train_num_trades'].sum()\n",
    "backtest_results_df[\"train_avg_trades_per_window\"] = round(backtest_results_df['train_num_trades'].mean(), 6)\n",
    "backtest_results_df[\"train_win_ratio\"] = round((train_returns > 0).mean() + (train_returns == 0).mean()/2, 6)\n",
    "\n",
    "# Test Performance metrics\n",
    "backtest_results_df['test_actual_return'] = round(backtest_results_df['test_actual_return'], 6)\n",
    "backtest_results_df[\"test_average_return\"] = round(test_returns.mean(), 6)\n",
    "backtest_results_df[\"test_sharpe_ratio\"] = round(test_sharpe, 6)\n",
    "backtest_results_df[\"test_profit_factor\"] = round(test_profit_factor, 6)\n",
    "backtest_results_df[\"test_total_trades\"] = backtest_results_df['test_num_trades'].sum()\n",
    "backtest_results_df[\"test_avg_trades_per_window\"] = round(backtest_results_df['test_num_trades'].mean(), 6)\n",
    "backtest_results_df[\"test_win_ratio\"] = round((test_returns > 0).mean() + (test_returns == 0).mean()/2, 6)\n",
    "\n",
    "# General statistics\n",
    "backtest_results_df[\"total_windows\"] = len(backtest_results_df)\n",
    "# Configuration parameters (no rounding needed for these)\n",
    "backtest_results_df[\"reverse_test\"] = REVERSE_TEST\n",
    "backtest_results_df[\"max_cluster_labels\"] = MAX_CLUSTER_LABELS\n",
    "backtest_results_df[\"num_clusters\"] = NUM_CLUSTERS\n",
    "backtest_results_df[\"clustering_algorithm\"] = CLUSTERING_ALGORITHM\n",
    "backtest_results_df[\"train_period\"] = TRAIN_PERIOD\n",
    "backtest_results_df[\"test_period\"] = TEST_PERIOD\n",
    "backtest_results_df[\"random_seed\"] = RANDOM_SEED\n",
    "backtest_results_df[\"instrument\"] = INSTRUMENT\n",
    "backtest_results_df[\"num_pips\"] = NUM_PERCEPTUALLY_IMPORTANT_POINTS\n",
    "\n",
    "backtest_results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save results to csv\n",
    "out_file = f\"results/{param_row}.csv\"\n",
    "results_df.to_csv(out_file, encoding=\"utf-8\", index=False)\n",
    "print(\"Backtesting completed.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs685",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
