{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import *\n",
    "from sklearn.decomposition import *\n",
    "from sklearn.preprocessing import *\n",
    "from sklearn.metrics import *\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import *\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "root_results_dir = \"/Users/newuser/Projects/robust-algo-trader/data/trades_seq_fixed_EURUSD_H1_2011_2023.csv\"\n",
    "df = pd.read_csv(f\"{root_results_dir}\")\n",
    "\n",
    "y = df[\"label\"]\n",
    "X = df[[\"position\", \"RSI\", \"ATR\", \"ADX\", \"AROON_Oscillator\"]]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=0)\n",
    "# scale the data\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# convert to tensors\n",
    "X_train = torch.from_numpy(X_train.astype(np.float32))\n",
    "X_test = torch.from_numpy(X_test.astype(np.float32))\n",
    "y_train = torch.tensor(y_train.values.astype(np.float32))\n",
    "y_test = torch.tensor(y_test.values.astype(np.float32))\n",
    "\n",
    "batch_size = 512\n",
    "\n",
    "# Create DataLoader objects for training and testing\n",
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "test_dataset = TensorDataset(X_test, y_test)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get cpu, gpu or mps device for training.\n",
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(f\"Using {device} device\")\n",
    "\n",
    "# Define model\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(5, 128),\n",
    "            # nn.ReLU(),\n",
    "            # nn.Linear(512, 256),\n",
    "            # nn.ReLU(),\n",
    "            # nn.Linear(256, 128),\n",
    "            # nn.ReLU(),\n",
    "            # nn.Linear(128, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n",
    "\n",
    "    \n",
    "input_size = len(X.columns)\n",
    "hidden_size = 64\n",
    "output_size = 1\n",
    "model = NeuralNetwork().to(device)\n",
    "# loss_fn = PrecisionLoss()\n",
    "loss_fn = torch.nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.3)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.7004918932914734\n",
      "Test Loss: 0.6776566505432129\n",
      "Epoch 402\n",
      "-------------------------------\n",
      "Train Loss: 0.6887659311294556\n",
      "Test Loss: 0.6908198118209838\n",
      "Epoch 403\n",
      "-------------------------------\n",
      "Train Loss: 0.6981793642044067\n",
      "Test Loss: 0.6708930015563965\n",
      "Epoch 404\n",
      "-------------------------------\n",
      "Train Loss: 0.6848081946372986\n",
      "Test Loss: 0.6780771255493164\n",
      "Epoch 405\n",
      "-------------------------------\n",
      "Train Loss: 0.6771685957908631\n",
      "Test Loss: 0.6756979823112488\n",
      "Epoch 406\n",
      "-------------------------------\n",
      "Train Loss: 0.6649244666099549\n",
      "Test Loss: 0.7061054229736328\n",
      "Epoch 407\n",
      "-------------------------------\n",
      "Train Loss: 0.7059559345245361\n",
      "Test Loss: 0.6917581915855407\n",
      "Epoch 408\n",
      "-------------------------------\n",
      "Train Loss: 0.6850586295127868\n",
      "Test Loss: 0.7008724570274353\n",
      "Epoch 409\n",
      "-------------------------------\n",
      "Train Loss: 0.6940372824668884\n",
      "Test Loss: 0.6804499864578247\n",
      "Epoch 410\n",
      "-------------------------------\n",
      "Train Loss: 0.6549736380577087\n",
      "Test Loss: 0.7035093903541565\n",
      "Epoch 411\n",
      "-------------------------------\n",
      "Train Loss: 0.6980652451515198\n",
      "Test Loss: 0.6672958254814148\n",
      "Epoch 412\n",
      "-------------------------------\n",
      "Train Loss: 0.6828108668327332\n",
      "Test Loss: 0.6913789272308349\n",
      "Epoch 413\n",
      "-------------------------------\n",
      "Train Loss: 0.6851303696632385\n",
      "Test Loss: 0.6776535153388977\n",
      "Epoch 414\n",
      "-------------------------------\n",
      "Train Loss: 0.7005771994590759\n",
      "Test Loss: 0.7077536702156066\n",
      "Epoch 415\n",
      "-------------------------------\n",
      "Train Loss: 0.6778819561004639\n",
      "Test Loss: 0.6814738631248474\n",
      "Epoch 416\n",
      "-------------------------------\n",
      "Train Loss: 0.6749170184135437\n",
      "Test Loss: 0.6831148028373718\n",
      "Epoch 417\n",
      "-------------------------------\n",
      "Train Loss: 0.6658997774124146\n",
      "Test Loss: 0.7052588820457458\n",
      "Epoch 418\n",
      "-------------------------------\n",
      "Train Loss: 0.6930336594581604\n",
      "Test Loss: 0.6630413889884949\n",
      "Epoch 419\n",
      "-------------------------------\n",
      "Train Loss: 0.6783677458763122\n",
      "Test Loss: 0.6836928129196167\n",
      "Epoch 420\n",
      "-------------------------------\n",
      "Train Loss: 0.6619176745414734\n",
      "Test Loss: 0.6919983863830567\n",
      "Epoch 421\n",
      "-------------------------------\n",
      "Train Loss: 0.6909676790237427\n",
      "Test Loss: 0.6988393187522888\n",
      "Epoch 422\n",
      "-------------------------------\n",
      "Train Loss: 0.6796090722084045\n",
      "Test Loss: 0.6815232276916504\n",
      "Epoch 423\n",
      "-------------------------------\n",
      "Train Loss: 0.6800849676132202\n",
      "Test Loss: 0.6744837403297425\n",
      "Epoch 424\n",
      "-------------------------------\n",
      "Train Loss: 0.674801230430603\n",
      "Test Loss: 0.6863540887832642\n",
      "Epoch 425\n",
      "-------------------------------\n",
      "Train Loss: 0.6766387462615967\n",
      "Test Loss: 0.6745381474494934\n",
      "Epoch 426\n",
      "-------------------------------\n",
      "Train Loss: 0.6694196224212646\n",
      "Test Loss: 0.6686046481132507\n",
      "Epoch 427\n",
      "-------------------------------\n",
      "Train Loss: 0.6698520302772522\n",
      "Test Loss: 0.6712163209915161\n",
      "Epoch 428\n",
      "-------------------------------\n",
      "Train Loss: 0.6843636512756348\n",
      "Test Loss: 0.6707144021987915\n",
      "Epoch 429\n",
      "-------------------------------\n",
      "Train Loss: 0.6705792188644409\n",
      "Test Loss: 0.6830666422843933\n",
      "Epoch 430\n",
      "-------------------------------\n",
      "Train Loss: 0.6784762620925904\n",
      "Test Loss: 0.6789597868919373\n",
      "Epoch 431\n",
      "-------------------------------\n",
      "Train Loss: 0.6675551772117615\n",
      "Test Loss: 0.6631410002708436\n",
      "Epoch 432\n",
      "-------------------------------\n",
      "Train Loss: 0.6552886009216309\n",
      "Test Loss: 0.6856191039085389\n",
      "Epoch 433\n",
      "-------------------------------\n",
      "Train Loss: 0.6924451112747192\n",
      "Test Loss: 0.6581278800964355\n",
      "Epoch 434\n",
      "-------------------------------\n",
      "Train Loss: 0.680439829826355\n",
      "Test Loss: 0.6817558884620667\n",
      "Epoch 435\n",
      "-------------------------------\n",
      "Train Loss: 0.6842473387718201\n",
      "Test Loss: 0.6854199290275573\n",
      "Epoch 436\n",
      "-------------------------------\n",
      "Train Loss: 0.682237160205841\n",
      "Test Loss: 0.6668681502342224\n",
      "Epoch 437\n",
      "-------------------------------\n",
      "Train Loss: 0.6752410769462586\n",
      "Test Loss: 0.6776904582977294\n",
      "Epoch 438\n",
      "-------------------------------\n",
      "Train Loss: 0.6716006994247437\n",
      "Test Loss: 0.671464455127716\n",
      "Epoch 439\n",
      "-------------------------------\n",
      "Train Loss: 0.681820273399353\n",
      "Test Loss: 0.6847988963127136\n",
      "Epoch 440\n",
      "-------------------------------\n",
      "Train Loss: 0.674837589263916\n",
      "Test Loss: 0.6945654392242432\n",
      "Epoch 441\n",
      "-------------------------------\n",
      "Train Loss: 0.6746082901954651\n",
      "Test Loss: 0.6854854702949524\n",
      "Epoch 442\n",
      "-------------------------------\n",
      "Train Loss: 0.6897082686424255\n",
      "Test Loss: 0.6746508598327636\n",
      "Epoch 443\n",
      "-------------------------------\n",
      "Train Loss: 0.682638657093048\n",
      "Test Loss: 0.6802630186080932\n",
      "Epoch 444\n",
      "-------------------------------\n",
      "Train Loss: 0.6793872594833374\n",
      "Test Loss: 0.6805136561393738\n",
      "Epoch 445\n",
      "-------------------------------\n",
      "Train Loss: 0.6746107697486877\n",
      "Test Loss: 0.6867933988571167\n",
      "Epoch 446\n",
      "-------------------------------\n",
      "Train Loss: 0.6684297084808349\n",
      "Test Loss: 0.6857138037681579\n",
      "Epoch 447\n",
      "-------------------------------\n",
      "Train Loss: 0.6743369221687316\n",
      "Test Loss: 0.6777109026908874\n",
      "Epoch 448\n",
      "-------------------------------\n",
      "Train Loss: 0.671285355091095\n",
      "Test Loss: 0.670909833908081\n",
      "Epoch 449\n",
      "-------------------------------\n",
      "Train Loss: 0.6828206539154053\n",
      "Test Loss: 0.6779190421104431\n",
      "Epoch 450\n",
      "-------------------------------\n",
      "Train Loss: 0.6748293042182922\n",
      "Test Loss: 0.6752022266387939\n",
      "Epoch 451\n",
      "-------------------------------\n",
      "Train Loss: 0.6783242344856262\n",
      "Test Loss: 0.6775845289230347\n",
      "Epoch 452\n",
      "-------------------------------\n",
      "Train Loss: 0.6879947900772094\n",
      "Test Loss: 0.6728928804397583\n",
      "Epoch 453\n",
      "-------------------------------\n",
      "Train Loss: 0.680071759223938\n",
      "Test Loss: 0.6799298882484436\n",
      "Epoch 454\n",
      "-------------------------------\n",
      "Train Loss: 0.6804634928703308\n",
      "Test Loss: 0.6757557988166809\n",
      "Epoch 455\n",
      "-------------------------------\n",
      "Train Loss: 0.6557152271270752\n",
      "Test Loss: 0.6670613288879395\n",
      "Epoch 456\n",
      "-------------------------------\n",
      "Train Loss: 0.6882667660713195\n",
      "Test Loss: 0.6711561441421509\n",
      "Epoch 457\n",
      "-------------------------------\n",
      "Train Loss: 0.6706466555595398\n",
      "Test Loss: 0.678400719165802\n",
      "Epoch 458\n",
      "-------------------------------\n",
      "Train Loss: 0.7008342146873474\n",
      "Test Loss: 0.6742405295372009\n",
      "Epoch 459\n",
      "-------------------------------\n",
      "Train Loss: 0.684260082244873\n",
      "Test Loss: 0.6852607011795044\n",
      "Epoch 460\n",
      "-------------------------------\n",
      "Train Loss: 0.6900207161903381\n",
      "Test Loss: 0.6804870247840882\n",
      "Epoch 461\n",
      "-------------------------------\n",
      "Train Loss: 0.6803080439567566\n",
      "Test Loss: 0.6989550590515137\n",
      "Epoch 462\n",
      "-------------------------------\n",
      "Train Loss: 0.6762194395065307\n",
      "Test Loss: 0.6862603187561035\n",
      "Epoch 463\n",
      "-------------------------------\n",
      "Train Loss: 0.6772815465927124\n",
      "Test Loss: 0.6906569600105286\n",
      "Epoch 464\n",
      "-------------------------------\n",
      "Train Loss: 0.6814982056617737\n",
      "Test Loss: 0.6855022192001343\n",
      "Epoch 465\n",
      "-------------------------------\n",
      "Train Loss: 0.6680917501449585\n",
      "Test Loss: 0.674543035030365\n",
      "Epoch 466\n",
      "-------------------------------\n",
      "Train Loss: 0.687333858013153\n",
      "Test Loss: 0.6851910829544068\n",
      "Epoch 467\n",
      "-------------------------------\n",
      "Train Loss: 0.6801339864730835\n",
      "Test Loss: 0.6807306289672852\n",
      "Epoch 468\n",
      "-------------------------------\n",
      "Train Loss: 0.6771188497543335\n",
      "Test Loss: 0.6858275055885314\n",
      "Epoch 469\n",
      "-------------------------------\n",
      "Train Loss: 0.6760429501533508\n",
      "Test Loss: 0.681745982170105\n",
      "Epoch 470\n",
      "-------------------------------\n",
      "Train Loss: 0.6962486505508423\n",
      "Test Loss: 0.6723685979843139\n",
      "Epoch 471\n",
      "-------------------------------\n",
      "Train Loss: 0.6871504783630371\n",
      "Test Loss: 0.6865846514701843\n",
      "Epoch 472\n",
      "-------------------------------\n",
      "Train Loss: 0.657738733291626\n",
      "Test Loss: 0.6827251195907593\n",
      "Epoch 473\n",
      "-------------------------------\n",
      "Train Loss: 0.6932561874389649\n",
      "Test Loss: 0.7150000333786011\n",
      "Epoch 474\n",
      "-------------------------------\n",
      "Train Loss: 0.6834600806236267\n",
      "Test Loss: 0.6815784215927124\n",
      "Epoch 475\n",
      "-------------------------------\n",
      "Train Loss: 0.6827309250831604\n",
      "Test Loss: 0.6834987163543701\n",
      "Epoch 476\n",
      "-------------------------------\n",
      "Train Loss: 0.658355450630188\n",
      "Test Loss: 0.7071334600448609\n",
      "Epoch 477\n",
      "-------------------------------\n",
      "Train Loss: 0.7003944039344787\n",
      "Test Loss: 0.6582315802574158\n",
      "Epoch 478\n",
      "-------------------------------\n",
      "Train Loss: 0.673295545578003\n",
      "Test Loss: 0.6835312366485595\n",
      "Epoch 479\n",
      "-------------------------------\n",
      "Train Loss: 0.6697789788246155\n",
      "Test Loss: 0.6856166124343872\n",
      "Epoch 480\n",
      "-------------------------------\n",
      "Train Loss: 0.7174168705940247\n",
      "Test Loss: 0.6822650790214538\n",
      "Epoch 481\n",
      "-------------------------------\n",
      "Train Loss: 0.6771287202835083\n",
      "Test Loss: 0.6875101208686829\n",
      "Epoch 482\n",
      "-------------------------------\n",
      "Train Loss: 0.6924041271209717\n",
      "Test Loss: 0.6748985886573792\n",
      "Epoch 483\n",
      "-------------------------------\n",
      "Train Loss: 0.6705320000648498\n",
      "Test Loss: 0.6571310877799987\n",
      "Epoch 484\n",
      "-------------------------------\n",
      "Train Loss: 0.681522536277771\n",
      "Test Loss: 0.6776103615760803\n",
      "Epoch 485\n",
      "-------------------------------\n",
      "Train Loss: 0.6760016202926635\n",
      "Test Loss: 0.677725875377655\n",
      "Epoch 486\n",
      "-------------------------------\n",
      "Train Loss: 0.676221227645874\n",
      "Test Loss: 0.6831636309623719\n",
      "Epoch 487\n",
      "-------------------------------\n",
      "Train Loss: 0.6934396982192993\n",
      "Test Loss: 0.693457555770874\n",
      "Epoch 488\n",
      "-------------------------------\n",
      "Train Loss: 0.6721892476081848\n",
      "Test Loss: 0.6830679297447204\n",
      "Epoch 489\n",
      "-------------------------------\n",
      "Train Loss: 0.674816882610321\n",
      "Test Loss: 0.6993618607521057\n",
      "Epoch 490\n",
      "-------------------------------\n",
      "Train Loss: 0.667582893371582\n",
      "Test Loss: 0.6832464456558227\n",
      "Epoch 491\n",
      "-------------------------------\n",
      "Train Loss: 0.6784255862236023\n",
      "Test Loss: 0.6710777878761292\n",
      "Epoch 492\n",
      "-------------------------------\n",
      "Train Loss: 0.6723316550254822\n",
      "Test Loss: 0.6775840997695923\n",
      "Epoch 493\n",
      "-------------------------------\n",
      "Train Loss: 0.6746532678604126\n",
      "Test Loss: 0.6791227102279663\n",
      "Epoch 494\n",
      "-------------------------------\n",
      "Train Loss: 0.6748159646987915\n",
      "Test Loss: 0.6569200992584229\n",
      "Epoch 495\n",
      "-------------------------------\n",
      "Train Loss: 0.6641789078712463\n",
      "Test Loss: 0.6706731915473938\n",
      "Epoch 496\n",
      "-------------------------------\n",
      "Train Loss: 0.6843458652496338\n",
      "Test Loss: 0.666791844367981\n",
      "Epoch 497\n",
      "-------------------------------\n",
      "Train Loss: 0.6778009414672852\n",
      "Test Loss: 0.6720040202140808\n",
      "Epoch 498\n",
      "-------------------------------\n",
      "Train Loss: 0.6815443396568298\n",
      "Test Loss: 0.6780201911926269\n",
      "Epoch 499\n",
      "-------------------------------\n",
      "Train Loss: 0.6608466863632202\n",
      "Test Loss: 0.6925438165664672\n",
      "Epoch 500\n",
      "-------------------------------\n",
      "Train Loss: 0.7115188360214233\n",
      "Test Loss: 0.694998037815094\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Define test function\n",
    "def test(dataloader, model, criterion):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    model.eval()\n",
    "    test_loss, correct = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred = model(X)\n",
    "            label = y.float().view(-1, 1)\n",
    "            test_loss += criterion(pred,label).item()\n",
    "            correct += (pred > 0.5).eq(y.view_as(pred)).sum().item()\n",
    "    test_loss /= num_batches\n",
    "    # correct /= size\n",
    "    print(f\"Test Loss: {test_loss}\")\n",
    "\n",
    "# Train and test the model\n",
    "epochs = 500\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    model.train()\n",
    "    loss_list = []\n",
    "    correct = 0\n",
    "    for batch, (X, y) in enumerate(train_dataloader):\n",
    "        # Compute prediction error\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        pred = model(X)\n",
    "        # print(pred)\n",
    "        label = y.float().view(-1, 1)\n",
    "        loss = loss_fn(pred, label)\n",
    "\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        loss_list.append(loss.item())\n",
    "\n",
    "        # if batch % 100 == 0:\n",
    "            # loss, current = loss.item(), (batch + 1) * len(X)\n",
    "    mean_loss = np.mean(loss_list)        \n",
    "    print(f\"Train Loss: {mean_loss}\")\n",
    "    \n",
    "    test(test_dataloader, model, loss_fn)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming you have a list to store train and test accuracies for each epoch\n",
    "train_accuracies = []\n",
    "test_accuracies = []\n",
    "\n",
    "# Your training loop with accuracy calculations during each epoch\n",
    "epochs = 20\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train_acc = train(train_dataloader, model, loss_fn, optimizer)\n",
    "    test_acc = test(test_dataloader, model, loss_fn)\n",
    "    \n",
    "    # Append accuracies to the lists\n",
    "    train_accuracies.append(train_acc)\n",
    "    test_accuracies.append(test_acc)\n",
    "\n",
    "# Plot the graph\n",
    "epochs_range = range(1, epochs+1)\n",
    "plt.plot(epochs_range, train_accuracies, label='Train Accuracy')\n",
    "plt.plot(epochs_range, test_accuracies, label='Test Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy (%)')\n",
    "plt.title('Train and Test Accuracy Over Epochs')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"model.pth\")\n",
    "print(\"Saved PyTorch Model State to model.pth\")\n",
    "\n",
    "model = NeuralNetwork().to(device)\n",
    "model.load_state_dict(torch.load(\"model.pth\"))\n",
    "\n",
    "\n",
    "# test_dataloader\n",
    "# calculate the accuracy over the whole test set\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs499f22",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
