{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Assuming you have your data in the form of tensors (train_data, train_labels, test_data, test_labels)\n",
    "# You can convert your data to PyTorch tensors and create DataLoader objects.\n",
    "\n",
    "# Define your neural network model\n",
    "class BinaryClassifier(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(BinaryClassifier, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_size, output_size)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.sigmoid(x)\n",
    "        return x\n",
    "\n",
    "# Define the custom loss function\n",
    "class PrecisionLoss(nn.Module):\n",
    "    def __init__(self, alpha=1.0):\n",
    "        super(PrecisionLoss, self).__init__()\n",
    "        self.alpha = alpha  # Adjust this parameter to control the emphasis on precision\n",
    "\n",
    "    def forward(self, predicted, target):\n",
    "        # Calculate precision\n",
    "        true_positives = (predicted * target).sum(dim=0)\n",
    "        false_positives = (predicted * (1 - target)).sum(dim=0)\n",
    "\n",
    "        precision = true_positives / (true_positives + self.alpha * false_positives + 1e-9)\n",
    "\n",
    "        # Use negative precision as the loss to maximize precision during training\n",
    "        loss = -precision.mean()\n",
    "\n",
    "        return loss\n",
    "\n",
    "# Hyperparameters\n",
    "input_size = 10  # Update with your input size\n",
    "hidden_size = 64  # Update with your desired hidden layer size\n",
    "output_size = 1   # Binary classification\n",
    "learning_rate = 0.001\n",
    "epochs = 10\n",
    "\n",
    "# Create DataLoader objects for training and testing\n",
    "train_dataset = TensorDataset(train_data, train_labels)\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "test_dataset = TensorDataset(test_data, test_labels)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "# Initialize the model, loss function, and optimizer\n",
    "model = BinaryClassifier(input_size, hidden_size, output_size)\n",
    "loss_function = PrecisionLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    for inputs, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = loss_function(outputs.squeeze(), labels.float())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Evaluate on the test set\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        total_precision = 0.0\n",
    "        total_samples = 0\n",
    "        for inputs, labels in test_loader:\n",
    "            outputs = model(inputs)\n",
    "            predicted = (outputs > 0.5).float()\n",
    "            total_precision += (predicted * labels).sum().item()\n",
    "            total_samples += labels.sum().item()\n",
    "\n",
    "        precision = total_precision / (total_samples + 1e-9)\n",
    "        print(f'Epoch {epoch + 1}/{epochs}, Precision: {precision}')\n",
    "\n",
    "# After training, you can use the model for predictions\n",
    "# For example:\n",
    "# model.eval()\n",
    "# with torch.no_grad():\n",
    "#     new_data = torch.tensor(...)  # Input your new data\n",
    "#     predicted_labels = model(new_data)\n",
    "#     predicted_labels = (predicted_labels > 0.5).float()\n",
    "#     print(predicted_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "\n",
    "\n",
    "class TorchModel(torch.nn.Module):\n",
    "    def __init__(self, n_hidden_layers, units_in_first_layer, units_per_hidden_layer=100):\n",
    "        super(TorchModel, self).__init__()\n",
    "        units_per_layer = [units_in_first_layer]\n",
    "        for layer_i in range(n_hidden_layers):\n",
    "            units_per_layer.append(units_per_hidden_layer)\n",
    "        units_per_layer.append(1)\n",
    "        seq_args = []\n",
    "        for layer_i in range(len(units_per_layer)-1):\n",
    "            units_in = units_per_layer[layer_i]\n",
    "            units_out = units_per_layer[layer_i+1]\n",
    "            seq_args.append(\n",
    "                torch.nn.Linear(units_in, units_out))\n",
    "            if layer_i != len(units_per_layer)-2:\n",
    "                seq_args.append(torch.nn.ReLU())\n",
    "        self.stack = torch.nn.Sequential(*seq_args)\n",
    "\n",
    "    def forward(self, feature_mat):\n",
    "        return self.stack(feature_mat)\n",
    "\n",
    "\n",
    "class NumpyData(torch.utils.data.Dataset):\n",
    "    def __init__(self, features, labels):\n",
    "        self.features = features\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        return self.features[item, :], self.labels[item]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "\n",
    "class MyCV:\n",
    "    def __init__(self, estimator, param_grid, cv):\n",
    "        self.estimator = estimator\n",
    "        self.param_grid = param_grid\n",
    "        self.cv = cv\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.train_features = X\n",
    "        self.train_labels = y\n",
    "        self.best_params_ = {}\n",
    "        np.random.seed(1)\n",
    "        fold_vec = np.random.randint(\n",
    "            low=0, high=self.cv, size=self.train_labels.size)\n",
    "        \n",
    "        best_mean_accuracy = 0\n",
    "        loss_df_list = []\n",
    "        for param_dict in self.param_grid:\n",
    "            for param_name, [param_value] in param_dict.items():\n",
    "                setattr(self.estimator, param_name, param_value)\n",
    "            \n",
    "            local_loss_df_list = []    \n",
    "            for test_fold in range(self.cv):\n",
    "                is_set_dict = {\n",
    "                    \"validation\": fold_vec == test_fold,\n",
    "                    \"subtrain\": fold_vec != test_fold,\n",
    "                }\n",
    "                set_features = {\n",
    "                    set_name: self.train_features[is_set, :]\n",
    "                    for set_name, is_set in is_set_dict.items()\n",
    "                }\n",
    "                set_labels = {\n",
    "                    set_name: self.train_labels[is_set]\n",
    "                    for set_name, is_set in is_set_dict.items()\n",
    "                }\n",
    "                self.estimator.fit(\n",
    "                    X=set_features, y=set_labels)\n",
    "                predicted_labels = self.estimator.predict(\n",
    "                    X=set_features[\"validation\"])\n",
    "           \n",
    "                local_loss_df_list.append(self.estimator.loss_df)\n",
    "            mean_local_loss_df = pd.concat(local_loss_df_list).groupby(\n",
    "                [\"hidden_layers\", \"step_size\", \"epoch\", \"set_name\"]).mean().reset_index()\n",
    "            loss_df_list.append(mean_local_loss_df)         \n",
    "        self.loss_mean_df = pd.concat(loss_df_list)\n",
    "        print(self.loss_mean_df)\n",
    "\n",
    "    def predict(self, X):\n",
    "        return self.estimator.predict(X)\n",
    "\n",
    "\n",
    "class RegularizedMLP:\n",
    "    def __init__(self, max_epochs, batch_size, step_size, hidden_layers, units_per_hidden_layer):\n",
    "        self.max_epochs = max_epochs\n",
    "        self.batch_size = batch_size\n",
    "        self.step_size = step_size\n",
    "        self.hidden_layers = hidden_layers\n",
    "        self.units_per_hidden_layer = units_per_hidden_layer\n",
    "        self.loss_fun = torch.nn.BCEWithLogitsLoss()\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        set_features = X\n",
    "        set_labels = y\n",
    "        # Preparing subtrain and validation data loaders\n",
    "        subtrain_csv = NumpyData(\n",
    "            set_features[\"subtrain\"], set_labels[\"subtrain\"])\n",
    "        subtrain_dl = torch.utils.data.DataLoader(\n",
    "            subtrain_csv, batch_size=self.batch_size, shuffle=True)\n",
    "        loss_df_list = []\n",
    "       \n",
    "        model = TorchModel(self.hidden_layers, set_features[\"subtrain\"].shape[1])\n",
    "        model.train()\n",
    "        optimizer = torch.optim.SGD(model.parameters(), lr=self.step_size)\n",
    "\n",
    "        for epoch in range(self.max_epochs):\n",
    "            for batch_features, batch_labels in subtrain_dl:\n",
    "                # Take a step and compute prediction error\n",
    "                # Compute prediction error\n",
    "                pred_tensor = model(batch_features.float()).reshape(\n",
    "                    len(batch_labels.float()))\n",
    "                loss_tensor = self.loss_fun(\n",
    "                    pred_tensor, batch_labels.float())\n",
    "                # Backpropagation\n",
    "                optimizer.zero_grad()\n",
    "                loss_tensor.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "            # then compute subtrain/validation loss.\n",
    "            for set_name in set_features:\n",
    "                feature_mat = set_features[set_name]\n",
    "                label_vec = set_labels[set_name]\n",
    "                feature_mat_tensor = torch.from_numpy(\n",
    "                    feature_mat.astype(np.float32))\n",
    "                label_vec_tensor = torch.from_numpy(\n",
    "                    label_vec.astype(np.float32))\n",
    "\n",
    "                pred_tensor = model(feature_mat_tensor.float()).reshape(\n",
    "                    len(label_vec_tensor.float()))\n",
    "                loss_tensor = self.loss_fun(\n",
    "                    pred_tensor, label_vec_tensor.float())\n",
    "                set_loss = loss_tensor.item()\n",
    "\n",
    "                loss_df_list.append(pd.DataFrame({\n",
    "                    \"hidden_layers\": self.hidden_layers,\n",
    "                    \"step_size\": self.step_size,\n",
    "                    \"set_name\": set_name,\n",
    "                    \"loss\": set_loss,\n",
    "                    \"epoch\": epoch,\n",
    "                }, index=[0]))\n",
    "        self.model = model\n",
    "        self.loss_df = pd.concat(loss_df_list)\n",
    "\n",
    "    def decision_function(self, X):\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            return self.model(torch.Tensor(X)).numpy().ravel()\n",
    "\n",
    "    def predict(self, X):\n",
    "        return np.where(self.decision_function(X) > 0, 1, 0)\n",
    "\n",
    "\n",
    "\n",
    "eurusd_df = pd.read_csv(\"/projects/genomic-ml/da2343/ml_project_2/dnn/trades_seq_fixed_EURUSD_H1_2011_2023.csv\")\n",
    "eur_usd_labels = eurusd_df[\"label\"]\n",
    "eur_usd_features = eurusd_df[[\"position\", \"RSI\", \"ATR\", \"ADX\", \"AROON_Oscillator\"]]\n",
    "\n",
    "\n",
    "data_dict = {\n",
    "    \"eurusd\": (eur_usd_features.to_numpy() , eur_usd_labels.to_numpy()),\n",
    "}\n",
    "\n",
    "param_list = [\n",
    "    {\n",
    "        # 'hidden_layers': [4],\n",
    "        'hidden_layers': [1],\n",
    "        'step_size': [0.1],\n",
    "    }\n",
    "]\n",
    "\n",
    "for data_set, (input_mat, output_vec) in data_dict.items():\n",
    "    rmlp = RegularizedMLP(\n",
    "        max_epochs=100,\n",
    "        batch_size=100,\n",
    "        step_size=0.1,\n",
    "        hidden_layers=3,\n",
    "        units_per_hidden_layer=100,\n",
    "    )\n",
    "    learner_instance = MyCV(estimator=rmlp, param_grid=param_list, cv=2)\n",
    "    learner_instance.fit(input_mat, output_vec)\n",
    "    custom_model = learner_instance.estimator.model\n",
    "    # save the model to disk\n",
    "    filename = 'finalized_model.pth'\n",
    "    torch.save(custom_model.state_dict(), filename)\n",
    "    \n",
    "    loss_df = learner_instance.loss_mean_df\n",
    "    loss_df.index = range(len(loss_df))\n",
    "    print(loss_df)\n",
    "    # loss_df.to_csv(out_file, encoding='utf-8', index=False)\n",
    "print(\"done\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = TorchModel(n_hidden_layers=1,units_in_first_layer =1)\n",
    "model.load_state_dict(torch.load(\"model.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eur_usd_features"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs499f22",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
