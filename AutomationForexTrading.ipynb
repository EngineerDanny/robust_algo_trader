{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from stable_baselines3 import PPO\n",
        "\n",
        "model = PPO(\"MlpPolicy\", \"CartPole-v1\").learn(10_000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import gymnasium as gym\n",
        "from stable_baselines3 import PPO\n",
        "\n",
        "env = gym.make(\"CartPole-v1\", render_mode=\"human\")\n",
        "model = PPO(\"MlpPolicy\", env, verbose=1)\n",
        "model.learn(total_timesteps=10_000)\n",
        "\n",
        "vec_env = model.get_env()\n",
        "obs = vec_env.reset()\n",
        "for i in range(1000):\n",
        "    action, _states = model.predict(obs, deterministic=True)\n",
        "    obs, reward, done, info = vec_env.step(action)\n",
        "    vec_env.render()\n",
        "    # VecEnv resets automatically\n",
        "    if done:\n",
        "      obs = env.reset()\n",
        "\n",
        "env.close()\n",
        "\n",
        "# exit program\n",
        "exit()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import gymnasium as gym\n",
        "from stable_baselines3 import PPO\n",
        "\n",
        "env = gym.make(\"CartPole-v1\", render_mode=\"human\")\n",
        "model = PPO(\"MlpPolicy\", env, verbose=1)\n",
        "total_timesteps = 100\n",
        "model.learn(total_timesteps=total_timesteps) \n",
        "\n",
        "# observation, info = env.reset(seed=42)\n",
        "# for _ in range(total_timesteps):\n",
        "#     # action = env.action_space.sample()\n",
        "#     action, _states = model.predict(observation, deterministic=True)\n",
        "#     observation, reward, terminated, truncated, info = env.step(action)\n",
        "#     env.render()\n",
        "    \n",
        "#     if terminated or truncated:\n",
        "#         observation, info = env.reset()\n",
        "        \n",
        "# env.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "from stable_baselines3 import SAC\n",
        "from stable_baselines3.common.callbacks import BaseCallback\n",
        "\n",
        "model = SAC(\"MlpPolicy\", \"Pendulum-v1\", tensorboard_log=\"./a2c_cartpole_tensorboard/\", verbose=1)\n",
        "\n",
        "\n",
        "class TensorboardCallback(BaseCallback):\n",
        "    \"\"\"\n",
        "    Custom callback for plotting additional values in tensorboard.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, verbose=0):\n",
        "        super().__init__(verbose)\n",
        "\n",
        "    def _on_step(self) -> bool:\n",
        "        # Log scalar value (here a random variable)\n",
        "        value = np.random.random()\n",
        "        self.logger.record(\"random_value\", value)\n",
        "        return True\n",
        "\n",
        "\n",
        "model.learn(50000,tb_log_name=\"4_run\", callback=TensorboardCallback())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from stable_baselines3 import A2C\n",
        "\n",
        "model = A2C(\"MlpPolicy\", \"CartPole-v1\", verbose=1, tensorboard_log=\"./a2c_cartpole_tensorboard/\")\n",
        "model.learn(total_timesteps=10_000, tb_log_name=\"first_run\")\n",
        "# Pass reset_num_timesteps=False to continue the training curve in tensorboard\n",
        "# By default, it will create a new curve\n",
        "# Keep tb_log_name constant to have continuous curve (see note below)\n",
        "model.learn(total_timesteps=10_000, tb_log_name=\"second_run\", reset_num_timesteps=False)\n",
        "model.learn(total_timesteps=10_000, tb_log_name=\"third_run\", reset_num_timesteps=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from stable_baselines3 import A2C\n",
        "\n",
        "model = A2C(\"MlpPolicy\", \"CartPole-v1\", verbose=1, tensorboard_log=\"./a2c_cartpole_tensorboard/\")\n",
        "model.learn(total_timesteps=100_000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model.policy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import gymnasium as gym\n",
        "import torch as th\n",
        "\n",
        "from stable_baselines3 import PPO\n",
        "\n",
        "# Custom actor (pi) and value function (vf) networks\n",
        "# of two layers of size 32 each with Relu activation function\n",
        "# Note: an extra linear layer will be added on top of the pi and the vf nets, respectively\n",
        "policy_kwargs = dict(activation_fn=th.nn.ReLU,\n",
        "                     net_arch=dict(pi=[32, 32], \n",
        "                                   vf=[32, 32]))\n",
        "# Create the agent\n",
        "model = PPO(\"MlpPolicy\", \"CartPole-v1\", policy_kwargs=policy_kwargs, verbose=1)\n",
        "# Retrieve the environment\n",
        "env = model.get_env()\n",
        "# Train the agent\n",
        "model.learn(total_timesteps=20_000)\n",
        "# Save the agent\n",
        "# model.save(\"ppo_cartpole\")\n",
        "\n",
        "# del model\n",
        "# the policy_kwargs are automatically loaded\n",
        "# model = PPO.load(\"ppo_cartpole\", env=env)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import gymnasium as gym\n",
        "\n",
        "from stable_baselines3 import A2C\n",
        "from stable_baselines3.common.env_util import make_vec_env\n",
        "\n",
        "# Parallel environments\n",
        "env = make_vec_env(\"CartPole-v1\", n_envs=4)\n",
        "\n",
        "# model = A2C(\"MlpPolicy\", env, verbose=1)\n",
        "# model.learn(total_timesteps=25000)\n",
        "# model.save(\"a2c_cartpole\")\n",
        "\n",
        "# del model # remove to demonstrate saving and loading\n",
        "\n",
        "model = A2C.load(\"a2c_cartpole\")\n",
        "\n",
        "obs = env.reset()\n",
        "while True:\n",
        "    action, _states = model.predict(obs)\n",
        "    obs, rewards, dones, info = env.step(action)\n",
        "    env.render()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import gym\n",
        "import gym_anytrading\n",
        "from gym_anytrading.envs import TradingEnv, ForexEnv, StocksEnv, Actions, Positions \n",
        "from gym_anytrading.datasets import FOREX_EURUSD_1H_ASK, STOCKS_GOOGL\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "env = gym.make('forex-v0', frame_bound=(50, 100), window_size=10)\n",
        "# env = gym.make('stocks-v0', frame_bound=(50, 100), window_size=10)\n",
        "\n",
        "observation = env.reset()\n",
        "while True:\n",
        "    action = env.action_space.sample()\n",
        "    observation, reward, done, info = env.step(action)\n",
        "    # env.render()\n",
        "    if done:\n",
        "        print(\"info:\", info)\n",
        "        break\n",
        "\n",
        "plt.cla()\n",
        "env.render_all()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Importing the necessary modules\n",
        "import gym\n",
        "from gym import spaces\n",
        "from gym.utils import seeding\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "# Defining some constants and helper functions\n",
        "WINDOW_SIZE = 10 # The number of previous observations to include in the state\n",
        "MAX_ACCOUNT_BALANCE = 2147483647 # The maximum possible account balance\n",
        "MAX_NUM_SHARES = 2147483647 # The maximum possible number of shares to hold\n",
        "MAX_STEPS = 20000 # The maximum possible number of steps in an episode\n",
        "INITIAL_ACCOUNT_BALANCE = 10000 # The initial account balance at the start of an episode\n",
        "\n",
        "# A function to format a price as a string\n",
        "def format_price(n):\n",
        "    return (\"-$\" if n < 0 else \"$\") + \"{0:.2f}\".format(abs(n))\n",
        "\n",
        "# A function to get the state vector from the data frame and the current step\n",
        "def get_state(data, t, n):\n",
        "    d = t - n + 1\n",
        "    block = data[d:t + 1] if d >= 0 else -d * [data[0]] + data[0:t + 1] # pad with t0\n",
        "    res = []\n",
        "    for i in range(n - 1):\n",
        "        res.append(block[i + 1] - block[i])\n",
        "    return np.array([res])\n",
        "\n",
        "# Defining the TradingEnv class, which inherits from the gym.Env class and implements the methods for creating, resetting, stepping, rendering, and closing the environment\n",
        "class TradingEnv(gym.Env):\n",
        "    # A constructor method that takes a data frame, a window size, and a frame bound as arguments and initializes the environment attributes\n",
        "    def __init__(self, df, window_size, frame_bound):\n",
        "        assert df.ndim == 2 # Check that the data frame has two dimensions (rows and columns)\n",
        "        assert len(frame_bound) == 2 # Check that the frame bound has two elements (start and end)\n",
        "        assert frame_bound[0] > window_size # Check that the frame bound start is larger than the window size\n",
        "\n",
        "        self.seed() # Set a random seed for reproducibility\n",
        "        self.df = df # Assign the data frame to an attribute\n",
        "        self.window_size = window_size # Assign the window size to an attribute\n",
        "        self.frame_bound = frame_bound # Assign the frame bound to an attribute\n",
        "        self.prices, self.signal_features = self._process_data() # Process the data and assign the prices and signal features to attributes\n",
        "\n",
        "        self.shape = (window_size, self.signal_features.shape[1]) # Define the shape of the state vector as a tuple of (window size, number of features)\n",
        "\n",
        "        self.current_step = None # Initialize the current step as None\n",
        "        self.balance = None # Initialize the balance as None\n",
        "        self.shares_held = None # Initialize the shares held as None\n",
        "        self.cost_basis = None # Initialize the cost basis as None\n",
        "        self.total_shares_sold = None # Initialize the total shares sold as None\n",
        "        self.total_sales_value = None # Initialize the total sales value as None\n",
        "\n",
        "        self.trades = [] # Initialize an empty list to store the trades\n",
        "\n",
        "        self.action_space = spaces.Discrete(3) # Define the action space as a discrete space with three possible actions: buy (0), sell (1), or hold (2)\n",
        "\n",
        "        self.observation_space = spaces.Box(low=-np.inf, high=np.inf, shape=self.shape, dtype=np.float32) # Define the observation space as a box space with infinite bounds and a shape equal to the state vector shape\n",
        "\n",
        "    # A method to process the data and extract the prices and signal features from the data frame\n",
        "    def _process_data(self):\n",
        "        prices = self.df.loc[:, 'Close'].to_numpy() # Get the close prices from the data frame and convert them to a numpy array\n",
        "\n",
        "        prices[self.frame_bound[0] - self.window_size]  # validate index; start point should be >= 0\n",
        "\n",
        "        diff = np.insert(np.diff(prices), 0, 0) # Compute the difference between consecutive prices and insert a zero at the beginning\n",
        "        signal_features = np.column_stack((prices, diff)) # Stack the prices and diff arrays horizontally to form a matrix of signal features\n",
        "\n",
        "        return prices, signal_features\n",
        "\n",
        "    # A method to update the profit based on the current price and action\n",
        "    def _update_profit(self, action, current_price):\n",
        "        if action == 0: # buy action\n",
        "            if self.balance > current_price: \n",
        "                self.balance -= current_price \n",
        "                self.cost_basis += current_price \n",
        "                self.shares_held += 1 \n",
        "                self.trades.append({'step': self.current_step,\n",
        "                                    'shares': 1,\n",
        "                                    'total': current_price,\n",
        "                                    'type': \"buy\"})\n",
        "        elif action == 1: # sell action\n",
        "            if self.shares_held > 0:\n",
        "                self.balance += current_price\n",
        "                self.shares_held -= 1\n",
        "                self.cost_basis -= current_price\n",
        "                self.total_shares_sold += 1\n",
        "                self.total_sales_value += current_price\n",
        "                self.trades.append({'step': self.current_step,\n",
        "                                    'shares': 1,\n",
        "                                    'total': current_price,\n",
        "                                    'type': \"sell\"})\n",
        "\n",
        "    # A method to get the next observation from the signal features and the current step\n",
        "    def _next_observation(self):\n",
        "        frame = np.arange(self.current_step - self.window_size + 1, self.current_step + 1) # Get a slice of the signal features corresponding to the window size\n",
        "        return self.signal_features[frame, :] # Return the sliced signal features as a numpy array\n",
        "\n",
        "    # A method to take an action and update the state, reward, done, and info\n",
        "    def _take_action(self, action):\n",
        "        current_price = self.prices[self.current_step] # Get the current price from the prices array\n",
        "        self._update_profit(action, current_price) # Update the profit based on the action and the current price\n",
        "\n",
        "        prev_cost = self.cost_basis / (self.shares_held + 1e-15) # Compute the previous cost per share\n",
        "        additional_cost = current_price - prev_cost # Compute the additional cost per share\n",
        "\n",
        "        reward = -additional_cost * self.shares_held # Compute the reward as the negative of the additional cost times the shares held\n",
        "\n",
        "        done = False # Initialize done as False\n",
        "\n",
        "        if self.balance <= 0: # If the balance is zero or negative, set done to True and penalize the reward\n",
        "            done = True\n",
        "            reward -= INITIAL_ACCOUNT_BALANCE / 2\n",
        "\n",
        "        if self.current_step == len(self.prices) - 1: # If the current step is the last step, set done to True\n",
        "            done = True\n",
        "\n",
        "        info = {'balance': self.balance, # Create a dictionary of info with some useful metrics\n",
        "                'shares_held': self.shares_held,\n",
        "                'total_shares_sold': self.total_shares_sold,\n",
        "                'total_sales_value': self.total_sales_value,\n",
        "                'cost_basis': self.cost_basis,\n",
        "                'net_worth': (self.balance + (self.shares_held * current_price)),\n",
        "                'profit': (self.balance + (self.shares_held * current_price)) - INITIAL_ACCOUNT_BALANCE}\n",
        "\n",
        "        return reward, done, info\n",
        "\n",
        "    # A method to reset the environment and return the initial observation\n",
        "    def reset(self):\n",
        "        self.current_step = random.randint(self.frame_bound[0], self.frame_bound[1]) # Set the current step to a random value within the frame bound\n",
        "\n",
        "        self.balance = INITIAL_ACCOUNT_BALANCE # Reset the balance to the initial value\n",
        "        self.shares_held = 0 # Reset the shares held to zero\n",
        "        self.cost_basis = 0 # Reset the cost basis to zero\n",
        "        self.total_shares_sold = 0 # Reset the total shares sold to zero\n",
        "        self.total_sales_value = 0 # Reset the total sales value to zero\n",
        "\n",
        "        return self._next_observation() # Return the initial observation\n",
        "\n",
        "    # A method to take a step in the environment and return the observation, reward, done, and info\n",
        "    def step(self, action):\n",
        "        assert action in [0, 1, 2] # Check that the action is valid\n",
        "\n",
        "        prev_net_worth = (self.balance + (self.shares_held * self.prices[self.current_step])) # Compute the previous net worth\n",
        "\n",
        "        self.current_step += 1 # Increment the current step by one\n",
        "\n",
        "        reward, done, info = self._take_action(action) # Take an action and get the reward, done, and info\n",
        "\n",
        "        next_state = self._next_observation() # Get the next state\n",
        "\n",
        "        info['prev_net_worth'] = prev_net_worth # Add the previous net worth to the info dictionary\n",
        "\n",
        "        return next_state, reward, done, info\n",
        "\n",
        "    # A method to render the environment using matplotlib\n",
        "    def render(self, mode='human'):\n",
        "        if mode == 'system':\n",
        "            print(f'Step: {self.current_step}, '\n",
        "                  f'Net Worth: {self.balance + (self.shares_held * self.prices[self.current_step])}')\n",
        "            return\n",
        "\n",
        "        elif mode == 'none':\n",
        "            return\n",
        "\n",
        "        window_start = max(self.current_step - WINDOW_SIZE, 0)\n",
        "        window_end = min(self.current_step + WINDOW_SIZE + 1, len(self.prices))\n",
        "        \n",
        "        date_range = pd.date_range(start=self.df.index[window_start], end=self.df.index[window_end-1])\n",
        "        \n",
        "        plt.figure(figsize=(10, 6)) # Create a figure with a specified size\n",
        "        plt.title('Trade History') # Set the title of the figure\n",
        "        plt.xlabel('Date') # Set the x-axis label of the figure\n",
        "        plt.ylabel('Price') # Set the y-axis label of the figure\n",
        "        plt.plot(date_range, self.prices[window_start:window_end], label='price', color='g') # Plot the prices in the window range with a green line and a label\n",
        "\n",
        "        for trade in self.trades: # Loop over the trades list\n",
        "            if trade['step'] >= window_start and trade['step'] < window_end: # Check if the trade step is within the window range\n",
        "                date = self.df.index[trade['step']] # Get the date of the trade step from the data frame index\n",
        "                color = 'r' if trade['type'] == 'sell' else 'b' # Set the color to red if the trade type is sell, or blue if it is buy\n",
        "                marker = 'v' if trade['type'] == 'sell' else '^' # Set the marker to a downward triangle if the trade type is sell, or an upward triangle if it is buy\n",
        "                plt.scatter(date, trade['total'], color=color, marker=marker, s=100) # Plot a scatter point for the trade with the specified color, marker, and size\n",
        "\n",
        "        plt.legend() # Show the legend of the plot\n",
        "        plt.show() # Show the plot\n",
        "\n",
        "    # A method to close the environment and release any resources\n",
        "    def close(self):\n",
        "        pass\n",
        "\n",
        "    # A method to save a rendered image to a file\n",
        "    def save_rendered_image(self, path):\n",
        "        fig = plt.gcf() # Get the current figure\n",
        "        fig.savefig(path) # Save the figure to a file\n",
        "\n",
        "# Defining the StocksEnv class, which inherits from the TradingEnv class and overrides some methods to customize the environment for stocks trading\n",
        "class StocksEnv(TradingEnv):\n",
        "    # A constructor method that takes a data frame and a window size as arguments and initializes the environment attributes\n",
        "    def __init__(self, df, window_size):\n",
        "        super().__init__(df, window_size, (WINDOW_SIZE, len(df) - 1)) # Call the constructor of the parent class with a frame bound equal to (window size, length of data frame - 1)\n",
        "\n",
        "    # A method to process the data and extract the prices and signal features from the data frame\n",
        "    def _process_data(self):\n",
        "        prices = self.df.loc[:, 'Close'].to_numpy() # Get the close prices from the data frame and convert them to a numpy array\n",
        "\n",
        "        prices[self.frame_bound[0] - self.window_size]  # validate index; start point should be >= 0\n",
        "\n",
        "        diff = np.insert(np.diff(prices), 0, 0) # Compute the difference between consecutive prices and insert a zero at the beginning\n",
        "        macd = self.df.loc[:, 'macd'].to_numpy() # Get the macd values from the data frame and convert them to a numpy array\n",
        "        rsi = self.df.loc[:, 'rsi'].to_numpy() # Get the rsi values from the data frame and convert them to a numpy array\n",
        "\n",
        "        signal_features = np.column_stack((prices, diff, macd, rsi)) # Stack the prices, diff, macd, and rsi arrays horizontally to form a matrix of signal features\n",
        "\n",
        "        return prices, signal_features\n",
        "\n",
        "# Defining the ForexEnv class, which inherits from the TradingEnv class and overrides some methods to customize the environment for forex trading\n",
        "class ForexEnv(TradingEnv):\n",
        "    # A constructor method that takes a data frame and a window size as arguments and initializes the environment attributes\n",
        "    def __init__(self, df, window_size):\n",
        "        super().__init__(df, window_size, (WINDOW_SIZE + 1, len(df))) # Call the constructor of the parent class with a frame bound equal to (window size + 1, length of data frame)\n",
        "\n",
        "    # A method to process the data and extract the prices and signal features from the data frame\n",
        "    def _process_data(self):\n",
        "        prices = self.df.loc[:, 'Close'].to_numpy() / 10000.0 # Get the close prices from the data frame, divide them by 10000.0, and convert them to a numpy array\n",
        "\n",
        "        prices[self.frame_bound[0] - self.window_size]  # validate index; start point should be >= 0\n",
        "\n",
        "        diff = np.insert(np.diff(prices), 0, 0) / 100.0 # Compute the difference between consecutive prices, divide them by 100.0, and insert a zero at the beginning\n",
        "\n",
        "        signal_features = np.column_stack((prices, diff)) # Stack the prices and diff arrays horizontally to form a matrix of signal features\n",
        "\n",
        "        return prices, signal_features\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "AutomationForexTrading.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
